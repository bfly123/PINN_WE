{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transonic flow through cylinder with 2D Euler equations\n",
    "\n",
    "This code is modified from the code https://github.com/alexpapados/Physics-Informed-Deep-Learning-Solid-and-Fluid-Mechanics\n",
    "\n",
    "The paper about this work can ref https://www.researchgate.net/publication/359480166_Discontinuity_Computing_with_Physics-Informed_Neural_Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import time\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import arange, meshgrid\n",
    "from smt.sampling_methods import LHS\n",
    "# Seeds\n",
    "torch.manual_seed(123)\n",
    "np.random.seed(123)\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    def closure():\n",
    "        optimizer.zero_grad()                                                     \n",
    "        loss_pde = model.loss_pde(x_int_train)                                   \n",
    "        loss_ic = model.loss_ic(x_ic_train, rho_ic_train,u_ic_train,v_ic_train,p_ic_train)   \n",
    "        loss_bdL = model.loss_bc(x_bcL_train, rho_bcL_train,u_bcL_train,v_bcL_train,p_bcL_train)   \n",
    "       # loss_bdR = model.loss_bc1(x_bcR_train,rho_bcR_train,u_bcR_train,v_bcR_train,p_bcR_train)   \n",
    "        loss_bdI = model.bd_B(x_bcI_train, sin_bcI_train,cos_bcI_train)  \n",
    "\n",
    "        loss_ib = loss_ic  +  loss_bdI +loss_bdL\n",
    "        loss = loss_pde + 10*loss_ib\n",
    "\n",
    "        print(f'epoch {epoch} loss_pde:{loss_pde:.8f}, loss_ib:{loss_ib:.8f}')\n",
    "        loss.backward()\n",
    "        return loss\n",
    "\n",
    "    # Optimize loss function\n",
    "    loss = optimizer.step(closure)\n",
    "    loss_value = loss.item() if not isinstance(loss, float) else loss\n",
    "    # Print total loss\n",
    "    print(f'epoch {epoch}: loss {loss_value:.6f}')\n",
    "    \n",
    "# Calculate gradients using torch.autograd.grad\n",
    "def gradients(outputs, inputs):\n",
    "    return torch.autograd.grad(outputs, inputs,grad_outputs=torch.ones_like(outputs), create_graph=True)\n",
    "\n",
    "# Convert torch tensor into np.array\n",
    "def to_numpy(input):\n",
    "    if isinstance(input, torch.Tensor):\n",
    "        return input.detach().cpu().numpy()\n",
    "    elif isinstance(input, np.ndarray):\n",
    "        return input\n",
    "    else:\n",
    "        raise TypeError('Unknown type of input, expected torch.Tensor or ' \\\n",
    "                        'np.ndarray, but got {}'.format(type(input)))\n",
    "def IC(x):\n",
    "    N =x.shape[0]\n",
    "    rho_init = np.zeros((x.shape[0]))                                              # rho - initial condition\n",
    "    u_init = np.zeros((x.shape[0]))                                                # u - initial condition\n",
    "    v_init = np.zeros((x.shape[0]))                                                # u - initial condition\n",
    "    p_init = np.zeros((x.shape[0]))                                                # p - initial condition\n",
    "    \n",
    "    gamma = 1.4\n",
    "    rho1 = 2.112\n",
    "    p1 =  3.011\n",
    "    v1 = 0.0\n",
    "    u1 = np.sqrt(1.4*p1/rho1)*0.728\n",
    "    \n",
    "    for i in range(N):\n",
    "        rho_init[i] = rho1\n",
    "        u_init[i] =   u1\n",
    "        v_init[i] =  v1\n",
    "        p_init[i] =  p1\n",
    "    return rho_init, u_init, v_init,p_init\n",
    "\n",
    "def BC_L(x):\n",
    "    N =x.shape[0]\n",
    "    rho_init = np.zeros((x.shape[0]))                                              # rho - initial condition\n",
    "    u_init = np.zeros((x.shape[0]))                                                # u - initial condition\n",
    "    v_init = np.zeros((x.shape[0]))                                                # u - initial condition\n",
    "    p_init = np.zeros((x.shape[0]))                                                # p - initial condition\n",
    "    \n",
    "    gamma = 1.4\n",
    "    #u1 = ms*npsqrt(gamma)\n",
    "    # rho, p - initial condition\n",
    "    rho1 = 2.112\n",
    "    p1 =  3.011\n",
    "    v1 = 0.0\n",
    "    u1 = np.sqrt(1.4*p1/rho1)*0.728\n",
    "    for i in range(N):\n",
    "        rho_init[i] = rho1\n",
    "        u_init[i] =  u1\n",
    "        v_init[i] =  v1\n",
    "        p_init[i] =  p1\n",
    "    return rho_init, u_init, v_init,p_init\n",
    "def BC_R(x):\n",
    "    N =x.shape[0]\n",
    "    rho_init = np.zeros((x.shape[0]))                                              # rho - initial condition\n",
    "    u_init = np.zeros((x.shape[0]))                                                # u - initial condition\n",
    "    v_init = np.zeros((x.shape[0]))                                                # u - initial condition\n",
    "    p_init = np.zeros((x.shape[0]))                                                # p - initial condition\n",
    "    \n",
    "    gamma = 1.4\n",
    "    ms = 2.0\n",
    "    rho1 = 1.0\n",
    "    p1 = 1.0\n",
    "    v1 = 0.0\n",
    "    u1 = 0\n",
    "    # rho, p - initial condition\n",
    "    for i in range(N):\n",
    "        rho_init[i] = rho1\n",
    "        u_init[i] = u1\n",
    "        v_init[i] = v1\n",
    "        p_init[i] = p1\n",
    "\n",
    "    return rho_init, u_init, v_init,p_init\n",
    "\n",
    "class DNN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DNN, self).__init__()\n",
    "        self.net = nn.Sequential()                                                  # Define neural network\n",
    "        self.net.add_module('Linear_layer_1', nn.Linear(3, 90))                     # First linear layer\n",
    "        self.net.add_module('Tanh_layer_1', nn.Tanh())                              # First activation Layer\n",
    "\n",
    "        for num in range(2, 6):                                                     # Number of layers (2 through 7)\n",
    "            self.net.add_module('Linear_layer_%d' % (num), nn.Linear(90, 90))       # Linear layer\n",
    "            self.net.add_module('Tanh_layer_%d' % (num), nn.Tanh())                 # Activation Layer\n",
    "        self.net.add_module('Linear_layer_final', nn.Linear(90, 4))                 # Output Layer\n",
    "\n",
    "    # Forward Feed\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "    def bd_B(self,x,sin,cos):\n",
    "        yb = self.net(x)\n",
    "        rhob,pb,ub,vb = yb[:, 0:1], yb[:, 1:2], yb[:, 2:3],yb[:,3:]\n",
    "        drhob_g = gradients(rhob, x)[0]                                      # Gradient [u_t, u_x]\n",
    "        rhob_x, rhob_y = drhob_g[:, 1:2], drhob_g[:, 2:3]                            # Partial derivatives u_t, u_x\n",
    "        dub_g = gradients(ub, x)[0]                                      # Gradient [u_t, u_x]\n",
    "        ub_x, ub_y = dub_g[:, 1:2], dub_g[:, 2:3]                            # Partial derivatives u_t, u_x\n",
    "        dvb_g = gradients(vb, x)[0]                                      # Gradient [u_t, u_x]\n",
    "        vb_x, vb_y = dvb_g[:, 1:2], dvb_g[:, 2:3]                            # Partial derivatives u_t, u_x\n",
    "        dpb_g = gradients(pb, x)[0]                                      # Gradient [p_t, p_x]\n",
    "        pb_x, pb_y = dpb_g[:, 1:2], dpb_g[:, 2:3]                            # Partial derivatives p_t, p_x\n",
    "        \n",
    "        deltau = ub_x + vb_y\n",
    "        lam = 0.1*(abs(deltau) - deltau) + 1\n",
    "        #lam = (deltau) - deltau) + 1\n",
    "        \n",
    "        fb = (((ub*cos + vb*sin)/lam)**2).mean() +\\\n",
    "            (((pb_x*cos + pb_y*sin)/lam)**2).mean() +\\\n",
    "            (((rhob_x*cos + rhob_y*sin)/lam)**2).mean()\n",
    "        return fb\n",
    "    def bd_OY(self,x):\n",
    "        y = self.net(x)\n",
    "        rho,p,u,v = y[:, 0:1], y[:, 1:2], y[:, 2:3],y[:,3:]\n",
    "        \n",
    "        drho_g = gradients(rho, x)[0]                                  # Gradient [rho_t, rho_x]\n",
    "        rho_x,rho_y = drho_g[:, :1], drho_g[:, 1:2]                    # Partial derivatives rho_t, rho_x\n",
    "        du_g = gradients(u, x)[0]                                      # Gradient [u_t, u_x]\n",
    "        u_x, u_y = du_g[:, :1], du_g[:, 1:2]                            # Partial derivatives u_t, u_x\n",
    "        dv_g = gradients(v, x)[0]                                      # Gradient [u_t, u_x]\n",
    "        v_x, v_y = dv_g[:, :1], dv_g[:, 1:2]                            # Partial derivatives u_t, u_x\n",
    "        dp_g = gradients(p, x)[0]                                      # Gradient [p_t, p_x]\n",
    "        p_x, p_y = dp_g[:, :1], dp_g[:, 1:2]                            # Partial derivatives p_t, p_x\n",
    "        \n",
    "        deltau = u_x + v_y\n",
    "        lam = 0.1*(abs(deltau) - deltau) + 1\n",
    "        \n",
    "        f = ((( u_y)/lam)**2).mean() +\\\n",
    "            ((( v_y)/lam)**2).mean() +\\\n",
    "            ((( p_y)/lam)**2).mean() +\\\n",
    "            ((( rho_y)/lam)**2).mean()\n",
    "        return f\n",
    "    \n",
    "    def bd_OX(self,x):\n",
    "        y = self.net(x)\n",
    "        rho,p,u,v = y[:, 0:1], y[:, 1:2], y[:, 2:3],y[:,3:]\n",
    "        \n",
    "        drho_g = gradients(rho, x)[0]                                  # Gradient [rho_t, rho_x]\n",
    "        rho_x,rho_y = drho_g[:, :1], drho_g[:, 1:2]                    # Partial derivatives rho_t, rho_x\n",
    "        du_g = gradients(u, x)[0]                                      # Gradient [u_t, u_x]\n",
    "        u_x, u_y = du_g[:, :1], du_g[:, 1:2]                            # Partial derivatives u_t, u_x\n",
    "        dv_g = gradients(v, x)[0]                                      # Gradient [u_t, u_x]\n",
    "        v_x, v_y = dv_g[:, :1], dv_g[:, 1:2]                            # Partial derivatives u_t, u_x\n",
    "        dp_g = gradients(p, x)[0]                                      # Gradient [p_t, p_x]\n",
    "        p_x, p_y = dp_g[:, :1], dp_g[:, 1:2]                            # Partial derivatives p_t, p_x\n",
    "        \n",
    "        deltau = u_x + v_y\n",
    "        lam = 0.1*(abs(deltau) - deltau) + 1\n",
    "        \n",
    "        f = ((( u_x)/lam)**2).mean() +\\\n",
    "            ((( v_x)/lam)**2).mean() +\\\n",
    "            ((( p_x)/lam)**2).mean() +\\\n",
    "            ((( rho_x)/lam)**2).mean()\n",
    "        return f\n",
    "     \n",
    "    # Loss function for PDE\n",
    "    def loss_pde(self, x):\n",
    "        \n",
    "        y = self.net(x)\n",
    "        gamma = 1.4                                                   \n",
    "        epsilon = 1e-5\n",
    "        rho,p,u,v = y[:, 0:1], y[:, 1:2], y[:, 2:3],y[:,3:]\n",
    "        \n",
    "        rhoE = p/(gamma - 1) +0.5*rho*(u**2+v**2)\n",
    "        \n",
    "        f1 = rho*u\n",
    "        f2 = rho*u*u+p\n",
    "        f3 = rho*u*v\n",
    "        f4 = (rhoE+p)*u\n",
    "        \n",
    "        g1 = rho*v\n",
    "        g2 = rho*v*u\n",
    "        g3 = rho*v*v + p\n",
    "        g4 = (rhoE+p)*v\n",
    "        \n",
    "        drho_g = gradients(rho,x)[0]\n",
    "        U1_t = drho_g[:, :1]\n",
    "        dU2_g = gradients(f1,x)[0]\n",
    "        U2_t = dU2_g[:, :1]\n",
    "        dU3_g = gradients(g1,x)[0]\n",
    "        U3_t = dU3_g[:, :1]\n",
    "        dU4_g = gradients(rhoE,x)[0]\n",
    "        U4_t = dU4_g[:, :1]\n",
    "        \n",
    "        df1_g = gradients(f1, x)[0]                                  \n",
    "        f1_x = df1_g[:, 1:2]\n",
    "        df2_g = gradients(f2, x)[0]                                  \n",
    "        f2_x = df2_g[:, 1:2]\n",
    "        df3_g = gradients(f3, x)[0]                                  \n",
    "        f3_x = df3_g[:, 1:2]\n",
    "        df4_g = gradients(f4, x)[0]                                  \n",
    "        f4_x = df4_g[:, 1:2]\n",
    "        \n",
    "        dg1_g = gradients(g1, x)[0]                                  \n",
    "        g1_y = dg1_g[:, 2:3]\n",
    "        dg2_g = gradients(g2, x)[0]                                  \n",
    "        g2_y = dg2_g[:, 2:3]\n",
    "        dg3_g = gradients(g3, x)[0]                                  \n",
    "        g3_y = dg3_g[:, 2:3]\n",
    "        dg4_g = gradients(g4, x)[0]                                  \n",
    "        g4_y = dg4_g[:, 2:3]\n",
    "        \n",
    "        \n",
    "        du_g = gradients(u, x)[0]                                \n",
    "        u_x = du_g[:, 1:2]         \n",
    "        dv_g = gradients(v, x)[0]                    \n",
    "        v_y = dv_g[:, 2:3]         \n",
    "        \n",
    "        deltau = u_x + v_y\n",
    "        nab = abs(deltau) - deltau\n",
    "        \n",
    "        lam = 0.1*nab + 1\n",
    "        \n",
    "        f = (((U1_t + f1_x+g1_y )/lam)**2).mean() +\\\n",
    "            (((U2_t + f2_x+g2_y )/lam)**2).mean() +\\\n",
    "            (((U3_t + f3_x+g3_y )/lam)**2).mean() +\\\n",
    "            (((U4_t + f4_x+g4_y )/lam)**2).mean()\n",
    "\n",
    "        return f\n",
    "\n",
    "    # Loss function for initial condition\n",
    "    def loss_ic(self, x_ic, rho_ic, u_ic, v_ic,p_ic):\n",
    "        U_ic = self.net(x_ic)                                                      # Initial condition\n",
    "        rho_ic_nn, p_ic_nn,u_ic_nn,v_ic_nn = U_ic[:, 0], U_ic[:, 1], U_ic[:, 2],U_ic[:,3]            # rho, u, p - initial condition\n",
    "\n",
    "        # Loss function for the initial condition\n",
    "        loss_ics = ((u_ic_nn - u_ic) ** 2).mean() + \\\n",
    "               ((rho_ic_nn- rho_ic) ** 2).mean()  + \\\n",
    "               ((p_ic_nn - p_ic) ** 2).mean() +\\\n",
    "               ((v_ic_nn - v_ic) ** 2).mean()\n",
    "\n",
    "        return loss_ics\n",
    "\n",
    "    def loss_bc(self, x_ic, rho_ic, u_ic, v_ic,p_ic):\n",
    "        U_ic = self.net(x_ic)                                                      # Initial condition\n",
    "        rho_ic_nn, p_ic_nn,u_ic_nn,v_ic_nn = U_ic[:, 0], U_ic[:, 1], U_ic[:, 2],U_ic[:,3]            # rho, u, p - initial condition\n",
    "\n",
    "        # Loss function for the initial condition\n",
    "        loss_ics = ((u_ic_nn - u_ic) ** 2).mean() + \\\n",
    "               ((rho_ic_nn- rho_ic) ** 2).mean()  + \\\n",
    "               ((p_ic_nn - p_ic) ** 2).mean() +\\\n",
    "               ((v_ic_nn - v_ic) ** 2).mean()\n",
    "\n",
    "        return loss_ics\n",
    "    def loss_bc1(self, x_ic, rho_ic, u_ic, v_ic,p_ic):\n",
    "        U_ic = self.net(x_ic)                                                      # Initial condition\n",
    "        rho_ic_nn, p_ic_nn,u_ic_nn,v_ic_nn = U_ic[:, 0], U_ic[:, 1], U_ic[:, 2],U_ic[:,3]            # rho, u, p - initial condition\n",
    "\n",
    "        # Loss function for the initial condition\n",
    "        loss_ics = ((rho_ic_nn- rho_ic) ** 2).mean()  + \\\n",
    "               ((p_ic_nn - p_ic) ** 2).mean() \n",
    "\n",
    "        return loss_ics\n",
    "\n",
    "def BD_circle(t,xc,yc,r,n):\n",
    "    x = np.zeros((n,3)) \n",
    "    sin = np.zeros((n,1)) \n",
    "    cos = np.zeros((n,1)) \n",
    "\n",
    "    for i in range(n):\n",
    "        the = 2*np.random.rand()*np.pi\n",
    "        xd = np.cos(the + np.pi/2)\n",
    "        yd = np.sin(the + np.pi/2)\n",
    "        x[i,0] = np.random.rand()*t\n",
    "        x[i,1] = xc  + xd*r\n",
    "        x[i,2] = yc  + yd*r\n",
    "        cos[i,0] = xd \n",
    "        sin[i,0] = yd\n",
    "    return x, sin,cos\n",
    "\n",
    "\n",
    "device = torch.device('cuda')                                          # Run on CPU\n",
    "lr = 0.001                                                           # Learning rate\n",
    "num_ib = 15000                                                # Random sampled points from IC0\n",
    "num_int = 100000                                                # Random sampled points in interior\n",
    "Tend = 0.4\n",
    "Lx = 1.5\n",
    "Ly = 2.0\n",
    "rx = 1.0\n",
    "ry = 1.0\n",
    "rd = 0.25\n",
    "\n",
    "xlimits = np.array([[0.,Tend],[0.0, Lx], [0,Ly]])  #interal\n",
    "sampling = LHS(xlimits=xlimits)\n",
    "x_int_train = sampling(num_int)\n",
    "A = []\n",
    "for i in range(num_int):\n",
    "    x = x_int_train[i,1]\n",
    "    y = x_int_train[i,2]\n",
    "    if ((x - rx)**2 +(y-ry)**2< rd**2):\n",
    "        A.append(i)\n",
    "x_int_train = np.delete(x_int_train,A,axis=0)\n",
    "\n",
    "xlimits = np.array([[0.,0.0],[0.0,Lx], [0.0,Ly]])  #interal\n",
    "sampling = LHS(xlimits=xlimits)\n",
    "x_ic_train = sampling(num_ib)\n",
    "A = []\n",
    "for i in range(num_ib):\n",
    "    x = x_ic_train[i,1]\n",
    "    y = x_ic_train[i,2]\n",
    "    if ((x - rx)**2 +(y-ry)**2< rd**2):\n",
    "        A.append(i)\n",
    "x_ic_train = np.delete(x_ic_train,A,axis=0)\n",
    "\n",
    "\n",
    "xlimits = np.array([[0.0,Tend],[0.0, 0.0], [0.0,Ly]])\n",
    "sampling = LHS(xlimits=xlimits)\n",
    "x_bcL_train =  sampling(num_ib)\n",
    "\n",
    "xlimits = np.array([[0.0,Tend], [Lx, Lx], [0.0,Ly]])\n",
    "sampling = LHS(xlimits=xlimits)\n",
    "x_bcR_train =  sampling(num_ib)\n",
    "\n",
    "\n",
    "xlimits = np.array([[0.0,Tend],[0.0, Lx], [Ly,Ly]])\n",
    "sampling = LHS(xlimits=xlimits)\n",
    "x_bcU_train =  sampling(num_ib)\n",
    "\n",
    "xlimits = np.array([[0.0,Tend], [0.0, Lx], [0.0,0.0]])\n",
    "sampling = LHS(xlimits=xlimits)\n",
    "x_bcD_train =  sampling(num_ib)                                           # Vectorized whole domain\n",
    "\n",
    "\n",
    "\n",
    "x_bcI_train,sin_bcI_train,cos_bcI_train = BD_circle(Tend,rx,ry,rd,num_ib)\n",
    "\n",
    "rho_bcL_train, u_bcL_train,v_bcL_train, p_bcL_train = BC_L(x_bcL_train)  \n",
    "rho_bcR_train, u_bcR_train,v_bcR_train, p_bcR_train = BC_L(x_bcR_train)  \n",
    "rho_ic_train, u_ic_train,v_ic_train, p_ic_train = IC(x_ic_train)  \n",
    "\n",
    "\n",
    "x_int_train = torch.tensor(x_int_train, requires_grad=True, dtype=torch.float32).to(device)\n",
    "\n",
    "rho_bcL_train = torch.tensor(rho_bcL_train, dtype=torch.float32).to(device)\n",
    "u_bcL_train = torch.tensor(u_bcL_train, dtype=torch.float32).to(device)\n",
    "v_bcL_train = torch.tensor(v_bcL_train, dtype=torch.float32).to(device)\n",
    "p_bcL_train = torch.tensor(p_bcL_train, dtype=torch.float32).to(device)\n",
    "\n",
    "rho_bcR_train = torch.tensor(rho_bcR_train, dtype=torch.float32).to(device)\n",
    "u_bcR_train = torch.tensor(u_bcR_train, dtype=torch.float32).to(device)\n",
    "v_bcR_train = torch.tensor(v_bcR_train, dtype=torch.float32).to(device)\n",
    "p_bcR_train = torch.tensor(p_bcR_train, dtype=torch.float32).to(device)\n",
    "\n",
    "x_bcD_train = torch.tensor(x_bcD_train,requires_grad=True, dtype=torch.float32).to(device)\n",
    "x_bcU_train = torch.tensor(x_bcU_train, requires_grad=True, dtype=torch.float32).to(device)\n",
    "x_bcR_train = torch.tensor(x_bcR_train, requires_grad=True, dtype=torch.float32).to(device)\n",
    "x_bcL_train = torch.tensor(x_bcL_train, dtype=torch.float32).to(device)\n",
    "x_bcI_train = torch.tensor(x_bcI_train, requires_grad=True, dtype=torch.float32).to(device)\n",
    "sin_bcI_train = torch.tensor(sin_bcI_train, dtype=torch.float32).to(device)\n",
    "cos_bcI_train = torch.tensor(cos_bcI_train, dtype=torch.float32).to(device)\n",
    "\n",
    "rho_ic_train = torch.tensor(rho_ic_train, dtype=torch.float32).to(device)\n",
    "u_ic_train = torch.tensor(u_ic_train, dtype=torch.float32).to(device)\n",
    "v_ic_train = torch.tensor(v_ic_train, dtype=torch.float32).to(device)\n",
    "p_ic_train = torch.tensor(p_ic_train, dtype=torch.float32).to(device)\n",
    "x_ic_train = torch.tensor(x_ic_train, dtype=torch.float32).to(device)\n",
    "\n",
    "\n",
    "model = DNN().to(device)\n",
    "\n",
    "print('Start training...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 66.00 MiB (GPU 0; 10.91 GiB total capacity; 6.10 GiB already allocated; 62.94 MiB free; 6.29 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m tic \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m----> 4\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m toc \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotal training time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtoc \u001b[38;5;241m-\u001b[39m tic\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Optimize loss function\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m loss_value \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(loss, \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m loss\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Print total loss\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/optim/optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/grad_mode.py:28\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m():\n\u001b[0;32m---> 28\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/optim/adam.py:92\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[0;32m---> 92\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups:\n\u001b[1;32m     95\u001b[0m     params_with_grad \u001b[38;5;241m=\u001b[39m []\n",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36mtrain.<locals>.closure\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclosure\u001b[39m():\n\u001b[1;32m     15\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()                                                     \n\u001b[0;32m---> 16\u001b[0m     loss_pde \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_pde\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_int_train\u001b[49m\u001b[43m)\u001b[49m                                   \n\u001b[1;32m     17\u001b[0m     loss_ic \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mloss_ic(x_ic_train, rho_ic_train,u_ic_train,v_ic_train,p_ic_train)   \n\u001b[1;32m     18\u001b[0m     loss_bdL \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mloss_bc(x_bcL_train, rho_bcL_train,u_bcL_train,v_bcL_train,p_bcL_train)   \n",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36mDNN.loss_pde\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    229\u001b[0m dg1_g \u001b[38;5;241m=\u001b[39m gradients(g1, x)[\u001b[38;5;241m0\u001b[39m]                                  \n\u001b[1;32m    230\u001b[0m g1_y \u001b[38;5;241m=\u001b[39m dg1_g[:, \u001b[38;5;241m2\u001b[39m:\u001b[38;5;241m3\u001b[39m]\n\u001b[0;32m--> 231\u001b[0m dg2_g \u001b[38;5;241m=\u001b[39m \u001b[43mgradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]                                  \n\u001b[1;32m    232\u001b[0m g2_y \u001b[38;5;241m=\u001b[39m dg2_g[:, \u001b[38;5;241m2\u001b[39m:\u001b[38;5;241m3\u001b[39m]\n\u001b[1;32m    233\u001b[0m dg3_g \u001b[38;5;241m=\u001b[39m gradients(g3, x)[\u001b[38;5;241m0\u001b[39m]                                  \n",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36mgradients\u001b[0;34m(outputs, inputs)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgradients\u001b[39m(outputs, inputs):\n\u001b[0;32m---> 37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mgrad_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:234\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    232\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m--> 234\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 0; 10.91 GiB total capacity; 6.10 GiB already allocated; 62.94 MiB free; 6.29 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "tic = time.time()\n",
    "for epoch in range(1, epochs+1):\n",
    "    train(epoch)\n",
    "toc = time.time()\n",
    "print(f'Total training time: {toc - tic}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0001\n",
    "optimizer = torch.optim.LBFGS(model.parameters(),lr=1,max_iter=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "tic = time.time()\n",
    "for epoch in range(1, epochs+1):\n",
    "    train(epoch)\n",
    "toc = time.time()\n",
    "print(f'Total training time: {toc - tic}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "device1 = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nd = 600\n",
    "T = 1.0\n",
    "t = np.linspace(T, T, 1)                                   # Partitioned spatial axis\n",
    "x = np.linspace(0.0,Lx,Nd)                                   # Partitioned spatial axis\n",
    "y = np.linspace(0.0,Ly,Nd)                                   # Partitioned spatial axis\n",
    "t_grid,x_grid,y_grid = np.meshgrid(t,x,y)                                    # (t,x) in [0,0.2]x[a,b]\n",
    "T = t_grid.flatten()[:, None]                                         # Vectorized x_grid\n",
    "X = x_grid.flatten()[:, None]                                         # Vectorized x_grid\n",
    "Y = y_grid.flatten()[:, None]                                         # Vectorized x_grid\n",
    "x_test = np.hstack((T,X,Y))                                            # Vectorized whole domain\n",
    "x_test = torch.tensor(x_test, dtype=torch.float32).to(device)\n",
    "u = to_numpy(model(x_test))\n",
    "\n",
    "\n",
    "x_test = np.hstack((T,X,Y))                                            # Vectorized whole domain\n",
    "ue = np.zeros((Nd,Nd))\n",
    "for j in range(0,Nd):\n",
    "    for i in range(0,Nd):\n",
    "        ue[i,j] = u[i*Nd+j,0]\n",
    "        x1 = x_test[i*Nd+j,1] \n",
    "        y1 = x_test[i*Nd+j,2] \n",
    "        if ((x1 - rx)**2 +(y1-ry)**2< rd**2):\n",
    "            ue[i,j] = 0.0\n",
    "            \n",
    "uo1 = ue.flatten()[:,None]\n",
    "ue = np.zeros((Nd,Nd))\n",
    "for j in range(0,Nd):\n",
    "    for i in range(0,Nd):\n",
    "        ue[i,j] = u[i*Nd+j,1]\n",
    "        x1 = x_test[i*Nd+j,1] \n",
    "        y1 = x_test[i*Nd+j,2] \n",
    "        if ((x1 - rx)**2 +(y1-ry)**2< rd**2):\n",
    "            ue[i,j] = 0.0\n",
    "uo2 = ue.flatten()[:,None]\n",
    "ue = np.zeros((Nd,Nd))\n",
    "for j in range(0,Nd):\n",
    "    for i in range(0,Nd):\n",
    "        ue[i,j] = u[i*Nd+j,2]\n",
    "        x1 = x_test[i*Nd+j,1] \n",
    "        y1 = x_test[i*Nd+j,2] \n",
    "        if ((x1 - rx)**2 +(y1-ry)**2< rd**2):\n",
    "            ue[i,j] = 0.0\n",
    "        \n",
    "uo3 = ue.flatten()[:,None]\n",
    "ue = np.zeros((Nd,Nd))\n",
    "for j in range(0,Nd):\n",
    "    for i in range(0,Nd):\n",
    "        ue[i,j] = u[i*Nd+j,0]\n",
    "        x1 = x_test[i*Nd+j,1] \n",
    "        y1 = x_test[i*Nd+j,2] \n",
    "        if ((x1 - rx)**2 +(y1-ry)**2< rd**2):\n",
    "            ue[i,j] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.contourf(x_grid[:,0,:],y_grid[:,0,:],ue,60)\n",
    "ax = plt.gca()\n",
    "ax.set_aspect(1)\n",
    "plt.show()\n",
    "plt.savefig(\"Riemann2.pdf\", dpi=150)\n"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
