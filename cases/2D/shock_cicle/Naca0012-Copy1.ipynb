{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NACA0012"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 无量纲化\n",
    "来流条件：\n",
    "$$p_0 = 75446 Pa ,\\quad \\rho_0 = 1.248 kg/m^3, \\quad u_0 = 203.65 m/s, \\quad Ma = 0.7, \\quad T = 283.24 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ L_\\inf = 1m,\\quad p^* = \\frac{p}{\\rho_\\infty u_\\infty^2} = 1.458, u_\\infty = 1,\\rho_\\infty = 1 , T^* = T/T_{\\infty}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mu^* = \\frac{\\mu}{\\mu_\\infty}, \\quad T^* = \\frac{T}{T_\\infty}, \\quad e^* = \\frac{e}{v_\\infty}, Re = \\frac{\\rho_\\infty u_\\infty L}{\\mu_\\infty}, M = \\frac{u_\\infty}{\\sqrt{\\gamma RT_\\infty}}, Pr = \\frac{c_p \\mu}{k} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ p^* = (\\gamma -1) \\rho^* e^*, T^* = \\frac{\\gamma M^2 p^*}{\\rho^*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NS equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial {\\bf u}}{\\partial t} + \\frac{\\partial { \\bf f}}{\\partial x} +\\frac{\\partial { \\bf g }}{\\partial y} = \\frac{\\partial {\\bf s_x}}{\\partial x} + \\frac{\\partial {\\bf s_y}}{\\partial y} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$${\\bf u} = (\\rho,\\rho u,\\rho v, E)^T$$, $$ {\\bf f} = (\\rho u, \\rho u^2,\\rho u v,(E+p)u)^T $$\n",
    "$${\\bf g} = (\\rho v, \\rho u v,\\rho v^2+p, (E+p)v)^T$$\n",
    "$${\\bf s_x} = \\frac{1}{Re}(0,\\tau_{xx},\\tau_{xy},\\tau_{xx}u+\\tau_{xy}v-q_x)^T$$\n",
    "$${\\bf s_y} = \\frac{1}{Re}(0,\\tau_{yx},\\tau_{yy},\\tau_{yx}u+\\tau_{yy}v-q_y)^T$$\n",
    "\n",
    "$$ E = \\rho e + \\frac{1}{2}\\rho(u^2 +v^2) $$\n",
    "$$ e = \\frac{1}{\\gamma -1} \\frac{p}{\\rho}$$\n",
    "$$ p = \\rho R T$$\n",
    "$$\\tau_{xx} = 2/3\\mu(2u_x - v_y)$$\n",
    "$$\\tau_{yy} = 2/3\\mu(2v_y - u_x)$$\n",
    "$$\\tau_{xy} =\\tau_{yx} = \\mu (u_y + v_x)$$\n",
    "$$q_x = -\\frac{c_p \\mu}{Pr Re}\\frac{\\partial T}{\\partial x}$$\n",
    "$$q_y = -\\frac{c_p \\mu}{Pr Re}\\frac{\\partial T}{\\partial y}$$\n",
    "\n",
    "$$ p = \\rho R T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 简化\n",
    "$$ U2_t = (\\rho u)_t = \\rho_t u+u_t \\rho$$\n",
    "$$ p_t = \\rho_t R T + \\rho R T_t $$\n",
    "$$ U3_t = E_t = 1/(\\gamma -1) p_t + 1/2 \\rho_t (u^2 + v^2) + \\rho (uu_t + vv_t) $$ \n",
    "$$ F1_x = \\rho_x u + \\rho u_x $$\n",
    "$$ F2_x = \\rho_x u^2 + 2\\rho u u_x$$\n",
    "$$ F3_x = \\rho_x uv + \\rho u v_x + \\rho u_x v$$\n",
    "$$ F4_x = (E_x +p_x) u + (E + p) u_x v$$\n",
    "\n",
    "$$ S1_{xx} = 0 $$\n",
    "$$ S2_{xx} = \\frac{1}{Re} \\left(2/3\\mu_x(2u_x -v_y) + 2/3 \\mu (2u_{xx} -v_{xy})\\right) $$\n",
    "$$ S3_{xx} = \\frac{1}{Re} \\left(\\mu_x(u_y +v_x) + \\mu (u_{xy} +v_{xx})\\right) $$\n",
    "$$ S4_{xx} =  S2_{xx} u + S3_{xx} v + \\tau_{xx} u_x +\\tau_{xy}v_x + q_{xx}  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ q_{xx} = \\frac{c_P}{Pr Re}  (\\mu_x T_x + \\mu T_{xx})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 粘性系数\n",
    "\n",
    "$$\\mu = \\frac{\\mu_0}{T_0^{3/2}} T^{3/2}\\frac{T_0+S}{T+S}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mu_x = \\frac{\\mu_0(T_0 +S) }{T_0^{3/2}} \\frac{\\sqrt{T}(T+3S)}{2(T+S)^2} T_x$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 非守恒NS\n",
    "\n",
    "$$\\frac{\\partial \\rho}{\\partial t} + u \\frac{\\partial \\rho}{\\partial x} + v{\\partial \\rho}{\\partial y} + \\rho \\Delta = 0 $$\n",
    "$$ \\rho(\\frac{\\partial u}{\\partial t} + u\\frac{\\partial u}{\\partial x} + u\\frac{\\partial u}{\\partial x} + \\frac{\\partial p}{\\partial x} = \\frac{\\mu}{\\Re}(4/3 v_{yy} - 2/3 u_{xy} +v_{xy}+u_{yy}+4/3 u_{xx}-2/3v_{xy}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ y= \\pm 0.594689181 \\cdot (0.298222773 \\cdot \\sqrt{x} - 0.127125232 \\cdot x \\\\ - 0.357907906 \\cdot x^2 + 0.291984971 \\cdot x^3 - 0.105174606 \\cdot x^4 )\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Naca0012data(x):\n",
    "    a = 0.594689181\n",
    "    b = 0.298222773  \n",
    "    c = 0.127125232 \n",
    "    d = 0.357907906 \n",
    "    e = 0.291984971 \n",
    "    f = 0.105174606 \n",
    "    y1 = a*(b*np.sqrt(x) - c*x-d*x**2+e*x**3 - f*x**4)\n",
    "    y2 = -y1\n",
    "    dy1 =  a*(0.5*b/np.sqrt(x) - c - 2*d*x +3*e*x**2 - 4*f*x**3)\n",
    "    dy2 = -a*(0.5*b/np.sqrt(x) - c - 2*d*x +3*e*x**2 - 4*f*x**3)\n",
    "    return y1,y2,dy1,dy2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import time\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import arange, meshgrid\n",
    "#\n",
    "from smt.sampling_methods import LHS\n",
    "# Seeds\n",
    "torch.manual_seed(123)\n",
    "np.random.seed(123)\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    def closure():\n",
    "        optimizer.zero_grad()                                                     # Optimizer\n",
    "        loss_pde = model.loss_pde(x_int_train)                                    # Loss function of PDE\n",
    "        loss_ic = model.loss_ic(x_ic_train, rho_ic_train,u_ic_train,v_ic_train,T_ic_train)   # Loss function of IC\n",
    "        loss_bdL = model.loss_bc(x_bcL_train, rho_bcL_train,u_bcL_train,v_bcL_train,T_bcL_train)   # Loss function of IC\n",
    "       # loss_bdR = model.loss_bc1(x_bcR_train,rho_bcR_train,u_bcR_train,v_bcR_train,p_bcR_train)   # Loss function of IC\n",
    "        #loss_cut = model.loss_bc1(x_cut_train,rho_cut_train,u_cut_train,v_cut_train,p_cut_train) \n",
    "        loss_bdI = model.bd_B(x_bcI_train, sin_bcI_train,cos_bcI_train)  \n",
    "\n",
    "        loss_ib = loss_ic  +  loss_bdI #+loss_bdL\n",
    "        loss = loss_pde + 10*loss_ib\n",
    "\n",
    "        # Print iteration, loss of PDE and ICs\n",
    "        print(f'epoch {epoch} loss_pde:{loss_pde:.8f}, loss_ib:{loss_ib:.8f}')\n",
    "        loss.backward()\n",
    "        return loss\n",
    "\n",
    "    # Optimize loss function\n",
    "    loss = optimizer.step(closure)\n",
    "    loss_value = loss.item() if not isinstance(loss, float) else loss\n",
    "    # Print total loss\n",
    "    print(f'epoch {epoch}: loss {loss_value:.6f}')\n",
    "    \n",
    "# Calculate gradients using torch.autograd.grad\n",
    "def gradients(outputs, inputs):\n",
    "    return torch.autograd.grad(outputs, inputs,grad_outputs=torch.ones_like(outputs), create_graph=True)\n",
    "\n",
    "# Convert torch tensor into np.array\n",
    "def to_numpy(input):\n",
    "    if isinstance(input, torch.Tensor):\n",
    "        return input.detach().cpu().numpy()\n",
    "    elif isinstance(input, np.ndarray):\n",
    "        return input\n",
    "    else:\n",
    "        raise TypeError('Unknown type of input, expected torch.Tensor or ' \\\n",
    "                        'np.ndarray, but got {}'.format(type(input)))\n",
    "def IC(x):\n",
    "    N =x.shape[0]\n",
    "    rho_init = np.zeros((x.shape[0]))                                              # rho - initial condition\n",
    "    u_init = np.zeros((x.shape[0]))                                                # u - initial condition\n",
    "    v_init = np.zeros((x.shape[0]))                                                # u - initial condition\n",
    "    T_init = np.zeros((x.shape[0]))                                                # p - initial condition\n",
    "    \n",
    "    gamma = 1.4\n",
    "    rho1 = 1.0\n",
    "    p1 =  1.458\n",
    "    v1 = 0.0\n",
    "    u1 = 1.0\n",
    "    T1 = 1.0\n",
    "    \n",
    "    rho2 = 1.0\n",
    "    p2 = 1.\n",
    "    v2 = 0.0\n",
    "    #u1 = ms*npsqrt(gamma)\n",
    "    u2 = 0.0\n",
    "    # rho, p - initial condition\n",
    "    for i in range(N):\n",
    "        rho_init[i] = rho1\n",
    "        u_init[i] =   u1\n",
    "        v_init[i] =  v1\n",
    "        T_init[i] =  T1\n",
    "    return rho_init, u_init, v_init,T_init\n",
    "\n",
    "def BC_L(x):\n",
    "    N =x.shape[0]\n",
    "    rho_init = np.zeros((x.shape[0]))                                              # rho - initial condition\n",
    "    u_init = np.zeros((x.shape[0]))                                                # u - initial condition\n",
    "    v_init = np.zeros((x.shape[0]))                                                # u - initial condition\n",
    "    T_init = np.zeros((x.shape[0]))                                                # p - initial condition\n",
    "    \n",
    "    gamma = 1.4\n",
    "    #u1 = ms*npsqrt(gamma)\n",
    "    # rho, p - initial condition\n",
    "    rho1 = 1.00\n",
    "    T1 =  1.0\n",
    "    v1 = 0.0\n",
    "    u1 = 1.0\n",
    "    for i in range(N):\n",
    "        rho_init[i] = rho1\n",
    "        u_init[i] =  u1\n",
    "        v_init[i] =  v1\n",
    "        T_init[i] =  T1\n",
    "    return rho_init, u_init, v_init,T_init\n",
    "def BC_R(x):\n",
    "    N =x.shape[0]\n",
    "    rho_init = np.zeros((x.shape[0]))                                              # rho - initial condition\n",
    "    u_init = np.zeros((x.shape[0]))                                                # u - initial condition\n",
    "    v_init = np.zeros((x.shape[0]))                                                # u - initial condition\n",
    "    p_init = np.zeros((x.shape[0]))                                                # p - initial condition\n",
    "    \n",
    "    gamma = 1.4\n",
    "    ms = 2.0\n",
    "    rho1 = 1.0\n",
    "    p1 = 1.0\n",
    "    v1 = 0.0\n",
    "    u1 = 0\n",
    "    # rho, p - initial condition\n",
    "    for i in range(N):\n",
    "        rho_init[i] = rho1\n",
    "        u_init[i] = u1\n",
    "        v_init[i] = v1\n",
    "        p_init[i] = p1\n",
    "\n",
    "    return rho_init, u_init, v_init,p_init\n",
    "def BC_Cut(x):\n",
    "    N =x.shape[0]\n",
    "    rho_init = np.zeros((x.shape[0]))\n",
    "    u_init = np.zeros((x.shape[0]))\n",
    "    v_init = np.zeros((x.shape[0]))\n",
    "    p_init = np.zeros((x.shape[0]))\n",
    "    \n",
    "    gamma = 1.4\n",
    "    ms = 2.0\n",
    "    rho1 = 1.0\n",
    "    p1 = 1.0\n",
    "    v1 = 0.0\n",
    "    u1 = 0\n",
    "    # rho, p - initial condition\n",
    "    for i in range(N):\n",
    "        rho_init[i] = 10.01\n",
    "        u_init[i] =  0\n",
    "        v_init[i] = 0\n",
    "        p_init[i] = 10.01\n",
    "\n",
    "    return rho_init, u_init, v_init,p_init\n",
    "    \n",
    "class DNN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DNN, self).__init__()\n",
    "        self.net = nn.Sequential()                                                  # Define neural network\n",
    "        self.net.add_module('Linear_layer_1', nn.Linear(3, 90))                     # First linear layer\n",
    "        self.net.add_module('Tanh_layer_1', nn.Tanh())                              # First activation Layer\n",
    "\n",
    "        for num in range(2, 6):                                                     # Number of layers (2 through 7)\n",
    "            self.net.add_module('Linear_layer_%d' % (num), nn.Linear(90, 90))       # Linear layer\n",
    "            self.net.add_module('Tanh_layer_%d' % (num), nn.Tanh())                 # Activation Layer\n",
    "        self.net.add_module('Linear_layer_final', nn.Linear(90, 4))                 # Output Layer\n",
    "\n",
    "    # Forward Feed\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "    def bd_B(self,x,sin,cos):\n",
    "        yb = self.net(x)\n",
    "        rhob,Tb,ub,vb = yb[:, 0:1], yb[:, 1:2], yb[:, 2:3],yb[:,3:]\n",
    "        drhob_g = gradients(rhob, x)[0]                                      # Gradient [u_t, u_x]\n",
    "        rhob_x, rhob_y = drhob_g[:, 1:2], drhob_g[:, 2:3]                            # Partial derivatives u_t, u_x\n",
    "        dub_g = gradients(ub, x)[0]                                      # Gradient [u_t, u_x]\n",
    "        ub_x, ub_y = dub_g[:, 1:2], dub_g[:, 2:3]                            # Partial derivatives u_t, u_x\n",
    "        dvb_g = gradients(vb, x)[0]                                      # Gradient [u_t, u_x]\n",
    "        vb_x, vb_y = dvb_g[:, 1:2], dvb_g[:, 2:3]                            # Partial derivatives u_t, u_x\n",
    "        dTb_g = gradients(Tb, x)[0]                                      # Gradient [p_t, p_x]\n",
    "        Tb_x, Tb_y = dTb_g[:, 1:2], dTb_g[:, 2:3]                            # Partial derivatives p_t, p_x\n",
    "        \n",
    "        deltau = ub_x + vb_y\n",
    "        lam = 0.1*(abs(deltau) - deltau) + 1\n",
    "        #lam = (deltau) - deltau) + 1\n",
    "        \n",
    "        fb = (((ub**2 + vb**2)/lam)**2).mean() +\\\n",
    "            (((Tb_x*cos + Tb_y*sin)/lam)**2).mean() +\\\n",
    "            (((rhob_x*cos + rhob_y*sin)/lam)**2).mean()\n",
    "        return fb\n",
    "    def bd_OY(self,x):\n",
    "        y = self.net(x)\n",
    "        rho,p,u,v = y[:, 0:1], y[:, 1:2], y[:, 2:3],y[:,3:]\n",
    "        \n",
    "        drho_g = gradients(rho, x)[0]                                  # Gradient [rho_t, rho_x]\n",
    "        rho_x,rho_y = drho_g[:, :1], drho_g[:, 1:2]                    # Partial derivatives rho_t, rho_x\n",
    "        du_g = gradients(u, x)[0]                                      # Gradient [u_t, u_x]\n",
    "        u_x, u_y = du_g[:, :1], du_g[:, 1:2]                            # Partial derivatives u_t, u_x\n",
    "        dv_g = gradients(v, x)[0]                                      # Gradient [u_t, u_x]\n",
    "        v_x, v_y = dv_g[:, :1], dv_g[:, 1:2]                            # Partial derivatives u_t, u_x\n",
    "        dp_g = gradients(p, x)[0]                                      # Gradient [p_t, p_x]\n",
    "        p_x, p_y = dp_g[:, :1], dp_g[:, 1:2]                            # Partial derivatives p_t, p_x\n",
    "        \n",
    "        deltau = u_x + v_y\n",
    "        lam = 0.1*(abs(deltau) - deltau) + 1\n",
    "        \n",
    "        f = ((( u_y)/lam)**2).mean() +\\\n",
    "            ((( v_y)/lam)**2).mean() +\\\n",
    "            ((( p_y)/lam)**2).mean() +\\\n",
    "            ((( rho_y)/lam)**2).mean()\n",
    "        return f\n",
    "    \n",
    "    def bd_OX(self,x):\n",
    "        y = self.net(x)\n",
    "        rho,p,u,v = y[:, 0:1], y[:, 1:2], y[:, 2:3],y[:,3:]\n",
    "        \n",
    "        drho_g = gradients(rho, x)[0]                                  # Gradient [rho_t, rho_x]\n",
    "        rho_x,rho_y = drho_g[:, :1], drho_g[:, 1:2]                    # Partial derivatives rho_t, rho_x\n",
    "        du_g = gradients(u, x)[0]                                      # Gradient [u_t, u_x]\n",
    "        u_x, u_y = du_g[:, :1], du_g[:, 1:2]                            # Partial derivatives u_t, u_x\n",
    "        dv_g = gradients(v, x)[0]                                      # Gradient [u_t, u_x]\n",
    "        v_x, v_y = dv_g[:, :1], dv_g[:, 1:2]                            # Partial derivatives u_t, u_x\n",
    "        dp_g = gradients(p, x)[0]                                      # Gradient [p_t, p_x]\n",
    "        p_x, p_y = dp_g[:, :1], dp_g[:, 1:2]                            # Partial derivatives p_t, p_x\n",
    "        \n",
    "        deltau = u_x + v_y\n",
    "        lam = 0.1*(abs(deltau) - deltau) + 1\n",
    "        \n",
    "        f = ((( u_x)/lam)**2).mean() +\\\n",
    "            ((( v_x)/lam)**2).mean() +\\\n",
    "            ((( p_x)/lam)**2).mean() +\\\n",
    "            ((( rho_x)/lam)**2).mean()\n",
    "        return f\n",
    "     \n",
    "    # Loss function for PDE\n",
    "    def loss_pde(self, x):\n",
    "        \n",
    "        y = self.net(x)\n",
    "        gamma = 1.4                                                    # Heat Capacity Ratio\n",
    "        epsilon = 1e-5\n",
    "        rho,T,u,v = y[:, 0:1], y[:, 1:2], y[:, 2:3],y[:,3:]\n",
    "        \n",
    "        drho_g = gradients(rho,x)[0]\n",
    "        rho_t,rho_x,rho_y = drho_g[:, :1],drho_g[:,1:2],drho_g[:,2:3]\n",
    "        dT_g = gradients(T,x)[0]\n",
    "        T_t,T_x,T_y = dT_g[:, :1],dT_g[:,1:2],dT_g[:,2:3]\n",
    "        du_g = gradients(u,x)[0]\n",
    "        u_t,u_x,u_y = du_g[:, :1],du_g[:,1:2],du_g[:,2:3]\n",
    "        dv_g = gradients(v,x)[0]\n",
    "        v_t,v_x,v_y = dv_g[:, :1],dv_g[:,1:2],dv_g[:,2:3]\n",
    "        dux_g = gradients(u_x,x)[0]\n",
    "        u_xx,u_xy = dux_g[:,1:2],dux_g[:,2:3]\n",
    "        duy_g = gradients(u_y,x)[0]\n",
    "        u_yy = duy_g[:,2:3]\n",
    "        dvx_g = gradients(v_x,x)[0]\n",
    "        v_xx,v_xy = dvx_g[:,1:2],dvx_g[:,2:3]\n",
    "        dvy_g = gradients(v_y,x)[0]\n",
    "        v_yy = dvy_g[:,2:3]\n",
    "        dTx_g = gradients(T_x,x)[0]\n",
    "        T_xx = dvy_g[:,1:2]\n",
    "        dTy_g = gradients(T_y,x)[0]\n",
    "        T_yy = dvy_g[:,2:3]\n",
    "        \n",
    "        \n",
    "        Ma = 0.7\n",
    "        gamma = 1.4\n",
    "        Re = 1e6\n",
    "        Pr = 0.72\n",
    "        R = 287\n",
    "        T0 = 283.24\n",
    "        cp = 1/(gamma-1)/Ma**2\n",
    "        \n",
    "        \n",
    "        p = 1/gamma/Ma**2 *rho *T\n",
    "        e =  p/(gamma - 1)\n",
    "        E = rho*e + 0.5*rho*(u**2+v**2)\n",
    "        mu =  T**(3/2)/(T+110/T0)*(1+110/T0)/Re\n",
    "        \n",
    "        \n",
    "        p_t = 1/(gamma*Ma**2)*(rho_t*T+T_t*rho)\n",
    "        p_x = 1/(gamma*Ma**2)*(rho_x*T+T_x*rho)\n",
    "        p_y = 1/(gamma*Ma**2)*(rho_y*T+T_y*rho)\n",
    "       # E_x = rho_x*e + rho*p_x/(gamma -1) + 0.5*rho_x*(u**2+v**2) + \\\n",
    "       #        rho*(u_x*u + v_x*v)\n",
    "       # E_y = rho_y*e + rho*p_y/(gamma -1) + 0.5*rho_y*(u**2+v**2) + \\\n",
    "       #        rho*(u_y*u + v_y*v)\n",
    "        mu_x = 0.5*(1+110/T0)*(T**(3/2)+3*T**(1/2)*110/T0)/(T+110/T0)**2/Re*T_x\n",
    "        mu_y = 0.5*(1+110/T0)*(T**(3/2)+3*T**(1/2)*110/T0)/(T+110/T0)**2/Re*T_y\n",
    "        \n",
    "      #  U1_t = rho_t\n",
    "      #  U2_t = rho_t*u + rho*u_t\n",
    "      #  U3_t = rho_t*v + rho*v_t\n",
    "      #  U4_t = rho_t*e + rho*p_t/(gamma -1) + 0.5*rho_t*(u**2+v**2) + \\\n",
    "      #         rho*(u_t*u + v_t*v)\n",
    "      #      \n",
    "      #  \n",
    "      #  f1_x = rho_x*u + u_x*rho\n",
    "      #  f2_x = rho_x*u**2 + 2*rho*u*u_x + p_x\n",
    "      #  f3_x = rho_x*u*v + rho*u_x*v + rho*u*v_x\n",
    "      #  f4_x = u_x*(E+p) +u*(E_x + p_x)\n",
    "      #  \n",
    "      #  g1_y = rho_y*v + v_y*rho\n",
    "      #  g2_y = rho_y*u*v + rho*u_y*v + rho*u*v_y\n",
    "      #  g3_y = rho_y*v**2 + 2*rho*v*v_y + p_y\n",
    "      #  g4_y = v_y*(E+p) +v*(E_y + p_y)\n",
    "      #  \n",
    "      #  sx1_x = 0\n",
    "      #  sx2_x = 2/3*mu_x*(2*u_x-v_y) +2/3*mu*(2*u_xx -v_xy)\n",
    "      #  sx3_x = mu_x*(u_y+v_x) +mu*(u_xy+v_xx)\n",
    "      #  sx4_x = sx2_x*u + 2/3*mu*(2*u_x-v_y)*u_x + sx3_x*v + mu*(u_y+v_x)*v_x+\\\n",
    "      #          cp/Pr*mu_x*T_x + cp/Pr*mu*T_xx\n",
    "      #  \n",
    "      #  sy1_y = 0\n",
    "      #  sy2_y = mu_y*(u_y+v_x) +mu*(u_yy+v_xy)\n",
    "      #  sy3_y = 2/3*mu_y*(2*v_y-u_x) +2/3*mu*(2*v_yy -u_xy)\n",
    "      #  sy4_y = sy2_y*u + mu*(u_y+v_x)*u_y + sy3_y*v + 2/3*mu*(2*v_y-u_x)*v_y+\\\n",
    "      #          cp/Pr*mu_y*T_y + cp/Pr*mu*T_yy\n",
    "        \n",
    "        \n",
    "        deltau = u_x + v_y\n",
    "        nab = abs(deltau) - deltau            \n",
    "        lam = 0.1*nab + 1\n",
    "        \n",
    "       # f = (((U1_t + f1_x+g1_y-sx1_x-sy1_y )/lam)**2).mean() +\\\n",
    "       #     (((U2_t + f2_x+g2_y-sx2_x-sy2_y )/lam)**2).mean() +\\\n",
    "       #     (((U3_t + f3_x+g3_y-sx3_x-sy3_y )/lam)**2).mean() +\\\n",
    "       #     (((U4_t + f4_x+g4_y-sx4_x-sy4_y )/lam)**2).mean()\n",
    "        \n",
    "        f = (((rho_t + (rho_x*u+rho_y*v) + rho*deltau)/lam)**2).mean() +\\\n",
    "             (((rho*(u_t +u_y*v + u_x*u)+p_x \\\n",
    "             -mu_x*(4/3*u_x-2/3*v_y)-mu_y*(u_y+v_x)\\\n",
    "             -mu*(4/3*u_xx +1/3*v_xy +u_yy))/lam)**2).mean() +\\\n",
    "             ((( rho*(v_t+u*v_x + v*v_y)+p_y \\\n",
    "             -mu_x*(u_y+v_x)- 2/3*mu_y*(2*v_y-u_x)\\\n",
    "             -mu*(1/3*u_xy+v_xx + 4/3*v_yy))/lam)**2).mean() + \\\n",
    "             (((rho*T_t +rho*(u*T_x + v*T_y)+(gamma-1)*T*deltau - \\\n",
    "             (gamma-1)*gamma*Ma**2*mu*(1/3*(deltau)**2 + (u_x)**2+(u_y)**2+(v_x)**2 + (v_y)**2)\\\n",
    "             -gamma/Pr*(mu_x*T_x+mu*T_xx+mu_y*T_y+mu*T_yy))/lam)**2).mean()\n",
    "        return f\n",
    "\n",
    "    # Loss function for initial condition\n",
    "    def loss_ic(self, x_ic, rho_ic, u_ic, v_ic,T_ic):\n",
    "        U_ic = self.net(x_ic)                                                      # Initial condition\n",
    "        rho_ic_nn, T_ic_nn,u_ic_nn,v_ic_nn = U_ic[:, 0], U_ic[:, 1], U_ic[:, 2],U_ic[:,3]            # rho, u, p - initial condition\n",
    "\n",
    "        # Loss function for the initial condition\n",
    "        loss_ics = ((u_ic_nn - u_ic) ** 2).mean() + \\\n",
    "               ((rho_ic_nn- rho_ic) ** 2).mean()  + \\\n",
    "               ((T_ic_nn - T_ic) ** 2).mean() +\\\n",
    "               ((v_ic_nn - v_ic) ** 2).mean()\n",
    "\n",
    "        return loss_ics\n",
    "\n",
    "    def loss_bc(self, x_ic, rho_ic, u_ic, v_ic,T_ic):\n",
    "        U_ic = self.net(x_ic)                                                      # Initial condition\n",
    "        rho_ic_nn, T_ic_nn,u_ic_nn,v_ic_nn = U_ic[:, 0], U_ic[:, 1], U_ic[:, 2],U_ic[:,3]            # rho, u, p - initial condition\n",
    "\n",
    "        # Loss function for the initial condition\n",
    "        loss_ics = ((u_ic_nn - u_ic) ** 2).mean() + \\\n",
    "               ((rho_ic_nn- rho_ic) ** 2).mean()  + \\\n",
    "               ((T_ic_nn - T_ic) ** 2).mean() +\\\n",
    "               ((v_ic_nn - v_ic) ** 2).mean()\n",
    "\n",
    "        return loss_ics\n",
    "    def loss_bc1(self, x_ic, rho_ic, u_ic, v_ic,p_ic):\n",
    "        U_ic = self.net(x_ic)                                                      # Initial condition\n",
    "        rho_ic_nn, p_ic_nn,u_ic_nn,v_ic_nn = U_ic[:, 0], U_ic[:, 1], U_ic[:, 2],U_ic[:,3]            # rho, u, p - initial condition\n",
    "\n",
    "        # Loss function for the initial condition\n",
    "        loss_ics = ((rho_ic_nn- rho_ic) ** 2).mean()  + \\\n",
    "               ((p_ic_nn - p_ic) ** 2).mean() \n",
    "\n",
    "        return loss_ics\n",
    "\n",
    "def BD_naca0012(t,xb,yb,n):\n",
    "    x = np.zeros((2*n,3)) \n",
    "    sin = np.zeros((2*n,1)) \n",
    "    cos = np.zeros((2*n,1)) \n",
    "\n",
    "    for i in range(n):\n",
    "        xd = np.random.rand()\n",
    "        yd1,yd2,dy1,dy2 = Naca0012data(xd)\n",
    "        \n",
    "        x[i,0] = np.random.rand()*t\n",
    "        x[i,1] = xb + xd\n",
    "        x[i,2] = yb  + yd1\n",
    "        cos[i,0] = -dy1/np.sqrt(dy1**2 + 1)\n",
    "        sin[i,0] =   1/np.sqrt(dy1**2 + 1)\n",
    "    for i in range(n):\n",
    "        xd = np.random.rand()\n",
    "        yd1,yd2,dy1,dy2 = Naca0012data(xd)\n",
    "        \n",
    "        x[i+n,0] = np.random.rand()*t\n",
    "        x[i+n,1] = xb + xd\n",
    "        x[i+n,2] = yb  + yd2\n",
    "        cos[i+n,0] = -dy2/np.sqrt(dy2**2 + 1)\n",
    "        sin[i+n,0] =  1/np.sqrt(dy2**2 + 1)\n",
    "    return x, sin,cos\n",
    "def BD_circle(t,xc,yc,r,n):\n",
    "    x = np.zeros((n,3)) \n",
    "    sin = np.zeros((n,1)) \n",
    "    cos = np.zeros((n,1)) \n",
    "\n",
    "    for i in range(n):\n",
    "        the = 2*np.random.rand()*np.pi\n",
    "        xd = np.cos(the + np.pi/2)\n",
    "        yd = np.sin(the + np.pi/2)\n",
    "        x[i,0] = np.random.rand()*t\n",
    "        x[i,1] = xc  + xd*r\n",
    "        x[i,2] = yc  + yd*r\n",
    "        cos[i,0] = xd \n",
    "        sin[i,0] = yd\n",
    "        #cos[i,0] = 1\n",
    "        #sin[i,0] = 0\n",
    "    return x, sin,cos\n",
    "\n",
    "def Pertur(x, dx):\n",
    "    N =x.shape[0]\n",
    "    xL = np.zeros((N,3))\n",
    "    xR = np.zeros((N,3))\n",
    "    xU = np.zeros((N,3))\n",
    "    xD = np.zeros((N,3))\n",
    "    \n",
    "    for i in range(N):\n",
    "        xL[i,0] = x[i,0]\n",
    "        xR[i,0] = x[i,0]\n",
    "        xU[i,0] = x[i,0]\n",
    "        xD[i,0] = x[i,0]\n",
    "        \n",
    "        \n",
    "        xL[i,1] = x[i,1] - dx\n",
    "        xR[i,1] = x[i,1] + dx\n",
    "        xU[i,1] = x[i,1]\n",
    "        xD[i,1] = x[i,1]\n",
    "        \n",
    "        xL[i,2] = x[i,2] \n",
    "        xR[i,2] = x[i,2]\n",
    "        xU[i,2] = x[i,2] + dx\n",
    "        xD[i,2] = x[i,2] - dx\n",
    "        \n",
    "    return xL,xR,xU,xD\n",
    "    \n",
    "    \n",
    "    \n",
    "def BD_BackCorner(t,n):\n",
    "    \n",
    "    x = np.zeros((n,3)) \n",
    "    x2 = np.zeros((n,3)) \n",
    "    sin = np.zeros((n,1)) \n",
    "    sin2 = np.zeros((n,1)) \n",
    "    cos = np.zeros((n,1)) \n",
    "    cos2 = np.zeros((n,1)) \n",
    "    \n",
    "    for i in range(n):\n",
    "        x[i,0] = np.random.rand()*t\n",
    "        x[i,1] = np.random.rand()*0.3 + 0.2\n",
    "        x[i,2] = 1.5\n",
    "        sin[i] = 1\n",
    "        cos[i] = 0\n",
    "    for i in range(n):\n",
    "        x2[i,0] = np.random.rand()*t\n",
    "        x2[i,1] = np.random.rand()*0.5\n",
    "        x2[i,2] = 1.5\n",
    "        sin2[i] = 1\n",
    "        cos2[i] = 0\n",
    "    x = np.vstack((x,x2))\n",
    "    sin = np.vstack((sin,sin2))\n",
    "    cos = np.vstack((cos,cos2))\n",
    "    \n",
    "    for i in range(n):\n",
    "        x2[i,0] = np.random.rand()*t\n",
    "        x2[i,1] = 0.5\n",
    "        x2[i,2] = np.random.rand()*1.5\n",
    "        sin2[i] = 0\n",
    "        cos2[i] = 1\n",
    "        \n",
    "    x = np.vstack((x,x2))\n",
    "    sin = np.vstack((sin,sin2))\n",
    "    cos = np.vstack((cos,cos2))\n",
    "    \n",
    "    for i in range(n):\n",
    "        x2[i,0] = np.random.rand()*t\n",
    "        x2[i,1] = 0.5\n",
    "        x2[i,2] = np.random.rand()*0.3 + 1.2\n",
    "        sin2[i] = 0\n",
    "        cos2[i] = 1\n",
    "        \n",
    "    x = np.vstack((x,x2))\n",
    "    sin = np.vstack((sin,sin2))\n",
    "    cos = np.vstack((cos,cos2))\n",
    "        \n",
    "    return x,sin,cos\n",
    "\n",
    "# Solve Euler equations using PINNs\n",
    "# def main():\n",
    "  # Initialization\n",
    "#device = torch.device('cuda')                                          # Run on CPU\n",
    "device = torch.device('cuda')                                          # Run on CPU\n",
    "lr = 0.001                                                           # Learning rate\n",
    "num_ib = 10000                                                # Random sampled points from IC0\n",
    "num_int = 50000                                                # Random sampled points in interior\n",
    "Tend = 0.4\n",
    "Lx = 2.0\n",
    "Ly = 1.0\n",
    "rx = 0.5\n",
    "ry = 0.50\n",
    "rd = 0.25\n",
    "\n",
    "\n",
    "xlimits = np.array([[0.,Tend],[0.0, Lx], [0,Ly]])  #interal\n",
    "sampling = LHS(xlimits=xlimits)\n",
    "x_int_train = sampling(num_int)\n",
    "\n",
    "#xlimits = np.array([[0.,Tend],[0.5, 2], [0.5,2]])  #interal\n",
    "#sampling = LHS(xlimits=xlimits)\n",
    "#x_int1_train = sampling(num_int)\n",
    "#x_int_train =  np.vstack((x_int_train,x_int1_train))\n",
    "\n",
    "A = []\n",
    "for i in range(num_int):\n",
    "    x = x_int_train[i,1]\n",
    "    y = x_int_train[i,2]\n",
    "    if ((x - rx)>0 and (x-rx)<1):\n",
    "        y1,y2,dy1,dy2 = Naca0012data(x-rx)\n",
    "        if ((y-ry)>y2 and (y-ry)<y1):\n",
    "            A.append(i)\n",
    "x_int_train = np.delete(x_int_train,A,axis=0)\n",
    "\n",
    "#xlimits = np.array([[0.0, Tend], [1.0, 4.0], [0.2,1.0]])\n",
    "#sampling = LHS(xlimits=xlimits)\n",
    "#x_int_train_add = sampling(3*num_int)\n",
    "#x_int_train = np.vstack((x_int_train,x_int_train_add))\n",
    "\n",
    "#x_intL_train,x_intR_train,x_intU_train,x_intD_train = Pertur(x_int_train, 0.01)\n",
    "\n",
    "xlimits = np.array([[0.,0.0],[0.0,Lx], [0.0,Ly]])  #interal\n",
    "sampling = LHS(xlimits=xlimits)\n",
    "x_ic_train = sampling(num_ib)\n",
    "A = []\n",
    "for i in range(num_ib):\n",
    "    x = x_ic_train[i,1]\n",
    "    y = x_ic_train[i,2]\n",
    "    if ((x - rx)>0 and (x-rx)<1):\n",
    "        y1,y2,dy1,dy2 = Naca0012data(x-rx)\n",
    "        if ((y-ry)>y2 and (y-ry)<y1):\n",
    "            A.append(i)\n",
    "x_ic_train = np.delete(x_ic_train,A,axis=0)\n",
    "\n",
    "\n",
    "#xlimits = np.array([[0.0, 0.0], [1.0, 4.0], [0.2,1.0]])\n",
    "#sampling = LHS(xlimits=xlimits)\n",
    "#x_ic_train_add = sampling(3*num_int)\n",
    "#x_ic_train = np.vstack((x_ic_train,x_ic_train_add))\n",
    "\n",
    "\n",
    "xlimits = np.array([[0.0,Tend],[0.0, 0.0], [0.0,Ly]])\n",
    "sampling = LHS(xlimits=xlimits)\n",
    "x_bcL_train =  sampling(num_ib)\n",
    "#\n",
    "#xlimits = np.array([[0.0,Tend], [Lx, Lx], [0.0,Ly]])\n",
    "#sampling = LHS(xlimits=xlimits)\n",
    "#x_bcR_train =  sampling(num_ib)\n",
    "#\n",
    "#\n",
    "#xlimits = np.array([[0.0,Tend],[0.0, Lx], [Ly,Ly]])\n",
    "#sampling = LHS(xlimits=xlimits)\n",
    "#x_bcU_train =  sampling(num_ib)\n",
    "#\n",
    "#xlimits = np.array([[0.0,Tend], [0.0, Lx], [0.0,0.0]])\n",
    "#sampling = LHS(xlimits=xlimits)\n",
    "#x_bcD_train =  sampling(num_ib)                                           # Vectorized whole domain\n",
    "\n",
    "#xlimits = np.array([[0.0,0.0],[0.0, Lx], [0.0,Ly]])\n",
    "#sampling = LHS(xlimits=xlimits)\n",
    "#x_ic_train =  sampling(num_ib)\n",
    "#A = []\n",
    "#for i in range(num_ib):\n",
    "#    x = x_ic_train[i,1]\n",
    "#    y = x_ic_train[i,2]\n",
    "#    if ((x-rx)**2 + (y-ry)**2 ) < rd**2:\n",
    "#        A.append(i)\n",
    "#x_ic_train = np.delete(x_ic_train,A,axis=0)\n",
    "\n",
    "\n",
    "x_bcI_train,sin_bcI_train,cos_bcI_train = BD_naca0012(Tend,rx,ry,num_ib)\n",
    "#x_bcI_train,sin_bcI_train,cos_bcI_train = BD_circle(Tend,rx,ry,rd,num_ib)\n",
    "#x_bcI_train,sin_bcI_train,cos_bcI_train = BD_BackCorner(Tend,num_ib)\n",
    "\n",
    "rho_bcL_train, u_bcL_train,v_bcL_train, T_bcL_train = BC_L(x_bcL_train)  \n",
    "#rho_bcR_train, u_bcR_train,v_bcR_train, p_bcR_train = BC_L(x_bcR_train)  \n",
    "#rho_cut_train, u_cut_train,v_cut_train, p_cut_train = BC_Cut(x_cut_train)  \n",
    "rho_ic_train, u_ic_train,v_ic_train, T_ic_train = IC(x_ic_train)  \n",
    "\n",
    "\n",
    "x_int_train = torch.tensor(x_int_train, requires_grad=True, dtype=torch.float32).to(device)\n",
    "#x_intL_train = torch.tensor(x_intL_train,dtype=torch.float32).to(device)\n",
    "#x_intR_train = torch.tensor(x_intR_train,dtype=torch.float32).to(device)\n",
    "#x_intU_train = torch.tensor(x_intU_train,dtype=torch.float32).to(device)\n",
    "#x_intD_train = torch.tensor(x_intD_train,dtype=torch.float32).to(device)\n",
    "\n",
    "rho_bcL_train = torch.tensor(rho_bcL_train, dtype=torch.float32).to(device)\n",
    "u_bcL_train = torch.tensor(u_bcL_train, dtype=torch.float32).to(device)\n",
    "v_bcL_train = torch.tensor(v_bcL_train, dtype=torch.float32).to(device)\n",
    "T_bcL_train = torch.tensor(T_bcL_train, dtype=torch.float32).to(device)\n",
    "\n",
    "#rho_bcR_train = torch.tensor(rho_bcR_train, dtype=torch.float32).to(device)\n",
    "#u_bcR_train = torch.tensor(u_bcR_train, dtype=torch.float32).to(device)\n",
    "#v_bcR_train = torch.tensor(v_bcR_train, dtype=torch.float32).to(device)\n",
    "#p_bcR_train = torch.tensor(p_bcR_train, dtype=torch.float32).to(device)\n",
    "#\n",
    "#x_bcD_train = torch.tensor(x_bcD_train,requires_grad=True, dtype=torch.float32).to(device)\n",
    "#x_bcU_train = torch.tensor(x_bcU_train, requires_grad=True, dtype=torch.float32).to(device)\n",
    "#x_bcR_train = torch.tensor(x_bcR_train, requires_grad=True, dtype=torch.float32).to(device)\n",
    "x_bcL_train = torch.tensor(x_bcL_train, dtype=torch.float32).to(device)\n",
    "x_bcI_train = torch.tensor(x_bcI_train, requires_grad=True, dtype=torch.float32).to(device)\n",
    "sin_bcI_train = torch.tensor(sin_bcI_train, dtype=torch.float32).to(device)\n",
    "cos_bcI_train = torch.tensor(cos_bcI_train, dtype=torch.float32).to(device)\n",
    "\n",
    "#rho_cut_train = torch.tensor(rho_cut_train, dtype=torch.float32).to(device)\n",
    "#u_cut_train = torch.tensor(u_cut_train, dtype=torch.float32).to(device)\n",
    "#v_cut_train = torch.tensor(v_cut_train, dtype=torch.float32).to(device)\n",
    "#p_cut_train = torch.tensor(p_cut_train, dtype=torch.float32).to(device)\n",
    "#x_cut_train = torch.tensor(x_cut_train, dtype=torch.float32).to(device)\n",
    "\n",
    "rho_ic_train = torch.tensor(rho_ic_train, dtype=torch.float32).to(device)\n",
    "u_ic_train = torch.tensor(u_ic_train, dtype=torch.float32).to(device)\n",
    "v_ic_train = torch.tensor(v_ic_train, dtype=torch.float32).to(device)\n",
    "T_ic_train = torch.tensor(T_ic_train, dtype=torch.float32).to(device)\n",
    "x_ic_train = torch.tensor(x_ic_train, dtype=torch.float32).to(device)\n",
    "\n",
    "\n",
    "\n",
    "print('Start training...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_bc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure()\n\u001b[0;32m----> 2\u001b[0m plt\u001b[38;5;241m.\u001b[39mscatter(\u001b[43mx_bc\u001b[49m[:,\u001b[38;5;241m1\u001b[39m],x_bc[:,\u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#plt.pcolor(x_grid[:,0,:],y_grid[:,0,:],ue)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#plt.colorbar(x_grid[:,0,:],y_grid[:,0,:],ue)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#plt.scatter(x_int_train[:,1],)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#plt.pcolor(x[:],u[:,1])\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#plt.pcolor(x[:],u[:,2])\u001b[39;00m\n\u001b[1;32m      8\u001b[0m ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mgca()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x_bc' is not defined"
     ]
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.scatter(x_bc[:,1],x_bc[:,2])\n",
    "#plt.pcolor(x_grid[:,0,:],y_grid[:,0,:],ue)\n",
    "#plt.colorbar(x_grid[:,0,:],y_grid[:,0,:],ue)\n",
    "#plt.scatter(x_int_train[:,1],)\n",
    "#plt.pcolor(x[:],u[:,1])\n",
    "#plt.pcolor(x[:],u[:,2])\n",
    "ax = plt.gca()\n",
    "ax.set_aspect(1)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "#optimizer = torch.optim.LBFGS(model.parameters(),lr=lr,max_iter=500)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "#scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer_Adam,gamma=0.995)\n",
    "#optimizer = torch.optim.LBFGS(model.parameters(),lr=0.5,max_iter=20)\n",
    "#optimizer_LBFGS = torch.optim.LBFGS(model.parameters(), lr=0.01, max_iter=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 loss_pde:0.00105937, loss_ib:2.42150235\n",
      "epoch 1: loss 24.216082\n",
      "epoch 2 loss_pde:0.00119504, loss_ib:2.02977276\n",
      "epoch 2: loss 20.298923\n",
      "epoch 3 loss_pde:0.00230486, loss_ib:1.65080416\n",
      "epoch 3: loss 16.510345\n",
      "epoch 4 loss_pde:0.00648937, loss_ib:1.27324080\n",
      "epoch 4: loss 12.738898\n",
      "epoch 5 loss_pde:0.01983634, loss_ib:0.91184562\n",
      "epoch 5: loss 9.138292\n",
      "epoch 6 loss_pde:0.05699433, loss_ib:0.61334276\n",
      "epoch 6: loss 6.190422\n",
      "epoch 7 loss_pde:0.14125687, loss_ib:0.44863969\n",
      "epoch 7: loss 4.627654\n",
      "epoch 8 loss_pde:0.26483583, loss_ib:0.41822833\n",
      "epoch 8: loss 4.447119\n",
      "epoch 9 loss_pde:0.35360000, loss_ib:0.41890967\n",
      "epoch 9: loss 4.542696\n",
      "epoch 10 loss_pde:0.35944355, loss_ib:0.41856265\n",
      "epoch 10: loss 4.545070\n",
      "epoch 11 loss_pde:0.29884043, loss_ib:0.43319675\n",
      "epoch 11: loss 4.630808\n",
      "epoch 12 loss_pde:0.21242659, loss_ib:0.44758114\n",
      "epoch 12: loss 4.688238\n",
      "epoch 13 loss_pde:0.13421009, loss_ib:0.44058663\n",
      "epoch 13: loss 4.540076\n",
      "epoch 14 loss_pde:0.07989044, loss_ib:0.41307506\n",
      "epoch 14: loss 4.210641\n",
      "epoch 15 loss_pde:0.05032191, loss_ib:0.37588197\n",
      "epoch 15: loss 3.809141\n",
      "epoch 16 loss_pde:0.03931103, loss_ib:0.33839360\n",
      "epoch 16: loss 3.423247\n",
      "epoch 17 loss_pde:0.03969061, loss_ib:0.30772674\n",
      "epoch 17: loss 3.116958\n",
      "epoch 18 loss_pde:0.04616058, loss_ib:0.29087055\n",
      "epoch 18: loss 2.954866\n",
      "epoch 19 loss_pde:0.05565782, loss_ib:0.29333928\n",
      "epoch 19: loss 2.989051\n",
      "epoch 20 loss_pde:0.06675223, loss_ib:0.31275934\n",
      "epoch 20: loss 3.194346\n",
      "epoch 21 loss_pde:0.07895736, loss_ib:0.33442831\n",
      "epoch 21: loss 3.423240\n",
      "epoch 22 loss_pde:0.09209676, loss_ib:0.34109619\n",
      "epoch 22: loss 3.503059\n",
      "epoch 23 loss_pde:0.10575901, loss_ib:0.32943404\n",
      "epoch 23: loss 3.400099\n",
      "epoch 24 loss_pde:0.11907220, loss_ib:0.31010264\n",
      "epoch 24: loss 3.220099\n",
      "epoch 25 loss_pde:0.13091063, loss_ib:0.29438439\n",
      "epoch 25: loss 3.074754\n",
      "epoch 26 loss_pde:0.14029178, loss_ib:0.28560475\n",
      "epoch 26: loss 2.996339\n",
      "epoch 27 loss_pde:0.14661090, loss_ib:0.28078640\n",
      "epoch 27: loss 2.954475\n",
      "epoch 28 loss_pde:0.14958560, loss_ib:0.27648175\n",
      "epoch 28: loss 2.914403\n",
      "epoch 29 loss_pde:0.14907394, loss_ib:0.27221152\n",
      "epoch 29: loss 2.871189\n",
      "epoch 30 loss_pde:0.14506440, loss_ib:0.26989171\n",
      "epoch 30: loss 2.843982\n",
      "epoch 31 loss_pde:0.13784590, loss_ib:0.27119845\n",
      "epoch 31: loss 2.849831\n",
      "epoch 32 loss_pde:0.12812611, loss_ib:0.27540913\n",
      "epoch 32: loss 2.882217\n",
      "epoch 33 loss_pde:0.11692078, loss_ib:0.27943692\n",
      "epoch 33: loss 2.911290\n",
      "epoch 34 loss_pde:0.10527110, loss_ib:0.28008935\n",
      "epoch 34: loss 2.906165\n",
      "epoch 35 loss_pde:0.09400421, loss_ib:0.27654845\n",
      "epoch 35: loss 2.859488\n",
      "epoch 36 loss_pde:0.08365771, loss_ib:0.27078226\n",
      "epoch 36: loss 2.791480\n",
      "epoch 37 loss_pde:0.07451056, loss_ib:0.26572675\n",
      "epoch 37: loss 2.731778\n",
      "epoch 38 loss_pde:0.06663834, loss_ib:0.26310480\n",
      "epoch 38: loss 2.697686\n",
      "epoch 39 loss_pde:0.05997937, loss_ib:0.26265386\n",
      "epoch 39: loss 2.686518\n",
      "epoch 40 loss_pde:0.05441814, loss_ib:0.26299334\n",
      "epoch 40: loss 2.684351\n",
      "epoch 41 loss_pde:0.04986997, loss_ib:0.26303852\n",
      "epoch 41: loss 2.680255\n",
      "epoch 42 loss_pde:0.04633917, loss_ib:0.26269794\n",
      "epoch 42: loss 2.673319\n",
      "epoch 43 loss_pde:0.04393313, loss_ib:0.26241723\n",
      "epoch 43: loss 2.668105\n",
      "epoch 44 loss_pde:0.04282069, loss_ib:0.26220977\n",
      "epoch 44: loss 2.664918\n",
      "epoch 45 loss_pde:0.04318739, loss_ib:0.26130700\n",
      "epoch 45: loss 2.656257\n",
      "epoch 46 loss_pde:0.04521725, loss_ib:0.25888732\n",
      "epoch 46: loss 2.634090\n",
      "epoch 47 loss_pde:0.04904807, loss_ib:0.25508633\n",
      "epoch 47: loss 2.599911\n",
      "epoch 48 loss_pde:0.05468856, loss_ib:0.25101280\n",
      "epoch 48: loss 2.564816\n",
      "epoch 49 loss_pde:0.06192334, loss_ib:0.24769770\n",
      "epoch 49: loss 2.538900\n",
      "epoch 50 loss_pde:0.07026948, loss_ib:0.24521568\n",
      "epoch 50: loss 2.522426\n",
      "epoch 51 loss_pde:0.07909150, loss_ib:0.24290222\n",
      "epoch 51: loss 2.508114\n",
      "epoch 52 loss_pde:0.08783720, loss_ib:0.24028005\n",
      "epoch 52: loss 2.490638\n",
      "epoch 53 loss_pde:0.09626778, loss_ib:0.23755342\n",
      "epoch 53: loss 2.471802\n",
      "epoch 54 loss_pde:0.10455696, loss_ib:0.23513116\n",
      "epoch 54: loss 2.455869\n",
      "epoch 55 loss_pde:0.11318994, loss_ib:0.23285145\n",
      "epoch 55: loss 2.441705\n",
      "epoch 56 loss_pde:0.12266192, loss_ib:0.23003687\n",
      "epoch 56: loss 2.423031\n",
      "epoch 57 loss_pde:0.13308699, loss_ib:0.22643659\n",
      "epoch 57: loss 2.397453\n",
      "epoch 58 loss_pde:0.14397773, loss_ib:0.22267836\n",
      "epoch 58: loss 2.370761\n",
      "epoch 59 loss_pde:0.15434976, loss_ib:0.21947621\n",
      "epoch 59: loss 2.349112\n",
      "epoch 60 loss_pde:0.16317281, loss_ib:0.21681094\n",
      "epoch 60: loss 2.331282\n",
      "epoch 61 loss_pde:0.16991904, loss_ib:0.21429989\n",
      "epoch 61: loss 2.312918\n",
      "epoch 62 loss_pde:0.17486873, loss_ib:0.21196833\n",
      "epoch 62: loss 2.294552\n",
      "epoch 63 loss_pde:0.17897801, loss_ib:0.21004188\n",
      "epoch 63: loss 2.279397\n",
      "epoch 64 loss_pde:0.18331032, loss_ib:0.20822433\n",
      "epoch 64: loss 2.265553\n",
      "epoch 65 loss_pde:0.18834317, loss_ib:0.20601486\n",
      "epoch 65: loss 2.248492\n",
      "epoch 66 loss_pde:0.19370972, loss_ib:0.20359914\n",
      "epoch 66: loss 2.229701\n",
      "epoch 67 loss_pde:0.19853598, loss_ib:0.20155236\n",
      "epoch 67: loss 2.214060\n",
      "epoch 68 loss_pde:0.20192707, loss_ib:0.19993520\n",
      "epoch 68: loss 2.201279\n",
      "epoch 69 loss_pde:0.20369335, loss_ib:0.19853616\n",
      "epoch 69: loss 2.189055\n",
      "epoch 70 loss_pde:0.20459571, loss_ib:0.19748715\n",
      "epoch 70: loss 2.179467\n",
      "epoch 71 loss_pde:0.20588212, loss_ib:0.19683644\n",
      "epoch 71: loss 2.174247\n",
      "epoch 72 loss_pde:0.20846832, loss_ib:0.19614741\n",
      "epoch 72: loss 2.169942\n",
      "epoch 73 loss_pde:0.21242012, loss_ib:0.19521490\n",
      "epoch 73: loss 2.164569\n",
      "epoch 74 loss_pde:0.21713270, loss_ib:0.19427061\n",
      "epoch 74: loss 2.159839\n",
      "epoch 75 loss_pde:0.22193302, loss_ib:0.19326577\n",
      "epoch 75: loss 2.154591\n",
      "epoch 76 loss_pde:0.22658372, loss_ib:0.19203749\n",
      "epoch 76: loss 2.146959\n",
      "epoch 77 loss_pde:0.23135395, loss_ib:0.19077778\n",
      "epoch 77: loss 2.139132\n",
      "epoch 78 loss_pde:0.23664111, loss_ib:0.18955187\n",
      "epoch 78: loss 2.132160\n",
      "epoch 79 loss_pde:0.24241345, loss_ib:0.18821795\n",
      "epoch 79: loss 2.124593\n",
      "epoch 80 loss_pde:0.24788833, loss_ib:0.18697569\n",
      "epoch 80: loss 2.117645\n",
      "epoch 81 loss_pde:0.25175625, loss_ib:0.18603556\n",
      "epoch 81: loss 2.112112\n",
      "epoch 82 loss_pde:0.25287035, loss_ib:0.18533614\n",
      "epoch 82: loss 2.106232\n",
      "epoch 83 loss_pde:0.25086084, loss_ib:0.18494579\n",
      "epoch 83: loss 2.100319\n",
      "epoch 84 loss_pde:0.24614157, loss_ib:0.18488792\n",
      "epoch 84: loss 2.095021\n",
      "epoch 85 loss_pde:0.23935674, loss_ib:0.18497750\n",
      "epoch 85: loss 2.089132\n",
      "epoch 86 loss_pde:0.23087634, loss_ib:0.18523370\n",
      "epoch 86: loss 2.083213\n",
      "epoch 87 loss_pde:0.22090326, loss_ib:0.18571183\n",
      "epoch 87: loss 2.078022\n",
      "epoch 88 loss_pde:0.21000867, loss_ib:0.18627131\n",
      "epoch 88: loss 2.072722\n",
      "epoch 89 loss_pde:0.19938716, loss_ib:0.18683070\n",
      "epoch 89: loss 2.067694\n",
      "epoch 90 loss_pde:0.19045231, loss_ib:0.18726429\n",
      "epoch 90: loss 2.063095\n",
      "epoch 91 loss_pde:0.18412161, loss_ib:0.18738577\n",
      "epoch 91: loss 2.057979\n",
      "epoch 92 loss_pde:0.18048462, loss_ib:0.18721615\n",
      "epoch 92: loss 2.052646\n",
      "epoch 93 loss_pde:0.17911565, loss_ib:0.18679857\n",
      "epoch 93: loss 2.047101\n",
      "epoch 94 loss_pde:0.17963721, loss_ib:0.18612629\n",
      "epoch 94: loss 2.040900\n",
      "epoch 95 loss_pde:0.18201283, loss_ib:0.18526588\n",
      "epoch 95: loss 2.034672\n",
      "epoch 96 loss_pde:0.18623598, loss_ib:0.18421683\n",
      "epoch 96: loss 2.028404\n",
      "epoch 97 loss_pde:0.19180328, loss_ib:0.18301636\n",
      "epoch 97: loss 2.021967\n",
      "epoch 98 loss_pde:0.19760634, loss_ib:0.18181698\n",
      "epoch 98: loss 2.015776\n",
      "epoch 99 loss_pde:0.20253369, loss_ib:0.18069208\n",
      "epoch 99: loss 2.009454\n",
      "epoch 100 loss_pde:0.20615669, loss_ib:0.17969078\n",
      "epoch 100: loss 2.003064\n",
      "epoch 101 loss_pde:0.20868716, loss_ib:0.17880657\n",
      "epoch 101: loss 1.996753\n",
      "epoch 102 loss_pde:0.21032795, loss_ib:0.17799313\n",
      "epoch 102: loss 1.990259\n",
      "epoch 103 loss_pde:0.21091929, loss_ib:0.17729834\n",
      "epoch 103: loss 1.983903\n",
      "epoch 104 loss_pde:0.21035793, loss_ib:0.17671956\n",
      "epoch 104: loss 1.977553\n",
      "epoch 105 loss_pde:0.20915926, loss_ib:0.17620784\n",
      "epoch 105: loss 1.971238\n",
      "epoch 106 loss_pde:0.20826420, loss_ib:0.17568001\n",
      "epoch 106: loss 1.965064\n",
      "epoch 107 loss_pde:0.20825253, loss_ib:0.17505638\n",
      "epoch 107: loss 1.958816\n",
      "epoch 108 loss_pde:0.20899762, loss_ib:0.17436679\n",
      "epoch 108: loss 1.952665\n",
      "epoch 109 loss_pde:0.21017969, loss_ib:0.17362732\n",
      "epoch 109: loss 1.946453\n",
      "epoch 110 loss_pde:0.21183786, loss_ib:0.17284563\n",
      "epoch 110: loss 1.940294\n",
      "epoch 111 loss_pde:0.21414363, loss_ib:0.17200735\n",
      "epoch 111: loss 1.934217\n",
      "epoch 112 loss_pde:0.21680512, loss_ib:0.17114106\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m tic \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m----> 4\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m toc \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotal training time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtoc \u001b[38;5;241m-\u001b[39m tic\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Optimize loss function\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m loss_value \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(loss, \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m loss\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Print total loss\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/optim/optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/grad_mode.py:28\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m():\n\u001b[0;32m---> 28\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/optim/adam.py:92\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[0;32m---> 92\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups:\n\u001b[1;32m     95\u001b[0m     params_with_grad \u001b[38;5;241m=\u001b[39m []\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mtrain.<locals>.closure\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Print iteration, loss of PDE and ICs\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m loss_pde:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_pde\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.8f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, loss_ib:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_ib\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.8f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 29\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_tensor.py:307\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    300\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    301\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    305\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    306\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 307\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:154\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m--> 154\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 10000\n",
    "tic = time.time()\n",
    "for epoch in range(1, epochs+1):\n",
    "    train(epoch)\n",
    "toc = time.time()\n",
    "print(f'Total training time: {toc - tic}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0001\n",
    "#optimizer = torch.optim.LBFGS(model.parameters(),lr=lr,max_iter=500)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "#scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer_Adam,gamma=0.995)\n",
    "#optimizer = torch.optim.LBFGS(model.parameters(),lr=0.1,max_iter=10)\n",
    "#optimizer_LBFGS = torch.optim.LBFGS(model.parameters(), lr=0.01, max_iter=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "tic = time.time()\n",
    "for epoch in range(1, epochs+1):\n",
    "    train(epoch)\n",
    "toc = time.time()\n",
    "print(f'Total training time: {toc - tic}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0001\n",
    "#optimizer = torch.optim.LBFGS(model.parameters(),lr=lr,max_iter=500)\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "#scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer_Adam,gamma=0.995)\n",
    "#optimizer_LBFGS = torch.optim.LBFGS(model.parameters(), lr=0.01, max_iter=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 loss_pde:0.03363353, loss_ib:0.00844427\n",
      "epoch 1 loss_pde:0.03692716, loss_ib:0.02826863\n",
      "epoch 1 loss_pde:0.03391111, loss_ib:0.01203153\n",
      "epoch 1 loss_pde:0.03322069, loss_ib:0.00796735\n",
      "epoch 1 loss_pde:0.03309247, loss_ib:0.00693440\n",
      "epoch 1 loss_pde:0.03308655, loss_ib:0.00666556\n",
      "epoch 1 loss_pde:0.03310243, loss_ib:0.00658988\n",
      "epoch 1 loss_pde:0.03312118, loss_ib:0.00656391\n",
      "epoch 1 loss_pde:0.03313914, loss_ib:0.00655066\n",
      "epoch 1 loss_pde:0.03316030, loss_ib:0.00654005\n",
      "epoch 1 loss_pde:0.03318403, loss_ib:0.00652974\n",
      "epoch 1 loss_pde:0.03321043, loss_ib:0.00652033\n",
      "epoch 1 loss_pde:0.03322602, loss_ib:0.00651493\n",
      "epoch 1 loss_pde:0.03323265, loss_ib:0.00651233\n",
      "epoch 1 loss_pde:0.03323390, loss_ib:0.00651086\n",
      "epoch 1 loss_pde:0.03322963, loss_ib:0.00651026\n",
      "epoch 1 loss_pde:0.03322074, loss_ib:0.00650984\n",
      "epoch 1 loss_pde:0.03321016, loss_ib:0.00650949\n",
      "epoch 1 loss_pde:0.03320366, loss_ib:0.00650853\n",
      "epoch 1 loss_pde:0.03320505, loss_ib:0.00650614\n",
      "epoch 1: loss 0.118076\n",
      "epoch 2 loss_pde:0.03320128, loss_ib:0.00650411\n",
      "epoch 2 loss_pde:0.03320758, loss_ib:0.00650104\n",
      "epoch 2 loss_pde:0.03321409, loss_ib:0.00649894\n",
      "epoch 2 loss_pde:0.03322095, loss_ib:0.00649676\n",
      "epoch 2 loss_pde:0.03322381, loss_ib:0.00649521\n",
      "epoch 2 loss_pde:0.03322173, loss_ib:0.00649412\n",
      "epoch 2 loss_pde:0.03321867, loss_ib:0.00649270\n",
      "epoch 2 loss_pde:0.03321967, loss_ib:0.00649021\n",
      "epoch 2 loss_pde:0.03322984, loss_ib:0.00648637\n",
      "epoch 2 loss_pde:0.03324479, loss_ib:0.00648263\n",
      "epoch 2 loss_pde:0.03325974, loss_ib:0.00647958\n",
      "epoch 2 loss_pde:0.03328270, loss_ib:0.00647550\n",
      "epoch 2 loss_pde:0.03331108, loss_ib:0.00647093\n",
      "epoch 2 loss_pde:0.03335592, loss_ib:0.00646485\n",
      "epoch 2 loss_pde:0.03338013, loss_ib:0.00646107\n",
      "epoch 2 loss_pde:0.03341086, loss_ib:0.00645646\n",
      "epoch 2 loss_pde:0.03344436, loss_ib:0.00645133\n",
      "epoch 2 loss_pde:0.03348754, loss_ib:0.00644545\n",
      "epoch 2 loss_pde:0.03351926, loss_ib:0.00644089\n",
      "epoch 2 loss_pde:0.03355172, loss_ib:0.00643657\n",
      "epoch 2: loss 0.098242\n",
      "epoch 3 loss_pde:0.03359535, loss_ib:0.00643101\n",
      "epoch 3 loss_pde:0.03364117, loss_ib:0.00642503\n",
      "epoch 3 loss_pde:0.03367718, loss_ib:0.00642037\n",
      "epoch 3 loss_pde:0.03370799, loss_ib:0.00641633\n",
      "epoch 3 loss_pde:0.03373716, loss_ib:0.00641246\n",
      "epoch 3 loss_pde:0.03376871, loss_ib:0.00640823\n",
      "epoch 3 loss_pde:0.03380822, loss_ib:0.00640289\n",
      "epoch 3 loss_pde:0.03386004, loss_ib:0.00639589\n",
      "epoch 3 loss_pde:0.03392568, loss_ib:0.00638704\n",
      "epoch 3 loss_pde:0.03399504, loss_ib:0.00637765\n",
      "epoch 3 loss_pde:0.03406949, loss_ib:0.00636770\n",
      "epoch 3 loss_pde:0.03412478, loss_ib:0.00635991\n",
      "epoch 3 loss_pde:0.03420243, loss_ib:0.00634931\n",
      "epoch 3 loss_pde:0.03423709, loss_ib:0.00634315\n",
      "epoch 3 loss_pde:0.03425644, loss_ib:0.00633801\n",
      "epoch 3 loss_pde:0.03425121, loss_ib:0.00633434\n",
      "epoch 3 loss_pde:0.03423583, loss_ib:0.00633162\n",
      "epoch 3 loss_pde:0.03420887, loss_ib:0.00633036\n",
      "epoch 3 loss_pde:0.03416534, loss_ib:0.00633042\n",
      "epoch 3 loss_pde:0.03413454, loss_ib:0.00633036\n",
      "epoch 3: loss 0.097905\n",
      "epoch 4 loss_pde:0.03411791, loss_ib:0.00632878\n",
      "epoch 4 loss_pde:0.03409971, loss_ib:0.00632766\n",
      "epoch 4 loss_pde:0.03411184, loss_ib:0.00632341\n",
      "epoch 4 loss_pde:0.03410605, loss_ib:0.00631951\n",
      "epoch 4 loss_pde:0.03413874, loss_ib:0.00631209\n",
      "epoch 4 loss_pde:0.03426737, loss_ib:0.00629291\n",
      "epoch 4 loss_pde:0.03433648, loss_ib:0.00628166\n",
      "epoch 4 loss_pde:0.03440972, loss_ib:0.00626848\n",
      "epoch 4 loss_pde:0.03447437, loss_ib:0.00625652\n",
      "epoch 4 loss_pde:0.03451158, loss_ib:0.00624802\n",
      "epoch 4 loss_pde:0.03452095, loss_ib:0.00624234\n",
      "epoch 4 loss_pde:0.03450610, loss_ib:0.00623738\n",
      "epoch 4 loss_pde:0.03447857, loss_ib:0.00623253\n",
      "epoch 4 loss_pde:0.03442873, loss_ib:0.00622791\n",
      "epoch 4 loss_pde:0.03443235, loss_ib:0.00621572\n",
      "epoch 4 loss_pde:0.03440203, loss_ib:0.00620842\n",
      "epoch 4 loss_pde:0.03438126, loss_ib:0.00620063\n",
      "epoch 4 loss_pde:0.03436269, loss_ib:0.00619311\n",
      "epoch 4 loss_pde:0.03435901, loss_ib:0.00618637\n",
      "epoch 4 loss_pde:0.03436363, loss_ib:0.00617905\n",
      "epoch 4: loss 0.097406\n",
      "epoch 5 loss_pde:0.03434454, loss_ib:0.00617128\n",
      "epoch 5 loss_pde:0.03435351, loss_ib:0.00615814\n",
      "epoch 5 loss_pde:0.03431132, loss_ib:0.00615063\n",
      "epoch 5 loss_pde:0.03432518, loss_ib:0.00613855\n",
      "epoch 5 loss_pde:0.03429535, loss_ib:0.00613300\n",
      "epoch 5 loss_pde:0.03429189, loss_ib:0.00612568\n",
      "epoch 5 loss_pde:0.03425672, loss_ib:0.00612142\n",
      "epoch 5 loss_pde:0.03422461, loss_ib:0.00611429\n",
      "epoch 5 loss_pde:0.03416039, loss_ib:0.00610555\n",
      "epoch 5 loss_pde:0.03412394, loss_ib:0.00609073\n",
      "epoch 5 loss_pde:0.03405922, loss_ib:0.00607603\n",
      "epoch 5 loss_pde:0.03404045, loss_ib:0.00606300\n",
      "epoch 5 loss_pde:0.03401182, loss_ib:0.00605425\n",
      "epoch 5 loss_pde:0.03403600, loss_ib:0.00604400\n",
      "epoch 5 loss_pde:0.03399647, loss_ib:0.00604197\n",
      "epoch 5 loss_pde:0.03398196, loss_ib:0.00603819\n",
      "epoch 5 loss_pde:0.03393485, loss_ib:0.00603742\n",
      "epoch 5 loss_pde:0.03387234, loss_ib:0.00603732\n",
      "epoch 5 loss_pde:0.03377845, loss_ib:0.00603994\n",
      "epoch 5 loss_pde:0.03366798, loss_ib:0.00604362\n",
      "epoch 5: loss 0.096057\n",
      "epoch 6 loss_pde:0.03356960, loss_ib:0.00604493\n",
      "epoch 6 loss_pde:0.03345371, loss_ib:0.00604628\n",
      "epoch 6 loss_pde:0.03336871, loss_ib:0.00604467\n",
      "epoch 6 loss_pde:0.03329624, loss_ib:0.00604187\n",
      "epoch 6 loss_pde:0.03325502, loss_ib:0.00603722\n",
      "epoch 6 loss_pde:0.03323245, loss_ib:0.00603224\n",
      "epoch 6 loss_pde:0.03323043, loss_ib:0.00602672\n",
      "epoch 6 loss_pde:0.03323476, loss_ib:0.00602135\n",
      "epoch 6 loss_pde:0.03324971, loss_ib:0.00601489\n",
      "epoch 6 loss_pde:0.03326593, loss_ib:0.00600856\n",
      "epoch 6 loss_pde:0.03325253, loss_ib:0.00600435\n",
      "epoch 6 loss_pde:0.03325575, loss_ib:0.00599902\n",
      "epoch 6 loss_pde:0.03318771, loss_ib:0.00600005\n",
      "epoch 6 loss_pde:0.03309846, loss_ib:0.00600174\n",
      "epoch 6 loss_pde:0.03291566, loss_ib:0.00601149\n",
      "epoch 6 loss_pde:0.03277725, loss_ib:0.00601852\n",
      "epoch 6 loss_pde:0.03267986, loss_ib:0.00602410\n",
      "epoch 6 loss_pde:0.03259258, loss_ib:0.00602881\n",
      "epoch 6 loss_pde:0.03250856, loss_ib:0.00603292\n",
      "epoch 6 loss_pde:0.03246517, loss_ib:0.00603400\n",
      "epoch 6: loss 0.094019\n",
      "epoch 7 loss_pde:0.03243346, loss_ib:0.00603384\n",
      "epoch 7 loss_pde:0.03244284, loss_ib:0.00602889\n",
      "epoch 7 loss_pde:0.03246265, loss_ib:0.00602263\n",
      "epoch 7 loss_pde:0.03251681, loss_ib:0.00601263\n",
      "epoch 7 loss_pde:0.03260477, loss_ib:0.00599706\n",
      "epoch 7 loss_pde:0.03273434, loss_ib:0.00597573\n",
      "epoch 7 loss_pde:0.03287351, loss_ib:0.00595558\n",
      "epoch 7 loss_pde:0.03293959, loss_ib:0.00594454\n",
      "epoch 7 loss_pde:0.03299815, loss_ib:0.00593312\n",
      "epoch 7 loss_pde:0.03303476, loss_ib:0.00592384\n",
      "epoch 7 loss_pde:0.03305595, loss_ib:0.00591574\n",
      "epoch 7 loss_pde:0.03304478, loss_ib:0.00591282\n",
      "epoch 7 loss_pde:0.03304467, loss_ib:0.00590789\n",
      "epoch 7 loss_pde:0.03303831, loss_ib:0.00590255\n",
      "epoch 7 loss_pde:0.03302538, loss_ib:0.00589833\n",
      "epoch 7 loss_pde:0.03300784, loss_ib:0.00589560\n",
      "epoch 7 loss_pde:0.03298271, loss_ib:0.00589384\n",
      "epoch 7 loss_pde:0.03294871, loss_ib:0.00589214\n",
      "epoch 7 loss_pde:0.03289186, loss_ib:0.00589101\n",
      "epoch 7 loss_pde:0.03281475, loss_ib:0.00589128\n",
      "epoch 7: loss 0.092772\n",
      "epoch 8 loss_pde:0.03274947, loss_ib:0.00589098\n",
      "epoch 8 loss_pde:0.03262423, loss_ib:0.00589466\n",
      "epoch 8 loss_pde:0.03253371, loss_ib:0.00589715\n",
      "epoch 8 loss_pde:0.03244762, loss_ib:0.00590004\n",
      "epoch 8 loss_pde:0.03236719, loss_ib:0.00590296\n",
      "epoch 8 loss_pde:0.03229526, loss_ib:0.00590679\n",
      "epoch 8 loss_pde:0.03226551, loss_ib:0.00590647\n",
      "epoch 8 loss_pde:0.03221606, loss_ib:0.00590774\n",
      "epoch 8 loss_pde:0.03220531, loss_ib:0.00590424\n",
      "epoch 8 loss_pde:0.03218844, loss_ib:0.00589994\n",
      "epoch 8 loss_pde:0.03220782, loss_ib:0.00589279\n",
      "epoch 8 loss_pde:0.03221809, loss_ib:0.00588677\n",
      "epoch 8 loss_pde:0.03224820, loss_ib:0.00587955\n",
      "epoch 8 loss_pde:0.03227273, loss_ib:0.00587295\n",
      "epoch 8 loss_pde:0.03230241, loss_ib:0.00586407\n",
      "epoch 8 loss_pde:0.03233787, loss_ib:0.00585425\n",
      "epoch 8 loss_pde:0.03235207, loss_ib:0.00584584\n",
      "epoch 8 loss_pde:0.03241186, loss_ib:0.00583197\n",
      "epoch 8 loss_pde:0.03238795, loss_ib:0.00582724\n",
      "epoch 8 loss_pde:0.03238319, loss_ib:0.00582221\n",
      "epoch 8: loss 0.091659\n",
      "epoch 9 loss_pde:0.03235244, loss_ib:0.00581960\n",
      "epoch 9 loss_pde:0.03231392, loss_ib:0.00581811\n",
      "epoch 9 loss_pde:0.03229980, loss_ib:0.00581483\n",
      "epoch 9 loss_pde:0.03227662, loss_ib:0.00581336\n",
      "epoch 9 loss_pde:0.03224950, loss_ib:0.00580936\n",
      "epoch 9 loss_pde:0.03217525, loss_ib:0.00580588\n",
      "epoch 9 loss_pde:0.03218027, loss_ib:0.00579450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9 loss_pde:0.03207171, loss_ib:0.00579633\n",
      "epoch 9 loss_pde:0.03198983, loss_ib:0.00579759\n",
      "epoch 9 loss_pde:0.03189174, loss_ib:0.00580098\n",
      "epoch 9 loss_pde:0.03178311, loss_ib:0.00580647\n",
      "epoch 9 loss_pde:0.03167802, loss_ib:0.00581247\n",
      "epoch 9 loss_pde:0.03156629, loss_ib:0.00581909\n",
      "epoch 9 loss_pde:0.03148773, loss_ib:0.00582126\n",
      "epoch 9 loss_pde:0.03134638, loss_ib:0.00582809\n",
      "epoch 9 loss_pde:0.03130231, loss_ib:0.00582665\n",
      "epoch 9 loss_pde:0.03127110, loss_ib:0.00582458\n",
      "epoch 9 loss_pde:0.03128806, loss_ib:0.00581728\n",
      "epoch 9 loss_pde:0.03133390, loss_ib:0.00580627\n",
      "epoch 9 loss_pde:0.03139865, loss_ib:0.00579358\n",
      "epoch 9: loss 0.090548\n",
      "epoch 10 loss_pde:0.03146595, loss_ib:0.00578107\n",
      "epoch 10 loss_pde:0.03150398, loss_ib:0.00577086\n",
      "epoch 10 loss_pde:0.03153765, loss_ib:0.00576048\n",
      "epoch 10 loss_pde:0.03150809, loss_ib:0.00575649\n",
      "epoch 10 loss_pde:0.03146530, loss_ib:0.00575332\n",
      "epoch 10 loss_pde:0.03137751, loss_ib:0.00575588\n",
      "epoch 10 loss_pde:0.03125600, loss_ib:0.00576154\n",
      "epoch 10 loss_pde:0.03113322, loss_ib:0.00576756\n",
      "epoch 10 loss_pde:0.03101246, loss_ib:0.00577428\n",
      "epoch 10 loss_pde:0.03093125, loss_ib:0.00577783\n",
      "epoch 10 loss_pde:0.03087122, loss_ib:0.00577966\n",
      "epoch 10 loss_pde:0.03085174, loss_ib:0.00577563\n",
      "epoch 10 loss_pde:0.03086465, loss_ib:0.00576632\n",
      "epoch 10 loss_pde:0.03092906, loss_ib:0.00575096\n",
      "epoch 10 loss_pde:0.03100196, loss_ib:0.00573492\n",
      "epoch 10 loss_pde:0.03108528, loss_ib:0.00572046\n",
      "epoch 10 loss_pde:0.03114247, loss_ib:0.00570890\n",
      "epoch 10 loss_pde:0.03120428, loss_ib:0.00569636\n",
      "epoch 10 loss_pde:0.03122098, loss_ib:0.00568770\n",
      "epoch 10 loss_pde:0.03122056, loss_ib:0.00567960\n",
      "epoch 10: loss 0.089277\n",
      "epoch 11 loss_pde:0.03115757, loss_ib:0.00567619\n",
      "epoch 11 loss_pde:0.03109174, loss_ib:0.00567374\n",
      "epoch 11 loss_pde:0.03099650, loss_ib:0.00567548\n",
      "epoch 11 loss_pde:0.03090871, loss_ib:0.00567690\n",
      "epoch 11 loss_pde:0.03083473, loss_ib:0.00567773\n",
      "epoch 11 loss_pde:0.03079654, loss_ib:0.00567576\n",
      "epoch 11 loss_pde:0.03079422, loss_ib:0.00566921\n",
      "epoch 11 loss_pde:0.03082553, loss_ib:0.00565844\n",
      "epoch 11 loss_pde:0.03086740, loss_ib:0.00564627\n",
      "epoch 11 loss_pde:0.03092186, loss_ib:0.00563204\n",
      "epoch 11 loss_pde:0.03097671, loss_ib:0.00561951\n",
      "epoch 11 loss_pde:0.03099460, loss_ib:0.00561121\n",
      "epoch 11 loss_pde:0.03100001, loss_ib:0.00560502\n",
      "epoch 11 loss_pde:0.03091907, loss_ib:0.00560958\n",
      "epoch 11 loss_pde:0.03090810, loss_ib:0.00560568\n",
      "epoch 11 loss_pde:0.03086418, loss_ib:0.00560503\n",
      "epoch 11 loss_pde:0.03076237, loss_ib:0.00561041\n",
      "epoch 11 loss_pde:0.03066291, loss_ib:0.00561636\n",
      "epoch 11 loss_pde:0.03055023, loss_ib:0.00562433\n",
      "epoch 11 loss_pde:0.03044676, loss_ib:0.00563169\n",
      "epoch 11: loss 0.087919\n",
      "epoch 12 loss_pde:0.03036022, loss_ib:0.00563752\n",
      "epoch 12 loss_pde:0.03029173, loss_ib:0.00564177\n",
      "epoch 12 loss_pde:0.03024503, loss_ib:0.00564317\n",
      "epoch 12 loss_pde:0.03022083, loss_ib:0.00564260\n",
      "epoch 12 loss_pde:0.03020592, loss_ib:0.00563956\n",
      "epoch 12 loss_pde:0.03016606, loss_ib:0.00563658\n",
      "epoch 12 loss_pde:0.03016011, loss_ib:0.00563135\n",
      "epoch 12 loss_pde:0.03012288, loss_ib:0.00563038\n",
      "epoch 12 loss_pde:0.03012049, loss_ib:0.00562659\n",
      "epoch 12 loss_pde:0.03009300, loss_ib:0.00562659\n",
      "epoch 12 loss_pde:0.03004523, loss_ib:0.00562687\n",
      "epoch 12 loss_pde:0.02995005, loss_ib:0.00563128\n",
      "epoch 12 loss_pde:0.02988171, loss_ib:0.00563395\n",
      "epoch 12 loss_pde:0.02980460, loss_ib:0.00563839\n",
      "epoch 12 loss_pde:0.02975625, loss_ib:0.00564012\n",
      "epoch 12 loss_pde:0.02972095, loss_ib:0.00564031\n",
      "epoch 12 loss_pde:0.02971129, loss_ib:0.00563761\n",
      "epoch 12 loss_pde:0.02972184, loss_ib:0.00563245\n",
      "epoch 12 loss_pde:0.02975858, loss_ib:0.00562520\n",
      "epoch 12 loss_pde:0.02979704, loss_ib:0.00561704\n",
      "epoch 12: loss 0.086735\n",
      "epoch 13 loss_pde:0.02986478, loss_ib:0.00560479\n",
      "epoch 13 loss_pde:0.02987516, loss_ib:0.00559856\n",
      "epoch 13 loss_pde:0.02986789, loss_ib:0.00559500\n",
      "epoch 13 loss_pde:0.02982319, loss_ib:0.00559458\n",
      "epoch 13 loss_pde:0.02975427, loss_ib:0.00559665\n",
      "epoch 13 loss_pde:0.02964717, loss_ib:0.00560257\n",
      "epoch 13 loss_pde:0.02954560, loss_ib:0.00560818\n",
      "epoch 13 loss_pde:0.02942549, loss_ib:0.00561584\n",
      "epoch 13 loss_pde:0.02931493, loss_ib:0.00562118\n",
      "epoch 13 loss_pde:0.02920431, loss_ib:0.00562602\n",
      "epoch 13 loss_pde:0.02915142, loss_ib:0.00562647\n",
      "epoch 13 loss_pde:0.02913814, loss_ib:0.00562430\n",
      "epoch 13 loss_pde:0.02919670, loss_ib:0.00561201\n",
      "epoch 13 loss_pde:0.02925664, loss_ib:0.00560020\n",
      "epoch 13 loss_pde:0.02931762, loss_ib:0.00558678\n",
      "epoch 13 loss_pde:0.02938821, loss_ib:0.00557271\n",
      "epoch 13 loss_pde:0.02943814, loss_ib:0.00556211\n",
      "epoch 13 loss_pde:0.02946993, loss_ib:0.00555404\n",
      "epoch 13 loss_pde:0.02947557, loss_ib:0.00554886\n",
      "epoch 13 loss_pde:0.02945863, loss_ib:0.00554543\n",
      "epoch 13: loss 0.085913\n",
      "epoch 14 loss_pde:0.02941484, loss_ib:0.00554494\n",
      "epoch 14 loss_pde:0.02937327, loss_ib:0.00554399\n",
      "epoch 14 loss_pde:0.02928714, loss_ib:0.00554736\n",
      "epoch 14 loss_pde:0.02923855, loss_ib:0.00554765\n",
      "epoch 14 loss_pde:0.02915669, loss_ib:0.00555037\n",
      "epoch 14 loss_pde:0.02913314, loss_ib:0.00554876\n",
      "epoch 14 loss_pde:0.02912777, loss_ib:0.00554306\n",
      "epoch 14 loss_pde:0.02912565, loss_ib:0.00553700\n",
      "epoch 14 loss_pde:0.02917007, loss_ib:0.00552871\n",
      "epoch 14 loss_pde:0.02922498, loss_ib:0.00551991\n",
      "epoch 14 loss_pde:0.02928383, loss_ib:0.00551044\n",
      "epoch 14 loss_pde:0.02934966, loss_ib:0.00550001\n",
      "epoch 14 loss_pde:0.02941694, loss_ib:0.00548924\n",
      "epoch 14 loss_pde:0.02947934, loss_ib:0.00547891\n",
      "epoch 14 loss_pde:0.02952586, loss_ib:0.00547021\n",
      "epoch 14 loss_pde:0.02955713, loss_ib:0.00546233\n",
      "epoch 14 loss_pde:0.02958509, loss_ib:0.00545429\n",
      "epoch 14 loss_pde:0.02949369, loss_ib:0.00545646\n",
      "epoch 14 loss_pde:0.02948233, loss_ib:0.00545109\n",
      "epoch 14 loss_pde:0.02943340, loss_ib:0.00544780\n",
      "epoch 14: loss 0.084864\n",
      "epoch 15 loss_pde:0.02939783, loss_ib:0.00544246\n",
      "epoch 15 loss_pde:0.02928082, loss_ib:0.00544462\n",
      "epoch 15 loss_pde:0.02923720, loss_ib:0.00544278\n",
      "epoch 15 loss_pde:0.02922004, loss_ib:0.00543761\n",
      "epoch 15 loss_pde:0.02923397, loss_ib:0.00542967\n",
      "epoch 15 loss_pde:0.02922927, loss_ib:0.00542506\n",
      "epoch 15 loss_pde:0.02924588, loss_ib:0.00541772\n",
      "epoch 15 loss_pde:0.02925885, loss_ib:0.00541132\n",
      "epoch 15 loss_pde:0.02929408, loss_ib:0.00540293\n",
      "epoch 15 loss_pde:0.02933095, loss_ib:0.00539451\n",
      "epoch 15 loss_pde:0.02936046, loss_ib:0.00538557\n",
      "epoch 15 loss_pde:0.02953439, loss_ib:0.00536030\n",
      "epoch 15 loss_pde:0.02954542, loss_ib:0.00535363\n",
      "epoch 15 loss_pde:0.02953610, loss_ib:0.00534999\n",
      "epoch 15 loss_pde:0.02949980, loss_ib:0.00534902\n",
      "epoch 15 loss_pde:0.02943703, loss_ib:0.00535019\n",
      "epoch 15 loss_pde:0.02935666, loss_ib:0.00535294\n",
      "epoch 15 loss_pde:0.02927345, loss_ib:0.00535539\n",
      "epoch 15 loss_pde:0.02917219, loss_ib:0.00536023\n",
      "epoch 15 loss_pde:0.02910855, loss_ib:0.00536166\n",
      "epoch 15: loss 0.083822\n",
      "epoch 16 loss_pde:0.02905432, loss_ib:0.00536211\n",
      "epoch 16 loss_pde:0.02902130, loss_ib:0.00536006\n",
      "epoch 16 loss_pde:0.02900512, loss_ib:0.00535646\n",
      "epoch 16 loss_pde:0.02900423, loss_ib:0.00534973\n",
      "epoch 16 loss_pde:0.02901403, loss_ib:0.00534025\n",
      "epoch 16 loss_pde:0.02904157, loss_ib:0.00533089\n",
      "epoch 16 loss_pde:0.02907139, loss_ib:0.00532049\n",
      "epoch 16 loss_pde:0.02908910, loss_ib:0.00530996\n",
      "epoch 16 loss_pde:0.02911508, loss_ib:0.00530061\n",
      "epoch 16 loss_pde:0.02908424, loss_ib:0.00529768\n",
      "epoch 16 loss_pde:0.02906354, loss_ib:0.00529393\n",
      "epoch 16 loss_pde:0.02898599, loss_ib:0.00529700\n",
      "epoch 16 loss_pde:0.02890894, loss_ib:0.00529998\n",
      "epoch 16 loss_pde:0.02879297, loss_ib:0.00530673\n",
      "epoch 16 loss_pde:0.02871149, loss_ib:0.00531098\n",
      "epoch 16 loss_pde:0.02863223, loss_ib:0.00531504\n",
      "epoch 16 loss_pde:0.02857767, loss_ib:0.00531677\n",
      "epoch 16 loss_pde:0.02856590, loss_ib:0.00531522\n",
      "epoch 16 loss_pde:0.02856279, loss_ib:0.00531210\n",
      "epoch 16 loss_pde:0.02860120, loss_ib:0.00530324\n",
      "epoch 16: loss 0.082675\n",
      "epoch 17 loss_pde:0.02863661, loss_ib:0.00529492\n",
      "epoch 17 loss_pde:0.02866548, loss_ib:0.00528733\n",
      "epoch 17 loss_pde:0.02871390, loss_ib:0.00527745\n",
      "epoch 17 loss_pde:0.02871374, loss_ib:0.00527378\n",
      "epoch 17 loss_pde:0.02870110, loss_ib:0.00527150\n",
      "epoch 17 loss_pde:0.02865317, loss_ib:0.00527175\n",
      "epoch 17 loss_pde:0.02861464, loss_ib:0.00527215\n",
      "epoch 17 loss_pde:0.02855150, loss_ib:0.00527398\n",
      "epoch 17 loss_pde:0.02844965, loss_ib:0.00527771\n",
      "epoch 17 loss_pde:0.02836181, loss_ib:0.00527944\n",
      "epoch 17 loss_pde:0.02826707, loss_ib:0.00528312\n",
      "epoch 17 loss_pde:0.02821619, loss_ib:0.00528186\n",
      "epoch 17 loss_pde:0.02814574, loss_ib:0.00528174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 17 loss_pde:0.02814041, loss_ib:0.00527687\n",
      "epoch 17 loss_pde:0.02811569, loss_ib:0.00527325\n",
      "epoch 17 loss_pde:0.02814316, loss_ib:0.00526536\n",
      "epoch 17 loss_pde:0.02818850, loss_ib:0.00525543\n",
      "epoch 17 loss_pde:0.02820786, loss_ib:0.00524660\n",
      "epoch 17 loss_pde:0.02823741, loss_ib:0.00523599\n",
      "epoch 17 loss_pde:0.02818938, loss_ib:0.00523439\n",
      "epoch 17: loss 0.081586\n",
      "epoch 18 loss_pde:0.02816670, loss_ib:0.00523270\n",
      "epoch 18 loss_pde:0.02809986, loss_ib:0.00523562\n",
      "epoch 18 loss_pde:0.02802822, loss_ib:0.00523824\n",
      "epoch 18 loss_pde:0.02790578, loss_ib:0.00524477\n",
      "epoch 18 loss_pde:0.02783748, loss_ib:0.00524647\n",
      "epoch 18 loss_pde:0.02775327, loss_ib:0.00525051\n",
      "epoch 18 loss_pde:0.02770706, loss_ib:0.00525032\n",
      "epoch 18 loss_pde:0.02768547, loss_ib:0.00524701\n",
      "epoch 18 loss_pde:0.02771296, loss_ib:0.00523944\n",
      "epoch 18 loss_pde:0.02777134, loss_ib:0.00523021\n",
      "epoch 18 loss_pde:0.02784221, loss_ib:0.00522050\n",
      "epoch 18 loss_pde:0.02791639, loss_ib:0.00520953\n",
      "epoch 18 loss_pde:0.02802176, loss_ib:0.00519415\n",
      "epoch 18 loss_pde:0.02810038, loss_ib:0.00518147\n",
      "epoch 18 loss_pde:0.02817508, loss_ib:0.00516921\n",
      "epoch 18 loss_pde:0.02819040, loss_ib:0.00516366\n",
      "epoch 18 loss_pde:0.02816566, loss_ib:0.00516203\n",
      "epoch 18 loss_pde:0.02812881, loss_ib:0.00516207\n",
      "epoch 18 loss_pde:0.02806994, loss_ib:0.00516454\n",
      "epoch 18 loss_pde:0.02801548, loss_ib:0.00516606\n",
      "epoch 18: loss 0.080494\n",
      "epoch 19 loss_pde:0.02793079, loss_ib:0.00517027\n",
      "epoch 19 loss_pde:0.02789055, loss_ib:0.00517070\n",
      "epoch 19 loss_pde:0.02785402, loss_ib:0.00516879\n",
      "epoch 19 loss_pde:0.02773523, loss_ib:0.00517668\n",
      "epoch 19 loss_pde:0.02774557, loss_ib:0.00516694\n",
      "epoch 19 loss_pde:0.02777280, loss_ib:0.00515706\n",
      "epoch 19 loss_pde:0.02779473, loss_ib:0.00514921\n",
      "epoch 19 loss_pde:0.02784092, loss_ib:0.00514000\n",
      "epoch 19 loss_pde:0.02784298, loss_ib:0.00513616\n",
      "epoch 19 loss_pde:0.02786022, loss_ib:0.00513153\n",
      "epoch 19 loss_pde:0.02784946, loss_ib:0.00512973\n",
      "epoch 19 loss_pde:0.02782322, loss_ib:0.00512940\n",
      "epoch 19 loss_pde:0.02776647, loss_ib:0.00513179\n",
      "epoch 19 loss_pde:0.02770936, loss_ib:0.00513408\n",
      "epoch 19 loss_pde:0.02762255, loss_ib:0.00513964\n",
      "epoch 19 loss_pde:0.02757610, loss_ib:0.00514177\n",
      "epoch 19 loss_pde:0.02753095, loss_ib:0.00514401\n",
      "epoch 19 loss_pde:0.02750923, loss_ib:0.00514281\n",
      "epoch 19 loss_pde:0.02743051, loss_ib:0.00514548\n",
      "epoch 19 loss_pde:0.02747940, loss_ib:0.00513461\n",
      "epoch 19: loss 0.079634\n",
      "epoch 20 loss_pde:0.02747688, loss_ib:0.00513061\n",
      "epoch 20 loss_pde:0.02750183, loss_ib:0.00512196\n",
      "epoch 20 loss_pde:0.02750989, loss_ib:0.00511315\n",
      "epoch 20 loss_pde:0.02751840, loss_ib:0.00510674\n",
      "epoch 20 loss_pde:0.02752525, loss_ib:0.00510094\n",
      "epoch 20 loss_pde:0.02753774, loss_ib:0.00509432\n",
      "epoch 20 loss_pde:0.02753140, loss_ib:0.00508994\n",
      "epoch 20 loss_pde:0.02752047, loss_ib:0.00508726\n",
      "epoch 20 loss_pde:0.02749949, loss_ib:0.00508582\n",
      "epoch 20 loss_pde:0.02746968, loss_ib:0.00508414\n",
      "epoch 20 loss_pde:0.02744527, loss_ib:0.00508078\n",
      "epoch 20 loss_pde:0.02741080, loss_ib:0.00507865\n",
      "epoch 20 loss_pde:0.02738953, loss_ib:0.00507569\n",
      "epoch 20 loss_pde:0.02735842, loss_ib:0.00507337\n",
      "epoch 20 loss_pde:0.02732481, loss_ib:0.00507000\n",
      "epoch 20 loss_pde:0.02732547, loss_ib:0.00506397\n",
      "epoch 20 loss_pde:0.02727799, loss_ib:0.00506391\n",
      "epoch 20 loss_pde:0.02727970, loss_ib:0.00506004\n",
      "epoch 20 loss_pde:0.02724897, loss_ib:0.00505890\n",
      "epoch 20 loss_pde:0.02728355, loss_ib:0.00504971\n",
      "epoch 20: loss 0.078783\n",
      "epoch 21 loss_pde:0.02723388, loss_ib:0.00504885\n",
      "epoch 21 loss_pde:0.02728490, loss_ib:0.00503715\n",
      "epoch 21 loss_pde:0.02722500, loss_ib:0.00503597\n",
      "epoch 21 loss_pde:0.02719178, loss_ib:0.00503329\n",
      "epoch 21 loss_pde:0.02710522, loss_ib:0.00503548\n",
      "epoch 21 loss_pde:0.02705813, loss_ib:0.00503505\n",
      "epoch 21 loss_pde:0.02700269, loss_ib:0.00503684\n",
      "epoch 21 loss_pde:0.02697193, loss_ib:0.00503616\n",
      "epoch 21 loss_pde:0.02694855, loss_ib:0.00503364\n",
      "epoch 21 loss_pde:0.02692771, loss_ib:0.00502922\n",
      "epoch 21 loss_pde:0.02693506, loss_ib:0.00502217\n",
      "epoch 21 loss_pde:0.02695599, loss_ib:0.00501439\n",
      "epoch 21 loss_pde:0.02702234, loss_ib:0.00500120\n",
      "epoch 21 loss_pde:0.02706669, loss_ib:0.00499070\n",
      "epoch 21 loss_pde:0.02713033, loss_ib:0.00497847\n",
      "epoch 21 loss_pde:0.02715521, loss_ib:0.00496978\n",
      "epoch 21 loss_pde:0.02717859, loss_ib:0.00496208\n",
      "epoch 21 loss_pde:0.02715259, loss_ib:0.00495894\n",
      "epoch 21 loss_pde:0.02717843, loss_ib:0.00495077\n",
      "epoch 21 loss_pde:0.02714780, loss_ib:0.00495026\n",
      "epoch 21: loss 0.077722\n",
      "epoch 22 loss_pde:0.02712796, loss_ib:0.00494800\n",
      "epoch 22 loss_pde:0.02708874, loss_ib:0.00494642\n",
      "epoch 22 loss_pde:0.02707334, loss_ib:0.00494231\n",
      "epoch 22 loss_pde:0.02706284, loss_ib:0.00493831\n",
      "epoch 22 loss_pde:0.02705795, loss_ib:0.00493449\n",
      "epoch 22 loss_pde:0.02706209, loss_ib:0.00493023\n",
      "epoch 22 loss_pde:0.02706098, loss_ib:0.00492625\n",
      "epoch 22 loss_pde:0.02707153, loss_ib:0.00492094\n",
      "epoch 22 loss_pde:0.02707343, loss_ib:0.00491722\n",
      "epoch 22 loss_pde:0.02709209, loss_ib:0.00490735\n",
      "epoch 22 loss_pde:0.02709930, loss_ib:0.00489682\n",
      "epoch 22 loss_pde:0.02709656, loss_ib:0.00488934\n",
      "epoch 22 loss_pde:0.02709673, loss_ib:0.00488467\n",
      "epoch 22 loss_pde:0.02710414, loss_ib:0.00488035\n",
      "epoch 22 loss_pde:0.02709831, loss_ib:0.00487621\n",
      "epoch 22 loss_pde:0.02710001, loss_ib:0.00486974\n",
      "epoch 22 loss_pde:0.02711333, loss_ib:0.00486246\n",
      "epoch 22 loss_pde:0.02712219, loss_ib:0.00485664\n",
      "epoch 22 loss_pde:0.02714550, loss_ib:0.00484970\n",
      "epoch 22 loss_pde:0.02714923, loss_ib:0.00484385\n",
      "epoch 22: loss 0.076608\n",
      "epoch 23 loss_pde:0.02716421, loss_ib:0.00483712\n",
      "epoch 23 loss_pde:0.02715141, loss_ib:0.00483471\n",
      "epoch 23 loss_pde:0.02714148, loss_ib:0.00483312\n",
      "epoch 23 loss_pde:0.02711913, loss_ib:0.00483280\n",
      "epoch 23 loss_pde:0.02708939, loss_ib:0.00483248\n",
      "epoch 23 loss_pde:0.02705936, loss_ib:0.00483087\n",
      "epoch 23 loss_pde:0.02702677, loss_ib:0.00482996\n",
      "epoch 23 loss_pde:0.02701531, loss_ib:0.00482740\n",
      "epoch 23 loss_pde:0.02701323, loss_ib:0.00482446\n",
      "epoch 23 loss_pde:0.02700488, loss_ib:0.00482139\n",
      "epoch 23 loss_pde:0.02703173, loss_ib:0.00481521\n",
      "epoch 23 loss_pde:0.02704969, loss_ib:0.00481023\n",
      "epoch 23 loss_pde:0.02708080, loss_ib:0.00480387\n",
      "epoch 23 loss_pde:0.02710475, loss_ib:0.00479819\n",
      "epoch 23 loss_pde:0.02712576, loss_ib:0.00479193\n",
      "epoch 23 loss_pde:0.02714085, loss_ib:0.00478591\n",
      "epoch 23 loss_pde:0.02712885, loss_ib:0.00478248\n",
      "epoch 23 loss_pde:0.02711994, loss_ib:0.00477957\n",
      "epoch 23 loss_pde:0.02706612, loss_ib:0.00478110\n",
      "epoch 23 loss_pde:0.02700626, loss_ib:0.00478228\n",
      "epoch 23: loss 0.075535\n",
      "epoch 24 loss_pde:0.02691420, loss_ib:0.00478548\n",
      "epoch 24 loss_pde:0.02688001, loss_ib:0.00478323\n",
      "epoch 24 loss_pde:0.02679344, loss_ib:0.00478698\n",
      "epoch 24 loss_pde:0.02677852, loss_ib:0.00478560\n",
      "epoch 24 loss_pde:0.02677406, loss_ib:0.00478222\n",
      "epoch 24 loss_pde:0.02681377, loss_ib:0.00477331\n",
      "epoch 24 loss_pde:0.02685245, loss_ib:0.00476452\n",
      "epoch 24 loss_pde:0.02690873, loss_ib:0.00475472\n",
      "epoch 24 loss_pde:0.02696235, loss_ib:0.00474380\n",
      "epoch 24 loss_pde:0.02700971, loss_ib:0.00473392\n",
      "epoch 24 loss_pde:0.02701843, loss_ib:0.00472769\n",
      "epoch 24 loss_pde:0.02704503, loss_ib:0.00472049\n",
      "epoch 24 loss_pde:0.02702607, loss_ib:0.00471807\n",
      "epoch 24 loss_pde:0.02698916, loss_ib:0.00471668\n",
      "epoch 24 loss_pde:0.02690495, loss_ib:0.00471933\n",
      "epoch 24 loss_pde:0.02683165, loss_ib:0.00472117\n",
      "epoch 24 loss_pde:0.02673939, loss_ib:0.00472490\n",
      "epoch 24 loss_pde:0.02667131, loss_ib:0.00472703\n",
      "epoch 24 loss_pde:0.02662138, loss_ib:0.00472684\n",
      "epoch 24 loss_pde:0.02657708, loss_ib:0.00472543\n",
      "epoch 24: loss 0.074769\n",
      "epoch 25 loss_pde:0.02658066, loss_ib:0.00472051\n",
      "epoch 25 loss_pde:0.02657937, loss_ib:0.00471672\n",
      "epoch 25 loss_pde:0.02659421, loss_ib:0.00471127\n",
      "epoch 25 loss_pde:0.02661587, loss_ib:0.00470477\n",
      "epoch 25 loss_pde:0.02661211, loss_ib:0.00470064\n",
      "epoch 25 loss_pde:0.02660443, loss_ib:0.00469707\n",
      "epoch 25 loss_pde:0.02656873, loss_ib:0.00469594\n",
      "epoch 25 loss_pde:0.02653589, loss_ib:0.00469511\n",
      "epoch 25 loss_pde:0.02650973, loss_ib:0.00469505\n",
      "epoch 25 loss_pde:0.02647303, loss_ib:0.00469510\n",
      "epoch 25 loss_pde:0.02644161, loss_ib:0.00469371\n",
      "epoch 25 loss_pde:0.02644130, loss_ib:0.00468925\n",
      "epoch 25 loss_pde:0.02643514, loss_ib:0.00468580\n",
      "epoch 25 loss_pde:0.02645476, loss_ib:0.00468029\n",
      "epoch 25 loss_pde:0.02647189, loss_ib:0.00467500\n",
      "epoch 25 loss_pde:0.02650884, loss_ib:0.00466787\n",
      "epoch 25 loss_pde:0.02654028, loss_ib:0.00466167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 25 loss_pde:0.02657322, loss_ib:0.00465483\n",
      "epoch 25 loss_pde:0.02660254, loss_ib:0.00464795\n",
      "epoch 25 loss_pde:0.02661223, loss_ib:0.00464286\n",
      "epoch 25: loss 0.073786\n",
      "epoch 26 loss_pde:0.02661152, loss_ib:0.00463908\n",
      "epoch 26 loss_pde:0.02659734, loss_ib:0.00463718\n",
      "epoch 26 loss_pde:0.02657875, loss_ib:0.00463480\n",
      "epoch 26 loss_pde:0.02655605, loss_ib:0.00463307\n",
      "epoch 26 loss_pde:0.02655263, loss_ib:0.00462951\n",
      "epoch 26 loss_pde:0.02654073, loss_ib:0.00462680\n",
      "epoch 26 loss_pde:0.02654080, loss_ib:0.00462243\n",
      "epoch 26 loss_pde:0.02653199, loss_ib:0.00462006\n",
      "epoch 26 loss_pde:0.02653591, loss_ib:0.00461634\n",
      "epoch 26 loss_pde:0.02654013, loss_ib:0.00461225\n",
      "epoch 26 loss_pde:0.02655605, loss_ib:0.00460715\n",
      "epoch 26 loss_pde:0.02656636, loss_ib:0.00460279\n",
      "epoch 26 loss_pde:0.02658215, loss_ib:0.00459762\n",
      "epoch 26 loss_pde:0.02659345, loss_ib:0.00459273\n",
      "epoch 26 loss_pde:0.02659893, loss_ib:0.00458826\n",
      "epoch 26 loss_pde:0.02660128, loss_ib:0.00458430\n",
      "epoch 26 loss_pde:0.02660021, loss_ib:0.00458069\n",
      "epoch 26 loss_pde:0.02661146, loss_ib:0.00457583\n",
      "epoch 26 loss_pde:0.02660132, loss_ib:0.00457451\n",
      "epoch 26 loss_pde:0.02663036, loss_ib:0.00456859\n",
      "epoch 26: loss 0.073002\n",
      "epoch 27 loss_pde:0.02663254, loss_ib:0.00456526\n",
      "epoch 27 loss_pde:0.02665397, loss_ib:0.00456033\n",
      "epoch 27 loss_pde:0.02666507, loss_ib:0.00455608\n",
      "epoch 27 loss_pde:0.02669060, loss_ib:0.00455067\n",
      "epoch 27 loss_pde:0.02672067, loss_ib:0.00454484\n",
      "epoch 27 loss_pde:0.02677796, loss_ib:0.00453544\n",
      "epoch 27 loss_pde:0.02679115, loss_ib:0.00452978\n",
      "epoch 27 loss_pde:0.02681550, loss_ib:0.00452379\n",
      "epoch 27 loss_pde:0.02681284, loss_ib:0.00452031\n",
      "epoch 27 loss_pde:0.02679780, loss_ib:0.00451860\n",
      "epoch 27 loss_pde:0.02674751, loss_ib:0.00452089\n",
      "epoch 27 loss_pde:0.02671576, loss_ib:0.00452157\n",
      "epoch 27 loss_pde:0.02665091, loss_ib:0.00452546\n",
      "epoch 27 loss_pde:0.02660721, loss_ib:0.00452713\n",
      "epoch 27 loss_pde:0.02656062, loss_ib:0.00452914\n",
      "epoch 27 loss_pde:0.02651122, loss_ib:0.00453050\n",
      "epoch 27 loss_pde:0.02648401, loss_ib:0.00452928\n",
      "epoch 27 loss_pde:0.02644695, loss_ib:0.00452845\n",
      "epoch 27 loss_pde:0.02650018, loss_ib:0.00451809\n",
      "epoch 27 loss_pde:0.02651154, loss_ib:0.00451352\n",
      "epoch 27: loss 0.072285\n",
      "epoch 28 loss_pde:0.02654522, loss_ib:0.00450719\n",
      "epoch 28 loss_pde:0.02654304, loss_ib:0.00450255\n",
      "epoch 28 loss_pde:0.02655546, loss_ib:0.00449678\n",
      "epoch 28 loss_pde:0.02654278, loss_ib:0.00449204\n",
      "epoch 28 loss_pde:0.02652205, loss_ib:0.00448800\n",
      "epoch 28 loss_pde:0.02648859, loss_ib:0.00448748\n",
      "epoch 28 loss_pde:0.02645313, loss_ib:0.00448818\n",
      "epoch 28 loss_pde:0.02640506, loss_ib:0.00449043\n",
      "epoch 28 loss_pde:0.02637037, loss_ib:0.00449096\n",
      "epoch 28 loss_pde:0.02630500, loss_ib:0.00449404\n",
      "epoch 28 loss_pde:0.02630370, loss_ib:0.00449091\n",
      "epoch 28 loss_pde:0.02627471, loss_ib:0.00449136\n",
      "epoch 28 loss_pde:0.02625773, loss_ib:0.00448945\n",
      "epoch 28 loss_pde:0.02625982, loss_ib:0.00448534\n",
      "epoch 28 loss_pde:0.02626898, loss_ib:0.00448050\n",
      "epoch 28 loss_pde:0.02629885, loss_ib:0.00447374\n",
      "epoch 28 loss_pde:0.02632282, loss_ib:0.00446783\n",
      "epoch 28 loss_pde:0.02633557, loss_ib:0.00446413\n",
      "epoch 28 loss_pde:0.02635237, loss_ib:0.00445925\n",
      "epoch 28 loss_pde:0.02630539, loss_ib:0.00446003\n",
      "epoch 28: loss 0.071617\n",
      "epoch 29 loss_pde:0.02627781, loss_ib:0.00446024\n",
      "epoch 29 loss_pde:0.02622780, loss_ib:0.00446197\n",
      "epoch 29 loss_pde:0.02618086, loss_ib:0.00446223\n",
      "epoch 29 loss_pde:0.02607987, loss_ib:0.00446699\n",
      "epoch 29 loss_pde:0.02605317, loss_ib:0.00446585\n",
      "epoch 29 loss_pde:0.02602440, loss_ib:0.00446524\n",
      "epoch 29 loss_pde:0.02603165, loss_ib:0.00446094\n",
      "epoch 29 loss_pde:0.02603778, loss_ib:0.00445678\n",
      "epoch 29 loss_pde:0.02606755, loss_ib:0.00444901\n",
      "epoch 29 loss_pde:0.02606077, loss_ib:0.00444250\n",
      "epoch 29 loss_pde:0.02611117, loss_ib:0.00443167\n",
      "epoch 29 loss_pde:0.02614587, loss_ib:0.00442314\n",
      "epoch 29 loss_pde:0.02616957, loss_ib:0.00441745\n",
      "epoch 29 loss_pde:0.02617348, loss_ib:0.00441426\n",
      "epoch 29 loss_pde:0.02616308, loss_ib:0.00440954\n",
      "epoch 29 loss_pde:0.02617722, loss_ib:0.00440199\n",
      "epoch 29 loss_pde:0.02617434, loss_ib:0.00439624\n",
      "epoch 29 loss_pde:0.02615962, loss_ib:0.00439006\n",
      "epoch 29 loss_pde:0.02612474, loss_ib:0.00438669\n",
      "epoch 29 loss_pde:0.02614098, loss_ib:0.00438036\n",
      "epoch 29: loss 0.070880\n",
      "epoch 30 loss_pde:0.02613069, loss_ib:0.00437845\n",
      "epoch 30 loss_pde:0.02611705, loss_ib:0.00437615\n",
      "epoch 30 loss_pde:0.02609210, loss_ib:0.00437410\n",
      "epoch 30 loss_pde:0.02604415, loss_ib:0.00437439\n",
      "epoch 30 loss_pde:0.02601352, loss_ib:0.00437418\n",
      "epoch 30 loss_pde:0.02594471, loss_ib:0.00437694\n",
      "epoch 30 loss_pde:0.02589047, loss_ib:0.00437791\n",
      "epoch 30 loss_pde:0.02582357, loss_ib:0.00438098\n",
      "epoch 30 loss_pde:0.02579223, loss_ib:0.00438010\n",
      "epoch 30 loss_pde:0.02572330, loss_ib:0.00438268\n",
      "epoch 30 loss_pde:0.02571298, loss_ib:0.00438130\n",
      "epoch 30 loss_pde:0.02572542, loss_ib:0.00437715\n",
      "epoch 30 loss_pde:0.02575698, loss_ib:0.00437050\n",
      "epoch 30 loss_pde:0.02582586, loss_ib:0.00435954\n",
      "epoch 30 loss_pde:0.02589240, loss_ib:0.00434911\n",
      "epoch 30 loss_pde:0.02598617, loss_ib:0.00433658\n",
      "epoch 30 loss_pde:0.02604580, loss_ib:0.00432794\n",
      "epoch 30 loss_pde:0.02611169, loss_ib:0.00431882\n",
      "epoch 30 loss_pde:0.02613875, loss_ib:0.00431378\n",
      "epoch 30 loss_pde:0.02614775, loss_ib:0.00431076\n",
      "epoch 30: loss 0.069915\n",
      "epoch 31 loss_pde:0.02612805, loss_ib:0.00431045\n",
      "epoch 31 loss_pde:0.02608654, loss_ib:0.00431157\n",
      "epoch 31 loss_pde:0.02601138, loss_ib:0.00431496\n",
      "epoch 31 loss_pde:0.02591446, loss_ib:0.00432009\n",
      "epoch 31 loss_pde:0.02583577, loss_ib:0.00432399\n",
      "epoch 31 loss_pde:0.02576755, loss_ib:0.00432751\n",
      "epoch 31 loss_pde:0.02569950, loss_ib:0.00433146\n",
      "epoch 31 loss_pde:0.02567879, loss_ib:0.00433128\n",
      "epoch 31 loss_pde:0.02566928, loss_ib:0.00432996\n",
      "epoch 31 loss_pde:0.02566654, loss_ib:0.00432803\n",
      "epoch 31 loss_pde:0.02567045, loss_ib:0.00432505\n",
      "epoch 31 loss_pde:0.02565644, loss_ib:0.00432380\n",
      "epoch 31 loss_pde:0.02567093, loss_ib:0.00432003\n",
      "epoch 31 loss_pde:0.02566955, loss_ib:0.00431739\n",
      "epoch 31 loss_pde:0.02566682, loss_ib:0.00431475\n",
      "epoch 31 loss_pde:0.02564419, loss_ib:0.00431438\n",
      "epoch 31 loss_pde:0.02562276, loss_ib:0.00431421\n",
      "epoch 31 loss_pde:0.02558597, loss_ib:0.00431537\n",
      "epoch 31 loss_pde:0.02554409, loss_ib:0.00431673\n",
      "epoch 31 loss_pde:0.02551141, loss_ib:0.00431717\n",
      "epoch 31: loss 0.069233\n",
      "epoch 32 loss_pde:0.02547750, loss_ib:0.00431838\n",
      "epoch 32 loss_pde:0.02545383, loss_ib:0.00431818\n",
      "epoch 32 loss_pde:0.02543457, loss_ib:0.00431643\n",
      "epoch 32 loss_pde:0.02546440, loss_ib:0.00431070\n",
      "epoch 32 loss_pde:0.02546353, loss_ib:0.00430687\n",
      "epoch 32 loss_pde:0.02548586, loss_ib:0.00430148\n",
      "epoch 32 loss_pde:0.02552054, loss_ib:0.00429566\n",
      "epoch 32 loss_pde:0.02555439, loss_ib:0.00429055\n",
      "epoch 32 loss_pde:0.02558203, loss_ib:0.00428628\n",
      "epoch 32 loss_pde:0.02560130, loss_ib:0.00428258\n",
      "epoch 32 loss_pde:0.02560554, loss_ib:0.00427928\n",
      "epoch 32 loss_pde:0.02558872, loss_ib:0.00427668\n",
      "epoch 32 loss_pde:0.02554579, loss_ib:0.00427695\n",
      "epoch 32 loss_pde:0.02551360, loss_ib:0.00427704\n",
      "epoch 32 loss_pde:0.02544599, loss_ib:0.00428048\n",
      "epoch 32 loss_pde:0.02541099, loss_ib:0.00428068\n",
      "epoch 32 loss_pde:0.02534633, loss_ib:0.00428422\n",
      "epoch 32 loss_pde:0.02531746, loss_ib:0.00428450\n",
      "epoch 32 loss_pde:0.02527706, loss_ib:0.00428479\n",
      "epoch 32 loss_pde:0.02526668, loss_ib:0.00428034\n",
      "epoch 32: loss 0.068661\n",
      "epoch 33 loss_pde:0.02528021, loss_ib:0.00427432\n",
      "epoch 33 loss_pde:0.02526803, loss_ib:0.00427146\n",
      "epoch 33 loss_pde:0.02529974, loss_ib:0.00426472\n",
      "epoch 33 loss_pde:0.02529659, loss_ib:0.00426192\n",
      "epoch 33 loss_pde:0.02532612, loss_ib:0.00425620\n",
      "epoch 33 loss_pde:0.02533628, loss_ib:0.00425295\n",
      "epoch 33 loss_pde:0.02532902, loss_ib:0.00425202\n",
      "epoch 33 loss_pde:0.02531912, loss_ib:0.00425145\n",
      "epoch 33 loss_pde:0.02529096, loss_ib:0.00425174\n",
      "epoch 33 loss_pde:0.02525077, loss_ib:0.00425183\n",
      "epoch 33 loss_pde:0.02520540, loss_ib:0.00425209\n",
      "epoch 33 loss_pde:0.02515152, loss_ib:0.00425464\n",
      "epoch 33 loss_pde:0.02514994, loss_ib:0.00425224\n",
      "epoch 33 loss_pde:0.02514898, loss_ib:0.00424921\n",
      "epoch 33 loss_pde:0.02515546, loss_ib:0.00424580\n",
      "epoch 33 loss_pde:0.02516668, loss_ib:0.00424219\n",
      "epoch 33 loss_pde:0.02518049, loss_ib:0.00423830\n",
      "epoch 33 loss_pde:0.02519162, loss_ib:0.00423420\n",
      "epoch 33 loss_pde:0.02517735, loss_ib:0.00423172\n",
      "epoch 33 loss_pde:0.02518543, loss_ib:0.00422725\n",
      "epoch 33: loss 0.068023\n",
      "epoch 34 loss_pde:0.02516718, loss_ib:0.00422635\n",
      "epoch 34 loss_pde:0.02515442, loss_ib:0.00422455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 34 loss_pde:0.02511421, loss_ib:0.00422526\n",
      "epoch 34 loss_pde:0.02509322, loss_ib:0.00422488\n",
      "epoch 34 loss_pde:0.02507789, loss_ib:0.00422409\n",
      "epoch 34 loss_pde:0.02505924, loss_ib:0.00422224\n",
      "epoch 34 loss_pde:0.02507516, loss_ib:0.00421696\n",
      "epoch 34 loss_pde:0.02509157, loss_ib:0.00421307\n",
      "epoch 34 loss_pde:0.02511746, loss_ib:0.00420816\n",
      "epoch 34 loss_pde:0.02516045, loss_ib:0.00420151\n",
      "epoch 34 loss_pde:0.02520055, loss_ib:0.00419537\n",
      "epoch 34 loss_pde:0.02525401, loss_ib:0.00418803\n",
      "epoch 34 loss_pde:0.02529725, loss_ib:0.00418158\n",
      "epoch 34 loss_pde:0.02535566, loss_ib:0.00417332\n",
      "epoch 34 loss_pde:0.02539590, loss_ib:0.00416637\n",
      "epoch 34 loss_pde:0.02545025, loss_ib:0.00415729\n",
      "epoch 34 loss_pde:0.02548004, loss_ib:0.00415097\n",
      "epoch 34 loss_pde:0.02548922, loss_ib:0.00414660\n",
      "epoch 34 loss_pde:0.02549326, loss_ib:0.00414280\n",
      "epoch 34 loss_pde:0.02547507, loss_ib:0.00414168\n",
      "epoch 34: loss 0.067431\n",
      "epoch 35 loss_pde:0.02546358, loss_ib:0.00414003\n",
      "epoch 35 loss_pde:0.02544754, loss_ib:0.00413937\n",
      "epoch 35 loss_pde:0.02543812, loss_ib:0.00413710\n",
      "epoch 35 loss_pde:0.02543019, loss_ib:0.00413340\n",
      "epoch 35 loss_pde:0.02544891, loss_ib:0.00412699\n",
      "epoch 35 loss_pde:0.02546289, loss_ib:0.00412125\n",
      "epoch 35 loss_pde:0.02547495, loss_ib:0.00411624\n",
      "epoch 35 loss_pde:0.02555570, loss_ib:0.00410440\n",
      "epoch 35 loss_pde:0.02555477, loss_ib:0.00410131\n",
      "epoch 35 loss_pde:0.02555535, loss_ib:0.00409776\n",
      "epoch 35 loss_pde:0.02554630, loss_ib:0.00409502\n",
      "epoch 35 loss_pde:0.02554368, loss_ib:0.00409181\n",
      "epoch 35 loss_pde:0.02554372, loss_ib:0.00408807\n",
      "epoch 35 loss_pde:0.02552347, loss_ib:0.00408586\n",
      "epoch 35 loss_pde:0.02552536, loss_ib:0.00408246\n",
      "epoch 35 loss_pde:0.02551268, loss_ib:0.00408042\n",
      "epoch 35 loss_pde:0.02552238, loss_ib:0.00407550\n",
      "epoch 35 loss_pde:0.02548880, loss_ib:0.00407479\n",
      "epoch 35 loss_pde:0.02548727, loss_ib:0.00407186\n",
      "epoch 35 loss_pde:0.02546222, loss_ib:0.00407145\n",
      "epoch 35: loss 0.066864\n",
      "epoch 36 loss_pde:0.02545121, loss_ib:0.00406914\n",
      "epoch 36 loss_pde:0.02539121, loss_ib:0.00407116\n",
      "epoch 36 loss_pde:0.02537148, loss_ib:0.00406996\n",
      "epoch 36 loss_pde:0.02530634, loss_ib:0.00407297\n",
      "epoch 36 loss_pde:0.02527814, loss_ib:0.00407264\n",
      "epoch 36 loss_pde:0.02526390, loss_ib:0.00407045\n",
      "epoch 36 loss_pde:0.02522907, loss_ib:0.00406938\n",
      "epoch 36 loss_pde:0.02522496, loss_ib:0.00406611\n",
      "epoch 36 loss_pde:0.02522385, loss_ib:0.00406282\n",
      "epoch 36 loss_pde:0.02526105, loss_ib:0.00405540\n",
      "epoch 36 loss_pde:0.02526741, loss_ib:0.00405224\n",
      "epoch 36 loss_pde:0.02527590, loss_ib:0.00404882\n",
      "epoch 36 loss_pde:0.02527839, loss_ib:0.00404531\n",
      "epoch 36 loss_pde:0.02527918, loss_ib:0.00404365\n",
      "epoch 36 loss_pde:0.02527321, loss_ib:0.00404059\n",
      "epoch 36 loss_pde:0.02526432, loss_ib:0.00403889\n",
      "epoch 36 loss_pde:0.02524559, loss_ib:0.00403810\n",
      "epoch 36 loss_pde:0.02522717, loss_ib:0.00403721\n",
      "epoch 36 loss_pde:0.02519973, loss_ib:0.00403703\n",
      "epoch 36 loss_pde:0.02517919, loss_ib:0.00403613\n",
      "epoch 36: loss 0.066143\n",
      "epoch 37 loss_pde:0.02513786, loss_ib:0.00403662\n",
      "epoch 37 loss_pde:0.02514526, loss_ib:0.00403241\n",
      "epoch 37 loss_pde:0.02513477, loss_ib:0.00403094\n",
      "epoch 37 loss_pde:0.02514307, loss_ib:0.00402734\n",
      "epoch 37 loss_pde:0.02514751, loss_ib:0.00402391\n",
      "epoch 37 loss_pde:0.02516486, loss_ib:0.00401887\n",
      "epoch 37 loss_pde:0.02518673, loss_ib:0.00401298\n",
      "epoch 37 loss_pde:0.02521713, loss_ib:0.00400685\n",
      "epoch 37 loss_pde:0.02523990, loss_ib:0.00400209\n",
      "epoch 37 loss_pde:0.02525960, loss_ib:0.00399767\n",
      "epoch 37 loss_pde:0.02526354, loss_ib:0.00399474\n",
      "epoch 37 loss_pde:0.02526424, loss_ib:0.00399248\n",
      "epoch 37 loss_pde:0.02525208, loss_ib:0.00399154\n",
      "epoch 37 loss_pde:0.02523672, loss_ib:0.00399065\n",
      "epoch 37 loss_pde:0.02522318, loss_ib:0.00398853\n",
      "epoch 37 loss_pde:0.02520192, loss_ib:0.00398607\n",
      "epoch 37 loss_pde:0.02518174, loss_ib:0.00398397\n",
      "epoch 37 loss_pde:0.02517343, loss_ib:0.00398134\n",
      "epoch 37 loss_pde:0.02516126, loss_ib:0.00397793\n",
      "epoch 37 loss_pde:0.02515245, loss_ib:0.00397353\n",
      "epoch 37: loss 0.065504\n",
      "epoch 38 loss_pde:0.02516134, loss_ib:0.00396798\n",
      "epoch 38 loss_pde:0.02515967, loss_ib:0.00396488\n",
      "epoch 38 loss_pde:0.02516016, loss_ib:0.00396177\n",
      "epoch 38 loss_pde:0.02516657, loss_ib:0.00395749\n",
      "epoch 38 loss_pde:0.02516086, loss_ib:0.00395488\n",
      "epoch 38 loss_pde:0.02516742, loss_ib:0.00395125\n",
      "epoch 38 loss_pde:0.02515115, loss_ib:0.00394979\n",
      "epoch 38 loss_pde:0.02516910, loss_ib:0.00394486\n",
      "epoch 38 loss_pde:0.02515589, loss_ib:0.00394339\n",
      "epoch 38 loss_pde:0.02516061, loss_ib:0.00394025\n",
      "epoch 38 loss_pde:0.02515259, loss_ib:0.00393775\n",
      "epoch 38 loss_pde:0.02516014, loss_ib:0.00393347\n",
      "epoch 38 loss_pde:0.02515334, loss_ib:0.00393037\n",
      "epoch 38 loss_pde:0.02520727, loss_ib:0.00392110\n",
      "epoch 38 loss_pde:0.02518014, loss_ib:0.00392027\n",
      "epoch 38 loss_pde:0.02517542, loss_ib:0.00391871\n",
      "epoch 38 loss_pde:0.02516132, loss_ib:0.00391675\n",
      "epoch 38 loss_pde:0.02513316, loss_ib:0.00391553\n",
      "epoch 38 loss_pde:0.02510657, loss_ib:0.00391364\n",
      "epoch 38 loss_pde:0.02507485, loss_ib:0.00391242\n",
      "epoch 38: loss 0.064841\n",
      "epoch 39 loss_pde:0.02506624, loss_ib:0.00390976\n",
      "epoch 39 loss_pde:0.02504468, loss_ib:0.00390887\n",
      "epoch 39 loss_pde:0.02503629, loss_ib:0.00390728\n",
      "epoch 39 loss_pde:0.02503839, loss_ib:0.00390385\n",
      "epoch 39 loss_pde:0.02504609, loss_ib:0.00389865\n",
      "epoch 39 loss_pde:0.02507975, loss_ib:0.00389093\n",
      "epoch 39 loss_pde:0.02510940, loss_ib:0.00388392\n",
      "epoch 39 loss_pde:0.02513235, loss_ib:0.00387773\n",
      "epoch 39 loss_pde:0.02514640, loss_ib:0.00387275\n",
      "epoch 39 loss_pde:0.02516501, loss_ib:0.00386804\n",
      "epoch 39 loss_pde:0.02516357, loss_ib:0.00386520\n",
      "epoch 39 loss_pde:0.02515497, loss_ib:0.00386188\n",
      "epoch 39 loss_pde:0.02512255, loss_ib:0.00386099\n",
      "epoch 39 loss_pde:0.02507699, loss_ib:0.00386032\n",
      "epoch 39 loss_pde:0.02499593, loss_ib:0.00386258\n",
      "epoch 39 loss_pde:0.02496574, loss_ib:0.00386163\n",
      "epoch 39 loss_pde:0.02492131, loss_ib:0.00386371\n",
      "epoch 39 loss_pde:0.02490037, loss_ib:0.00386339\n",
      "epoch 39 loss_pde:0.02487392, loss_ib:0.00386281\n",
      "epoch 39 loss_pde:0.02487448, loss_ib:0.00385915\n",
      "epoch 39: loss 0.064164\n",
      "epoch 40 loss_pde:0.02488543, loss_ib:0.00385442\n",
      "epoch 40 loss_pde:0.02491901, loss_ib:0.00384786\n",
      "epoch 40 loss_pde:0.02496222, loss_ib:0.00384040\n",
      "epoch 40 loss_pde:0.02499870, loss_ib:0.00383433\n",
      "epoch 40 loss_pde:0.02503096, loss_ib:0.00382860\n",
      "epoch 40 loss_pde:0.02505025, loss_ib:0.00382322\n",
      "epoch 40 loss_pde:0.02506835, loss_ib:0.00381711\n",
      "epoch 40 loss_pde:0.02502002, loss_ib:0.00381759\n",
      "epoch 40 loss_pde:0.02500897, loss_ib:0.00381557\n",
      "epoch 40 loss_pde:0.02496910, loss_ib:0.00381574\n",
      "epoch 40 loss_pde:0.02492970, loss_ib:0.00381603\n",
      "epoch 40 loss_pde:0.02486423, loss_ib:0.00381957\n",
      "epoch 40 loss_pde:0.02482415, loss_ib:0.00382137\n",
      "epoch 40 loss_pde:0.02477677, loss_ib:0.00382402\n",
      "epoch 40 loss_pde:0.02473380, loss_ib:0.00382504\n",
      "epoch 40 loss_pde:0.02467316, loss_ib:0.00382621\n",
      "epoch 40 loss_pde:0.02464113, loss_ib:0.00382580\n",
      "epoch 40 loss_pde:0.02462499, loss_ib:0.00382408\n",
      "epoch 40 loss_pde:0.02459366, loss_ib:0.00382310\n",
      "epoch 40 loss_pde:0.02460671, loss_ib:0.00381751\n",
      "epoch 40: loss 0.063430\n",
      "epoch 41 loss_pde:0.02459455, loss_ib:0.00381604\n",
      "epoch 41 loss_pde:0.02458068, loss_ib:0.00381464\n",
      "epoch 41 loss_pde:0.02456545, loss_ib:0.00381337\n",
      "epoch 41 loss_pde:0.02454682, loss_ib:0.00381243\n",
      "epoch 41 loss_pde:0.02453215, loss_ib:0.00381084\n",
      "epoch 41 loss_pde:0.02450835, loss_ib:0.00381070\n",
      "epoch 41 loss_pde:0.02449463, loss_ib:0.00380921\n",
      "epoch 41 loss_pde:0.02445518, loss_ib:0.00380994\n",
      "epoch 41 loss_pde:0.02444404, loss_ib:0.00380849\n",
      "epoch 41 loss_pde:0.02443289, loss_ib:0.00380644\n",
      "epoch 41 loss_pde:0.02441309, loss_ib:0.00380431\n",
      "epoch 41 loss_pde:0.02441977, loss_ib:0.00380120\n",
      "epoch 41 loss_pde:0.02442039, loss_ib:0.00379844\n",
      "epoch 41 loss_pde:0.02441292, loss_ib:0.00379681\n",
      "epoch 41 loss_pde:0.02440682, loss_ib:0.00379519\n",
      "epoch 41 loss_pde:0.02439715, loss_ib:0.00379389\n",
      "epoch 41 loss_pde:0.02438443, loss_ib:0.00379225\n",
      "epoch 41 loss_pde:0.02437683, loss_ib:0.00378920\n",
      "epoch 41 loss_pde:0.02435353, loss_ib:0.00378762\n",
      "epoch 41 loss_pde:0.02434603, loss_ib:0.00378609\n",
      "epoch 41: loss 0.062755\n",
      "epoch 42 loss_pde:0.02434627, loss_ib:0.00378290\n",
      "epoch 42 loss_pde:0.02432028, loss_ib:0.00378268\n",
      "epoch 42 loss_pde:0.02432508, loss_ib:0.00378057\n",
      "epoch 42 loss_pde:0.02433456, loss_ib:0.00377801\n",
      "epoch 42 loss_pde:0.02435834, loss_ib:0.00377302\n",
      "epoch 42 loss_pde:0.02437762, loss_ib:0.00376765\n",
      "epoch 42 loss_pde:0.02445311, loss_ib:0.00375667\n",
      "epoch 42 loss_pde:0.02445796, loss_ib:0.00375393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 42 loss_pde:0.02446217, loss_ib:0.00375081\n",
      "epoch 42 loss_pde:0.02443537, loss_ib:0.00375054\n",
      "epoch 42 loss_pde:0.02441547, loss_ib:0.00374989\n",
      "epoch 42 loss_pde:0.02439315, loss_ib:0.00375032\n",
      "epoch 42 loss_pde:0.02434656, loss_ib:0.00375247\n",
      "epoch 42 loss_pde:0.02429215, loss_ib:0.00375372\n",
      "epoch 42 loss_pde:0.02418520, loss_ib:0.00375877\n",
      "epoch 42 loss_pde:0.02413755, loss_ib:0.00375925\n",
      "epoch 42 loss_pde:0.02402858, loss_ib:0.00376426\n",
      "epoch 42 loss_pde:0.02399775, loss_ib:0.00376267\n",
      "epoch 42 loss_pde:0.02397756, loss_ib:0.00376046\n",
      "epoch 42 loss_pde:0.02396329, loss_ib:0.00375745\n",
      "epoch 42: loss 0.062175\n",
      "epoch 43 loss_pde:0.02395645, loss_ib:0.00375361\n",
      "epoch 43 loss_pde:0.02393539, loss_ib:0.00375076\n",
      "epoch 43 loss_pde:0.02392615, loss_ib:0.00374564\n",
      "epoch 43 loss_pde:0.02389900, loss_ib:0.00375301\n",
      "epoch 43 loss_pde:0.02388725, loss_ib:0.00374467\n",
      "epoch 43 loss_pde:0.02388614, loss_ib:0.00374004\n",
      "epoch 43 loss_pde:0.02387956, loss_ib:0.00373755\n",
      "epoch 43 loss_pde:0.02388094, loss_ib:0.00373476\n",
      "epoch 43 loss_pde:0.02387491, loss_ib:0.00373298\n",
      "epoch 43 loss_pde:0.02388906, loss_ib:0.00372904\n",
      "epoch 43 loss_pde:0.02389123, loss_ib:0.00372630\n",
      "epoch 43 loss_pde:0.02390376, loss_ib:0.00372244\n",
      "epoch 43 loss_pde:0.02391125, loss_ib:0.00371917\n",
      "epoch 43 loss_pde:0.02392251, loss_ib:0.00371575\n",
      "epoch 43 loss_pde:0.02393751, loss_ib:0.00371167\n",
      "epoch 43 loss_pde:0.02395660, loss_ib:0.00370689\n",
      "epoch 43 loss_pde:0.02397181, loss_ib:0.00370217\n",
      "epoch 43 loss_pde:0.02402707, loss_ib:0.00369351\n",
      "epoch 43 loss_pde:0.02401958, loss_ib:0.00369077\n",
      "epoch 43 loss_pde:0.02401916, loss_ib:0.00368889\n",
      "epoch 43: loss 0.061493\n",
      "epoch 44 loss_pde:0.02400105, loss_ib:0.00368732\n",
      "epoch 44 loss_pde:0.02396572, loss_ib:0.00368729\n",
      "epoch 44 loss_pde:0.02392302, loss_ib:0.00368793\n",
      "epoch 44 loss_pde:0.02387251, loss_ib:0.00369007\n",
      "epoch 44 loss_pde:0.02380931, loss_ib:0.00369404\n",
      "epoch 44 loss_pde:0.02377937, loss_ib:0.00369524\n",
      "epoch 44 loss_pde:0.02374035, loss_ib:0.00369689\n",
      "epoch 44 loss_pde:0.02370233, loss_ib:0.00369848\n",
      "epoch 44 loss_pde:0.02366965, loss_ib:0.00369971\n",
      "epoch 44 loss_pde:0.02364065, loss_ib:0.00370046\n",
      "epoch 44 loss_pde:0.02361206, loss_ib:0.00370033\n",
      "epoch 44 loss_pde:0.02354392, loss_ib:0.00370355\n",
      "epoch 44 loss_pde:0.02353515, loss_ib:0.00370132\n",
      "epoch 44 loss_pde:0.02352092, loss_ib:0.00369905\n",
      "epoch 44 loss_pde:0.02350927, loss_ib:0.00369627\n",
      "epoch 44 loss_pde:0.02349777, loss_ib:0.00369390\n",
      "epoch 44 loss_pde:0.02349156, loss_ib:0.00369104\n",
      "epoch 44 loss_pde:0.02347817, loss_ib:0.00368883\n",
      "epoch 44 loss_pde:0.02348530, loss_ib:0.00368479\n",
      "epoch 44 loss_pde:0.02347335, loss_ib:0.00368363\n",
      "epoch 44: loss 0.060874\n",
      "epoch 45 loss_pde:0.02346503, loss_ib:0.00368178\n",
      "epoch 45 loss_pde:0.02344364, loss_ib:0.00368009\n",
      "epoch 45 loss_pde:0.02343767, loss_ib:0.00367737\n",
      "epoch 45 loss_pde:0.02344055, loss_ib:0.00367379\n",
      "epoch 45 loss_pde:0.02343904, loss_ib:0.00366986\n",
      "epoch 45 loss_pde:0.02345205, loss_ib:0.00366495\n",
      "epoch 45 loss_pde:0.02345796, loss_ib:0.00366035\n",
      "epoch 45 loss_pde:0.02344171, loss_ib:0.00365636\n",
      "epoch 45 loss_pde:0.02344318, loss_ib:0.00365071\n",
      "epoch 45 loss_pde:0.02337764, loss_ib:0.00365372\n",
      "epoch 45 loss_pde:0.02339164, loss_ib:0.00364928\n",
      "epoch 45 loss_pde:0.02339241, loss_ib:0.00364617\n",
      "epoch 45 loss_pde:0.02338557, loss_ib:0.00364407\n",
      "epoch 45 loss_pde:0.02337713, loss_ib:0.00364239\n",
      "epoch 45 loss_pde:0.02336706, loss_ib:0.00364078\n",
      "epoch 45 loss_pde:0.02335820, loss_ib:0.00363866\n",
      "epoch 45 loss_pde:0.02334985, loss_ib:0.00363565\n",
      "epoch 45 loss_pde:0.02334292, loss_ib:0.00363141\n",
      "epoch 45 loss_pde:0.02333998, loss_ib:0.00362629\n",
      "epoch 45 loss_pde:0.02334692, loss_ib:0.00362164\n",
      "epoch 45: loss 0.060283\n",
      "epoch 46 loss_pde:0.02335395, loss_ib:0.00361791\n",
      "epoch 46 loss_pde:0.02335985, loss_ib:0.00361509\n",
      "epoch 46 loss_pde:0.02340693, loss_ib:0.00360855\n",
      "epoch 46 loss_pde:0.02340187, loss_ib:0.00360764\n",
      "epoch 46 loss_pde:0.02339091, loss_ib:0.00360655\n",
      "epoch 46 loss_pde:0.02337618, loss_ib:0.00360515\n",
      "epoch 46 loss_pde:0.02336043, loss_ib:0.00360361\n",
      "epoch 46 loss_pde:0.02334561, loss_ib:0.00360203\n",
      "epoch 46 loss_pde:0.02332948, loss_ib:0.00360040\n",
      "epoch 46 loss_pde:0.02331544, loss_ib:0.00359835\n",
      "epoch 46 loss_pde:0.02329646, loss_ib:0.00359682\n",
      "epoch 46 loss_pde:0.02328869, loss_ib:0.00359451\n",
      "epoch 46 loss_pde:0.02327658, loss_ib:0.00359268\n",
      "epoch 46 loss_pde:0.02327153, loss_ib:0.00359025\n",
      "epoch 46 loss_pde:0.02326882, loss_ib:0.00358816\n",
      "epoch 46 loss_pde:0.02328255, loss_ib:0.00358426\n",
      "epoch 46 loss_pde:0.02329139, loss_ib:0.00358082\n",
      "epoch 46 loss_pde:0.02331053, loss_ib:0.00357675\n",
      "epoch 46 loss_pde:0.02333599, loss_ib:0.00357090\n",
      "epoch 46 loss_pde:0.02339881, loss_ib:0.00356110\n",
      "epoch 46: loss 0.059533\n",
      "epoch 47 loss_pde:0.02341844, loss_ib:0.00355650\n",
      "epoch 47 loss_pde:0.02343529, loss_ib:0.00355224\n",
      "epoch 47 loss_pde:0.02344681, loss_ib:0.00354899\n",
      "epoch 47 loss_pde:0.02345843, loss_ib:0.00354584\n",
      "epoch 47 loss_pde:0.02346906, loss_ib:0.00354222\n",
      "epoch 47 loss_pde:0.02347912, loss_ib:0.00353907\n",
      "epoch 47 loss_pde:0.02347621, loss_ib:0.00353640\n",
      "epoch 47 loss_pde:0.02351740, loss_ib:0.00352890\n",
      "epoch 47 loss_pde:0.02350301, loss_ib:0.00352721\n",
      "epoch 47 loss_pde:0.02349411, loss_ib:0.00352584\n",
      "epoch 47 loss_pde:0.02348189, loss_ib:0.00352403\n",
      "epoch 47 loss_pde:0.02349006, loss_ib:0.00351919\n",
      "epoch 47 loss_pde:0.02347350, loss_ib:0.00351701\n",
      "epoch 47 loss_pde:0.02348770, loss_ib:0.00351242\n",
      "epoch 47 loss_pde:0.02352265, loss_ib:0.00350546\n",
      "epoch 47 loss_pde:0.02355281, loss_ib:0.00349801\n",
      "epoch 47 loss_pde:0.02360413, loss_ib:0.00348853\n",
      "epoch 47 loss_pde:0.02362408, loss_ib:0.00348309\n",
      "epoch 47 loss_pde:0.02367864, loss_ib:0.00347451\n",
      "epoch 47 loss_pde:0.02368729, loss_ib:0.00347108\n",
      "epoch 47: loss 0.058983\n",
      "epoch 48 loss_pde:0.02370057, loss_ib:0.00346744\n",
      "epoch 48 loss_pde:0.02369394, loss_ib:0.00346528\n",
      "epoch 48 loss_pde:0.02369515, loss_ib:0.00346197\n",
      "epoch 48 loss_pde:0.02366820, loss_ib:0.00346143\n",
      "epoch 48 loss_pde:0.02365164, loss_ib:0.00345955\n",
      "epoch 48 loss_pde:0.02361088, loss_ib:0.00345994\n",
      "epoch 48 loss_pde:0.02357842, loss_ib:0.00345946\n",
      "epoch 48 loss_pde:0.02354985, loss_ib:0.00345948\n",
      "epoch 48 loss_pde:0.02351326, loss_ib:0.00346038\n",
      "epoch 48 loss_pde:0.02348703, loss_ib:0.00345856\n",
      "epoch 48 loss_pde:0.02345296, loss_ib:0.00345598\n",
      "epoch 48 loss_pde:0.02343671, loss_ib:0.00345258\n",
      "epoch 48 loss_pde:0.02344541, loss_ib:0.00344802\n",
      "epoch 48 loss_pde:0.02345412, loss_ib:0.00344428\n",
      "epoch 48 loss_pde:0.02347554, loss_ib:0.00343911\n",
      "epoch 48 loss_pde:0.02349254, loss_ib:0.00343454\n",
      "epoch 48 loss_pde:0.02352073, loss_ib:0.00342930\n",
      "epoch 48 loss_pde:0.02355083, loss_ib:0.00342283\n",
      "epoch 48 loss_pde:0.02359218, loss_ib:0.00341412\n",
      "epoch 48 loss_pde:0.02369868, loss_ib:0.00340188\n",
      "epoch 48: loss 0.058375\n",
      "epoch 49 loss_pde:0.02368814, loss_ib:0.00339831\n",
      "epoch 49 loss_pde:0.02368341, loss_ib:0.00339536\n",
      "epoch 49 loss_pde:0.02367415, loss_ib:0.00339370\n",
      "epoch 49 loss_pde:0.02367605, loss_ib:0.00339139\n",
      "epoch 49 loss_pde:0.02367532, loss_ib:0.00338908\n",
      "epoch 49 loss_pde:0.02367713, loss_ib:0.00338530\n",
      "epoch 49 loss_pde:0.02366044, loss_ib:0.00338284\n",
      "epoch 49 loss_pde:0.02364512, loss_ib:0.00338079\n",
      "epoch 49 loss_pde:0.02362739, loss_ib:0.00338007\n",
      "epoch 49 loss_pde:0.02360108, loss_ib:0.00338055\n",
      "epoch 49 loss_pde:0.02357462, loss_ib:0.00338101\n",
      "epoch 49 loss_pde:0.02352933, loss_ib:0.00338243\n",
      "epoch 49 loss_pde:0.02347979, loss_ib:0.00338431\n",
      "epoch 49 loss_pde:0.02344441, loss_ib:0.00338710\n",
      "epoch 49 loss_pde:0.02342015, loss_ib:0.00338703\n",
      "epoch 49 loss_pde:0.02340454, loss_ib:0.00338679\n",
      "epoch 49 loss_pde:0.02339198, loss_ib:0.00338675\n",
      "epoch 49 loss_pde:0.02338681, loss_ib:0.00338617\n",
      "epoch 49 loss_pde:0.02338388, loss_ib:0.00338531\n",
      "epoch 49 loss_pde:0.02339106, loss_ib:0.00338295\n",
      "epoch 49: loss 0.057671\n",
      "epoch 50 loss_pde:0.02338818, loss_ib:0.00338080\n",
      "epoch 50 loss_pde:0.02341890, loss_ib:0.00337489\n",
      "epoch 50 loss_pde:0.02342953, loss_ib:0.00337149\n",
      "epoch 50 loss_pde:0.02345137, loss_ib:0.00336639\n",
      "epoch 50 loss_pde:0.02344796, loss_ib:0.00336301\n",
      "epoch 50 loss_pde:0.02346837, loss_ib:0.00335750\n",
      "epoch 50 loss_pde:0.02347209, loss_ib:0.00335388\n",
      "epoch 50 loss_pde:0.02346855, loss_ib:0.00335135\n",
      "epoch 50 loss_pde:0.02347141, loss_ib:0.00334869\n",
      "epoch 50 loss_pde:0.02346161, loss_ib:0.00334719\n",
      "epoch 50 loss_pde:0.02345845, loss_ib:0.00334519\n",
      "epoch 50 loss_pde:0.02345963, loss_ib:0.00334275\n",
      "epoch 50 loss_pde:0.02344681, loss_ib:0.00334180\n",
      "epoch 50 loss_pde:0.02344058, loss_ib:0.00334050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50 loss_pde:0.02342684, loss_ib:0.00333959\n",
      "epoch 50 loss_pde:0.02343778, loss_ib:0.00333557\n",
      "epoch 50 loss_pde:0.02341204, loss_ib:0.00333462\n",
      "epoch 50 loss_pde:0.02343340, loss_ib:0.00332968\n",
      "epoch 50 loss_pde:0.02343344, loss_ib:0.00332679\n",
      "epoch 50 loss_pde:0.02345483, loss_ib:0.00332194\n",
      "epoch 50: loss 0.057196\n",
      "epoch 51 loss_pde:0.02346539, loss_ib:0.00331823\n",
      "epoch 51 loss_pde:0.02348233, loss_ib:0.00331368\n",
      "epoch 51 loss_pde:0.02350039, loss_ib:0.00330878\n",
      "epoch 51 loss_pde:0.02350681, loss_ib:0.00330549\n",
      "epoch 51 loss_pde:0.02348531, loss_ib:0.00330538\n",
      "epoch 51 loss_pde:0.02350355, loss_ib:0.00330133\n",
      "epoch 51 loss_pde:0.02349346, loss_ib:0.00330064\n",
      "epoch 51 loss_pde:0.02347073, loss_ib:0.00330023\n",
      "epoch 51 loss_pde:0.02342702, loss_ib:0.00330140\n",
      "epoch 51 loss_pde:0.02339160, loss_ib:0.00330147\n",
      "epoch 51 loss_pde:0.02333159, loss_ib:0.00330437\n",
      "epoch 51 loss_pde:0.02330438, loss_ib:0.00330442\n",
      "epoch 51 loss_pde:0.02325947, loss_ib:0.00330642\n",
      "epoch 51 loss_pde:0.02324004, loss_ib:0.00330582\n",
      "epoch 51 loss_pde:0.02321462, loss_ib:0.00330615\n",
      "epoch 51 loss_pde:0.02319664, loss_ib:0.00330521\n",
      "epoch 51 loss_pde:0.02318175, loss_ib:0.00330461\n",
      "epoch 51 loss_pde:0.02318557, loss_ib:0.00330160\n",
      "epoch 51 loss_pde:0.02317556, loss_ib:0.00329950\n",
      "epoch 51 loss_pde:0.02318310, loss_ib:0.00329640\n",
      "epoch 51: loss 0.056648\n",
      "epoch 52 loss_pde:0.02318123, loss_ib:0.00329432\n",
      "epoch 52 loss_pde:0.02317310, loss_ib:0.00329245\n",
      "epoch 52 loss_pde:0.02316366, loss_ib:0.00329020\n",
      "epoch 52 loss_pde:0.02313510, loss_ib:0.00329070\n",
      "epoch 52 loss_pde:0.02309879, loss_ib:0.00329050\n",
      "epoch 52 loss_pde:0.02302898, loss_ib:0.00329391\n",
      "epoch 52 loss_pde:0.02299958, loss_ib:0.00329437\n",
      "epoch 52 loss_pde:0.02296109, loss_ib:0.00329617\n",
      "epoch 52 loss_pde:0.02293874, loss_ib:0.00329545\n",
      "epoch 52 loss_pde:0.02290630, loss_ib:0.00329438\n",
      "epoch 52 loss_pde:0.02288685, loss_ib:0.00329254\n",
      "epoch 52 loss_pde:0.02288386, loss_ib:0.00329061\n",
      "epoch 52 loss_pde:0.02287696, loss_ib:0.00328897\n",
      "epoch 52 loss_pde:0.02289049, loss_ib:0.00328588\n",
      "epoch 52 loss_pde:0.02287636, loss_ib:0.00328520\n",
      "epoch 52 loss_pde:0.02285872, loss_ib:0.00328498\n",
      "epoch 52 loss_pde:0.02283439, loss_ib:0.00328520\n",
      "epoch 52 loss_pde:0.02280530, loss_ib:0.00328578\n",
      "epoch 52 loss_pde:0.02277352, loss_ib:0.00328637\n",
      "epoch 52 loss_pde:0.02274681, loss_ib:0.00328556\n",
      "epoch 52: loss 0.056124\n",
      "epoch 53 loss_pde:0.02270314, loss_ib:0.00328593\n",
      "epoch 53 loss_pde:0.02268966, loss_ib:0.00328380\n",
      "epoch 53 loss_pde:0.02268524, loss_ib:0.00328133\n",
      "epoch 53 loss_pde:0.02269327, loss_ib:0.00327791\n",
      "epoch 53 loss_pde:0.02270588, loss_ib:0.00327450\n",
      "epoch 53 loss_pde:0.02272337, loss_ib:0.00327104\n",
      "epoch 53 loss_pde:0.02274128, loss_ib:0.00326747\n",
      "epoch 53 loss_pde:0.02275782, loss_ib:0.00326350\n",
      "epoch 53 loss_pde:0.02280081, loss_ib:0.00325658\n",
      "epoch 53 loss_pde:0.02280942, loss_ib:0.00325359\n",
      "epoch 53 loss_pde:0.02282006, loss_ib:0.00324981\n",
      "epoch 53 loss_pde:0.02280408, loss_ib:0.00324849\n",
      "epoch 53 loss_pde:0.02280369, loss_ib:0.00324592\n",
      "epoch 53 loss_pde:0.02275306, loss_ib:0.00324873\n",
      "epoch 53 loss_pde:0.02274654, loss_ib:0.00324792\n",
      "epoch 53 loss_pde:0.02274601, loss_ib:0.00324596\n",
      "epoch 53 loss_pde:0.02273618, loss_ib:0.00324481\n",
      "epoch 53 loss_pde:0.02273296, loss_ib:0.00324308\n",
      "epoch 53 loss_pde:0.02272623, loss_ib:0.00324178\n",
      "epoch 53 loss_pde:0.02272708, loss_ib:0.00323974\n",
      "epoch 53: loss 0.055562\n",
      "epoch 54 loss_pde:0.02271326, loss_ib:0.00323884\n",
      "epoch 54 loss_pde:0.02272294, loss_ib:0.00323549\n",
      "epoch 54 loss_pde:0.02273933, loss_ib:0.00323203\n",
      "epoch 54 loss_pde:0.02273801, loss_ib:0.00323010\n",
      "epoch 54 loss_pde:0.02273875, loss_ib:0.00322839\n",
      "epoch 54 loss_pde:0.02273293, loss_ib:0.00322627\n",
      "epoch 54 loss_pde:0.02273018, loss_ib:0.00322389\n",
      "epoch 54 loss_pde:0.02272407, loss_ib:0.00322184\n",
      "epoch 54 loss_pde:0.02272124, loss_ib:0.00321892\n",
      "epoch 54 loss_pde:0.02271983, loss_ib:0.00321561\n",
      "epoch 54 loss_pde:0.02272962, loss_ib:0.00321209\n",
      "epoch 54 loss_pde:0.02273205, loss_ib:0.00321001\n",
      "epoch 54 loss_pde:0.02274023, loss_ib:0.00320748\n",
      "epoch 54 loss_pde:0.02274810, loss_ib:0.00320479\n",
      "epoch 54 loss_pde:0.02275945, loss_ib:0.00320146\n",
      "epoch 54 loss_pde:0.02278932, loss_ib:0.00319638\n",
      "epoch 54 loss_pde:0.02280384, loss_ib:0.00319328\n",
      "epoch 54 loss_pde:0.02282443, loss_ib:0.00318935\n",
      "epoch 54 loss_pde:0.02284045, loss_ib:0.00318567\n",
      "epoch 54 loss_pde:0.02285491, loss_ib:0.00318247\n",
      "epoch 54: loss 0.055102\n",
      "epoch 55 loss_pde:0.02287176, loss_ib:0.00317951\n",
      "epoch 55 loss_pde:0.02286987, loss_ib:0.00317838\n",
      "epoch 55 loss_pde:0.02287814, loss_ib:0.00317516\n",
      "epoch 55 loss_pde:0.02284437, loss_ib:0.00317562\n",
      "epoch 55 loss_pde:0.02283031, loss_ib:0.00317479\n",
      "epoch 55 loss_pde:0.02280157, loss_ib:0.00317614\n",
      "epoch 55 loss_pde:0.02277706, loss_ib:0.00317712\n",
      "epoch 55 loss_pde:0.02273944, loss_ib:0.00317874\n",
      "epoch 55 loss_pde:0.02270417, loss_ib:0.00317949\n",
      "epoch 55 loss_pde:0.02265440, loss_ib:0.00318100\n",
      "epoch 55 loss_pde:0.02263333, loss_ib:0.00317987\n",
      "epoch 55 loss_pde:0.02262164, loss_ib:0.00317789\n",
      "epoch 55 loss_pde:0.02262609, loss_ib:0.00317416\n",
      "epoch 55 loss_pde:0.02263620, loss_ib:0.00317107\n",
      "epoch 55 loss_pde:0.02263703, loss_ib:0.00316863\n",
      "epoch 55 loss_pde:0.02265187, loss_ib:0.00316419\n",
      "epoch 55 loss_pde:0.02265074, loss_ib:0.00316126\n",
      "epoch 55 loss_pde:0.02265399, loss_ib:0.00315837\n",
      "epoch 55 loss_pde:0.02269405, loss_ib:0.00315202\n",
      "epoch 55 loss_pde:0.02268887, loss_ib:0.00315082\n",
      "epoch 55: loss 0.054667\n",
      "epoch 56 loss_pde:0.02267595, loss_ib:0.00315053\n",
      "epoch 56 loss_pde:0.02266207, loss_ib:0.00315077\n",
      "epoch 56 loss_pde:0.02264671, loss_ib:0.00315121\n",
      "epoch 56 loss_pde:0.02263116, loss_ib:0.00315130\n",
      "epoch 56 loss_pde:0.02259835, loss_ib:0.00315241\n",
      "epoch 56 loss_pde:0.02258286, loss_ib:0.00315153\n",
      "epoch 56 loss_pde:0.02257121, loss_ib:0.00315065\n",
      "epoch 56 loss_pde:0.02255836, loss_ib:0.00314981\n",
      "epoch 56 loss_pde:0.02256166, loss_ib:0.00314786\n",
      "epoch 56 loss_pde:0.02255838, loss_ib:0.00314598\n",
      "epoch 56 loss_pde:0.02255300, loss_ib:0.00314418\n",
      "epoch 56 loss_pde:0.02256145, loss_ib:0.00314101\n",
      "epoch 56 loss_pde:0.02250546, loss_ib:0.00314402\n",
      "epoch 56 loss_pde:0.02250990, loss_ib:0.00314169\n",
      "epoch 56 loss_pde:0.02250939, loss_ib:0.00313960\n",
      "epoch 56 loss_pde:0.02250616, loss_ib:0.00313756\n",
      "epoch 56 loss_pde:0.02250512, loss_ib:0.00313538\n",
      "epoch 56 loss_pde:0.02250230, loss_ib:0.00313364\n",
      "epoch 56 loss_pde:0.02251128, loss_ib:0.00313095\n",
      "epoch 56 loss_pde:0.02251958, loss_ib:0.00312844\n",
      "epoch 56: loss 0.054181\n",
      "epoch 57 loss_pde:0.02253539, loss_ib:0.00312503\n",
      "epoch 57 loss_pde:0.02255064, loss_ib:0.00312213\n",
      "epoch 57 loss_pde:0.02256467, loss_ib:0.00311902\n",
      "epoch 57 loss_pde:0.02258415, loss_ib:0.00311520\n",
      "epoch 57 loss_pde:0.02259881, loss_ib:0.00311197\n",
      "epoch 57 loss_pde:0.02260382, loss_ib:0.00311004\n",
      "epoch 57 loss_pde:0.02260386, loss_ib:0.00310892\n",
      "epoch 57 loss_pde:0.02259866, loss_ib:0.00310782\n",
      "epoch 57 loss_pde:0.02258438, loss_ib:0.00310758\n",
      "epoch 57 loss_pde:0.02256660, loss_ib:0.00310783\n",
      "epoch 57 loss_pde:0.02254396, loss_ib:0.00310869\n",
      "epoch 57 loss_pde:0.02251729, loss_ib:0.00310983\n",
      "epoch 57 loss_pde:0.02248309, loss_ib:0.00311130\n",
      "epoch 57 loss_pde:0.02243870, loss_ib:0.00311304\n",
      "epoch 57 loss_pde:0.02239361, loss_ib:0.00311472\n",
      "epoch 57 loss_pde:0.02237318, loss_ib:0.00311511\n",
      "epoch 57 loss_pde:0.02235663, loss_ib:0.00311531\n",
      "epoch 57 loss_pde:0.02235069, loss_ib:0.00311470\n",
      "epoch 57 loss_pde:0.02234858, loss_ib:0.00311388\n",
      "epoch 57 loss_pde:0.02234959, loss_ib:0.00311268\n",
      "epoch 57: loss 0.053786\n",
      "epoch 58 loss_pde:0.02236104, loss_ib:0.00311020\n",
      "epoch 58 loss_pde:0.02236276, loss_ib:0.00310860\n",
      "epoch 58 loss_pde:0.02236978, loss_ib:0.00310654\n",
      "epoch 58 loss_pde:0.02235972, loss_ib:0.00310530\n",
      "epoch 58 loss_pde:0.02236472, loss_ib:0.00310238\n",
      "epoch 58 loss_pde:0.02235791, loss_ib:0.00310175\n",
      "epoch 58 loss_pde:0.02233979, loss_ib:0.00310200\n",
      "epoch 58 loss_pde:0.02232229, loss_ib:0.00310223\n",
      "epoch 58 loss_pde:0.02229854, loss_ib:0.00310296\n",
      "epoch 58 loss_pde:0.02227755, loss_ib:0.00310313\n",
      "epoch 58 loss_pde:0.02224620, loss_ib:0.00310421\n",
      "epoch 58 loss_pde:0.02223146, loss_ib:0.00310391\n",
      "epoch 58 loss_pde:0.02222066, loss_ib:0.00310323\n",
      "epoch 58 loss_pde:0.02221110, loss_ib:0.00310219\n",
      "epoch 58 loss_pde:0.02220477, loss_ib:0.00310082\n",
      "epoch 58 loss_pde:0.02220822, loss_ib:0.00309864\n",
      "epoch 58 loss_pde:0.02219953, loss_ib:0.00309794\n",
      "epoch 58 loss_pde:0.02220596, loss_ib:0.00309604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 58 loss_pde:0.02220277, loss_ib:0.00309457\n",
      "epoch 58 loss_pde:0.02220352, loss_ib:0.00309206\n",
      "epoch 58: loss 0.053463\n",
      "epoch 59 loss_pde:0.02217752, loss_ib:0.00309220\n",
      "epoch 59 loss_pde:0.02216598, loss_ib:0.00309118\n",
      "epoch 59 loss_pde:0.02210954, loss_ib:0.00309451\n",
      "epoch 59 loss_pde:0.02208155, loss_ib:0.00309509\n",
      "epoch 59 loss_pde:0.02203573, loss_ib:0.00309795\n",
      "epoch 59 loss_pde:0.02198878, loss_ib:0.00310086\n",
      "epoch 59 loss_pde:0.02194662, loss_ib:0.00310289\n",
      "epoch 59 loss_pde:0.02190657, loss_ib:0.00310464\n",
      "epoch 59 loss_pde:0.02188993, loss_ib:0.00310417\n",
      "epoch 59 loss_pde:0.02187790, loss_ib:0.00310333\n",
      "epoch 59 loss_pde:0.02189344, loss_ib:0.00309969\n",
      "epoch 59 loss_pde:0.02191486, loss_ib:0.00309518\n",
      "epoch 59 loss_pde:0.02196026, loss_ib:0.00308840\n",
      "epoch 59 loss_pde:0.02200159, loss_ib:0.00308235\n",
      "epoch 59 loss_pde:0.02204667, loss_ib:0.00307574\n",
      "epoch 59 loss_pde:0.02209714, loss_ib:0.00306842\n",
      "epoch 59 loss_pde:0.02213272, loss_ib:0.00306239\n",
      "epoch 59 loss_pde:0.02217510, loss_ib:0.00305558\n",
      "epoch 59 loss_pde:0.02218762, loss_ib:0.00305214\n",
      "epoch 59 loss_pde:0.02219732, loss_ib:0.00304921\n",
      "epoch 59: loss 0.053100\n",
      "epoch 60 loss_pde:0.02218921, loss_ib:0.00304817\n",
      "epoch 60 loss_pde:0.02216777, loss_ib:0.00304864\n",
      "epoch 60 loss_pde:0.02214901, loss_ib:0.00304898\n",
      "epoch 60 loss_pde:0.02212197, loss_ib:0.00305021\n",
      "epoch 60 loss_pde:0.02210074, loss_ib:0.00305041\n",
      "epoch 60 loss_pde:0.02206743, loss_ib:0.00305112\n",
      "epoch 60 loss_pde:0.02204717, loss_ib:0.00305040\n",
      "epoch 60 loss_pde:0.02202495, loss_ib:0.00304996\n",
      "epoch 60 loss_pde:0.02201692, loss_ib:0.00304844\n",
      "epoch 60 loss_pde:0.02200960, loss_ib:0.00304674\n",
      "epoch 60 loss_pde:0.02201145, loss_ib:0.00304438\n",
      "epoch 60 loss_pde:0.02200896, loss_ib:0.00304273\n",
      "epoch 60 loss_pde:0.02201269, loss_ib:0.00304056\n",
      "epoch 60 loss_pde:0.02200969, loss_ib:0.00303897\n",
      "epoch 60 loss_pde:0.02201420, loss_ib:0.00303632\n",
      "epoch 60 loss_pde:0.02200243, loss_ib:0.00303554\n",
      "epoch 60 loss_pde:0.02198495, loss_ib:0.00303521\n",
      "epoch 60 loss_pde:0.02194643, loss_ib:0.00303640\n",
      "epoch 60 loss_pde:0.02192019, loss_ib:0.00303662\n",
      "epoch 60 loss_pde:0.02188854, loss_ib:0.00303811\n",
      "epoch 60: loss 0.052671\n",
      "epoch 61 loss_pde:0.02188436, loss_ib:0.00303605\n",
      "epoch 61 loss_pde:0.02186247, loss_ib:0.00303577\n",
      "epoch 61 loss_pde:0.02185021, loss_ib:0.00303435\n",
      "epoch 61 loss_pde:0.02181720, loss_ib:0.00303511\n",
      "epoch 61 loss_pde:0.02182482, loss_ib:0.00303299\n",
      "epoch 61 loss_pde:0.02184089, loss_ib:0.00303004\n",
      "epoch 61 loss_pde:0.02185302, loss_ib:0.00302700\n",
      "epoch 61 loss_pde:0.02187696, loss_ib:0.00302230\n",
      "epoch 61 loss_pde:0.02189165, loss_ib:0.00301804\n",
      "epoch 61 loss_pde:0.02191296, loss_ib:0.00301291\n",
      "epoch 61 loss_pde:0.02191852, loss_ib:0.00300975\n",
      "epoch 61 loss_pde:0.02190321, loss_ib:0.00300854\n",
      "epoch 61 loss_pde:0.02188800, loss_ib:0.00300760\n",
      "epoch 61 loss_pde:0.02185939, loss_ib:0.00300899\n",
      "epoch 61 loss_pde:0.02185777, loss_ib:0.00300771\n",
      "epoch 61 loss_pde:0.02183417, loss_ib:0.00300876\n",
      "epoch 61 loss_pde:0.02180309, loss_ib:0.00301016\n",
      "epoch 61 loss_pde:0.02178015, loss_ib:0.00301059\n",
      "epoch 61 loss_pde:0.02176238, loss_ib:0.00301031\n",
      "epoch 61 loss_pde:0.02176184, loss_ib:0.00300810\n",
      "epoch 61: loss 0.052245\n",
      "epoch 62 loss_pde:0.02174822, loss_ib:0.00300726\n",
      "epoch 62 loss_pde:0.02176077, loss_ib:0.00300446\n",
      "epoch 62 loss_pde:0.02177732, loss_ib:0.00300114\n",
      "epoch 62 loss_pde:0.02180041, loss_ib:0.00299710\n",
      "epoch 62 loss_pde:0.02181812, loss_ib:0.00299368\n",
      "epoch 62 loss_pde:0.02183531, loss_ib:0.00299037\n",
      "epoch 62 loss_pde:0.02183978, loss_ib:0.00298841\n",
      "epoch 62 loss_pde:0.02185278, loss_ib:0.00298557\n",
      "epoch 62 loss_pde:0.02183752, loss_ib:0.00298569\n",
      "epoch 62 loss_pde:0.02181415, loss_ib:0.00298653\n",
      "epoch 62 loss_pde:0.02178376, loss_ib:0.00298788\n",
      "epoch 62 loss_pde:0.02175487, loss_ib:0.00298905\n",
      "epoch 62 loss_pde:0.02172487, loss_ib:0.00299016\n",
      "epoch 62 loss_pde:0.02172015, loss_ib:0.00298835\n",
      "epoch 62 loss_pde:0.02167439, loss_ib:0.00299011\n",
      "epoch 62 loss_pde:0.02167089, loss_ib:0.00298889\n",
      "epoch 62 loss_pde:0.02168800, loss_ib:0.00298589\n",
      "epoch 62 loss_pde:0.02168258, loss_ib:0.00298389\n",
      "epoch 62 loss_pde:0.02167454, loss_ib:0.00298299\n",
      "epoch 62 loss_pde:0.02167018, loss_ib:0.00298213\n",
      "epoch 62: loss 0.051821\n",
      "epoch 63 loss_pde:0.02165597, loss_ib:0.00298231\n",
      "epoch 63 loss_pde:0.02164348, loss_ib:0.00298222\n",
      "epoch 63 loss_pde:0.02161830, loss_ib:0.00298329\n",
      "epoch 63 loss_pde:0.02159861, loss_ib:0.00298365\n",
      "epoch 63 loss_pde:0.02156051, loss_ib:0.00298560\n",
      "epoch 63 loss_pde:0.02153829, loss_ib:0.00298631\n",
      "epoch 63 loss_pde:0.02152771, loss_ib:0.00298639\n",
      "epoch 63 loss_pde:0.02151387, loss_ib:0.00298618\n",
      "epoch 63 loss_pde:0.02150888, loss_ib:0.00298505\n",
      "epoch 63 loss_pde:0.02150589, loss_ib:0.00298363\n",
      "epoch 63 loss_pde:0.02151453, loss_ib:0.00298085\n",
      "epoch 63 loss_pde:0.02152318, loss_ib:0.00297793\n",
      "epoch 63 loss_pde:0.02153939, loss_ib:0.00297459\n",
      "epoch 63 loss_pde:0.02155563, loss_ib:0.00297061\n",
      "epoch 63 loss_pde:0.02157166, loss_ib:0.00296699\n",
      "epoch 63 loss_pde:0.02158353, loss_ib:0.00296326\n",
      "epoch 63 loss_pde:0.02159490, loss_ib:0.00295919\n",
      "epoch 63 loss_pde:0.02159406, loss_ib:0.00295605\n",
      "epoch 63 loss_pde:0.02162993, loss_ib:0.00294978\n",
      "epoch 63 loss_pde:0.02162138, loss_ib:0.00294873\n",
      "epoch 63: loss 0.051479\n",
      "epoch 64 loss_pde:0.02160192, loss_ib:0.00294894\n",
      "epoch 64 loss_pde:0.02158728, loss_ib:0.00294891\n",
      "epoch 64 loss_pde:0.02156822, loss_ib:0.00294946\n",
      "epoch 64 loss_pde:0.02155605, loss_ib:0.00294911\n",
      "epoch 64 loss_pde:0.02153688, loss_ib:0.00294886\n",
      "epoch 64 loss_pde:0.02152521, loss_ib:0.00294736\n",
      "epoch 64 loss_pde:0.02151951, loss_ib:0.00294447\n",
      "epoch 64 loss_pde:0.02151885, loss_ib:0.00294148\n",
      "epoch 64 loss_pde:0.02152737, loss_ib:0.00293861\n",
      "epoch 64 loss_pde:0.02153636, loss_ib:0.00293629\n",
      "epoch 64 loss_pde:0.02154144, loss_ib:0.00293440\n",
      "epoch 64 loss_pde:0.02154416, loss_ib:0.00293242\n",
      "epoch 64 loss_pde:0.02153546, loss_ib:0.00293100\n",
      "epoch 64 loss_pde:0.02152940, loss_ib:0.00292912\n",
      "epoch 64 loss_pde:0.02149118, loss_ib:0.00292963\n",
      "epoch 64 loss_pde:0.02147867, loss_ib:0.00292714\n",
      "epoch 64 loss_pde:0.02145461, loss_ib:0.00292700\n",
      "epoch 64 loss_pde:0.02144335, loss_ib:0.00292580\n",
      "epoch 64 loss_pde:0.02143465, loss_ib:0.00292459\n",
      "epoch 64 loss_pde:0.02143951, loss_ib:0.00292203\n",
      "epoch 64: loss 0.051091\n",
      "epoch 65 loss_pde:0.02144708, loss_ib:0.00291897\n",
      "epoch 65 loss_pde:0.02146216, loss_ib:0.00291564\n",
      "epoch 65 loss_pde:0.02148096, loss_ib:0.00291241\n",
      "epoch 65 loss_pde:0.02150021, loss_ib:0.00290896\n",
      "epoch 65 loss_pde:0.02153071, loss_ib:0.00290395\n",
      "epoch 65 loss_pde:0.02155354, loss_ib:0.00289983\n",
      "epoch 65 loss_pde:0.02155656, loss_ib:0.00289739\n",
      "epoch 65 loss_pde:0.02157579, loss_ib:0.00289295\n",
      "epoch 65 loss_pde:0.02156276, loss_ib:0.00289199\n",
      "epoch 65 loss_pde:0.02153791, loss_ib:0.00289279\n",
      "epoch 65 loss_pde:0.02150775, loss_ib:0.00289414\n",
      "epoch 65 loss_pde:0.02147222, loss_ib:0.00289610\n",
      "epoch 65 loss_pde:0.02143440, loss_ib:0.00289835\n",
      "epoch 65 loss_pde:0.02140084, loss_ib:0.00290004\n",
      "epoch 65 loss_pde:0.02136041, loss_ib:0.00290204\n",
      "epoch 65 loss_pde:0.02132865, loss_ib:0.00290253\n",
      "epoch 65 loss_pde:0.02128678, loss_ib:0.00290349\n",
      "epoch 65 loss_pde:0.02127925, loss_ib:0.00290158\n",
      "epoch 65 loss_pde:0.02125475, loss_ib:0.00290176\n",
      "epoch 65 loss_pde:0.02126122, loss_ib:0.00289963\n",
      "epoch 65: loss 0.050637\n",
      "epoch 66 loss_pde:0.02126732, loss_ib:0.00289792\n",
      "epoch 66 loss_pde:0.02126795, loss_ib:0.00289657\n",
      "epoch 66 loss_pde:0.02126215, loss_ib:0.00289548\n",
      "epoch 66 loss_pde:0.02126094, loss_ib:0.00289420\n",
      "epoch 66 loss_pde:0.02124384, loss_ib:0.00289469\n",
      "epoch 66 loss_pde:0.02122098, loss_ib:0.00289574\n",
      "epoch 66 loss_pde:0.02119245, loss_ib:0.00289756\n",
      "epoch 66 loss_pde:0.02116822, loss_ib:0.00289913\n",
      "epoch 66 loss_pde:0.02114417, loss_ib:0.00290073\n",
      "epoch 66 loss_pde:0.02112095, loss_ib:0.00290200\n",
      "epoch 66 loss_pde:0.02109728, loss_ib:0.00290279\n",
      "epoch 66 loss_pde:0.02105931, loss_ib:0.00290492\n",
      "epoch 66 loss_pde:0.02105783, loss_ib:0.00290356\n",
      "epoch 66 loss_pde:0.02106841, loss_ib:0.00290042\n",
      "epoch 66 loss_pde:0.02109186, loss_ib:0.00289611\n",
      "epoch 66 loss_pde:0.02111473, loss_ib:0.00289230\n",
      "epoch 66 loss_pde:0.02113688, loss_ib:0.00288881\n",
      "epoch 66 loss_pde:0.02115505, loss_ib:0.00288565\n",
      "epoch 66 loss_pde:0.02117494, loss_ib:0.00288199\n",
      "epoch 66 loss_pde:0.02120243, loss_ib:0.00287782\n",
      "epoch 66: loss 0.050246\n",
      "epoch 67 loss_pde:0.02120023, loss_ib:0.00287688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 67 loss_pde:0.02119217, loss_ib:0.00287571\n",
      "epoch 67 loss_pde:0.02117776, loss_ib:0.00287560\n",
      "epoch 67 loss_pde:0.02117236, loss_ib:0.00287504\n",
      "epoch 67 loss_pde:0.02116739, loss_ib:0.00287457\n",
      "epoch 67 loss_pde:0.02117405, loss_ib:0.00287260\n",
      "epoch 67 loss_pde:0.02118695, loss_ib:0.00286934\n",
      "epoch 67 loss_pde:0.02123742, loss_ib:0.00286269\n",
      "epoch 67 loss_pde:0.02125131, loss_ib:0.00286023\n",
      "epoch 67 loss_pde:0.02126656, loss_ib:0.00285747\n",
      "epoch 67 loss_pde:0.02128320, loss_ib:0.00285491\n",
      "epoch 67 loss_pde:0.02129279, loss_ib:0.00285323\n",
      "epoch 67 loss_pde:0.02130325, loss_ib:0.00285140\n",
      "epoch 67 loss_pde:0.02130641, loss_ib:0.00284992\n",
      "epoch 67 loss_pde:0.02130536, loss_ib:0.00284817\n",
      "epoch 67 loss_pde:0.02133582, loss_ib:0.00284391\n",
      "epoch 67 loss_pde:0.02130988, loss_ib:0.00284446\n",
      "epoch 67 loss_pde:0.02127796, loss_ib:0.00284566\n",
      "epoch 67 loss_pde:0.02123981, loss_ib:0.00284779\n",
      "epoch 67 loss_pde:0.02121673, loss_ib:0.00284877\n",
      "epoch 67: loss 0.049969\n",
      "epoch 68 loss_pde:0.02118556, loss_ib:0.00285076\n",
      "epoch 68 loss_pde:0.02117684, loss_ib:0.00285071\n",
      "epoch 68 loss_pde:0.02116910, loss_ib:0.00285042\n",
      "epoch 68 loss_pde:0.02117291, loss_ib:0.00284856\n",
      "epoch 68 loss_pde:0.02118159, loss_ib:0.00284587\n",
      "epoch 68 loss_pde:0.02119324, loss_ib:0.00284297\n",
      "epoch 68 loss_pde:0.02120797, loss_ib:0.00284005\n",
      "epoch 68 loss_pde:0.02120415, loss_ib:0.00283885\n",
      "epoch 68 loss_pde:0.02120234, loss_ib:0.00283735\n",
      "epoch 68 loss_pde:0.02117211, loss_ib:0.00283892\n",
      "epoch 68 loss_pde:0.02116104, loss_ib:0.00283881\n",
      "epoch 68 loss_pde:0.02112938, loss_ib:0.00284003\n",
      "epoch 68 loss_pde:0.02108654, loss_ib:0.00284227\n",
      "epoch 68 loss_pde:0.02104265, loss_ib:0.00284499\n",
      "epoch 68 loss_pde:0.02100492, loss_ib:0.00284734\n",
      "epoch 68 loss_pde:0.02097092, loss_ib:0.00284934\n",
      "epoch 68 loss_pde:0.02094178, loss_ib:0.00285038\n",
      "epoch 68 loss_pde:0.02090579, loss_ib:0.00285149\n",
      "epoch 68 loss_pde:0.02089429, loss_ib:0.00285058\n",
      "epoch 68 loss_pde:0.02087865, loss_ib:0.00285007\n",
      "epoch 68: loss 0.049693\n",
      "epoch 69 loss_pde:0.02090010, loss_ib:0.00284609\n",
      "epoch 69 loss_pde:0.02089930, loss_ib:0.00284452\n",
      "epoch 69 loss_pde:0.02089111, loss_ib:0.00284381\n",
      "epoch 69 loss_pde:0.02089168, loss_ib:0.00284234\n",
      "epoch 69 loss_pde:0.02087498, loss_ib:0.00284256\n",
      "epoch 69 loss_pde:0.02085433, loss_ib:0.00284265\n",
      "epoch 69 loss_pde:0.02081075, loss_ib:0.00284452\n",
      "epoch 69 loss_pde:0.02077370, loss_ib:0.00284517\n",
      "epoch 69 loss_pde:0.02071167, loss_ib:0.00284832\n",
      "epoch 69 loss_pde:0.02069517, loss_ib:0.00284810\n",
      "epoch 69 loss_pde:0.02068984, loss_ib:0.00284731\n",
      "epoch 69 loss_pde:0.02068485, loss_ib:0.00284656\n",
      "epoch 69 loss_pde:0.02068217, loss_ib:0.00284547\n",
      "epoch 69 loss_pde:0.02067429, loss_ib:0.00284450\n",
      "epoch 69 loss_pde:0.02066327, loss_ib:0.00284320\n",
      "epoch 69 loss_pde:0.02064525, loss_ib:0.00284269\n",
      "epoch 69 loss_pde:0.02063644, loss_ib:0.00284156\n",
      "epoch 69 loss_pde:0.02062456, loss_ib:0.00284098\n",
      "epoch 69 loss_pde:0.02062045, loss_ib:0.00283985\n",
      "epoch 69 loss_pde:0.02060594, loss_ib:0.00284004\n",
      "epoch 69: loss 0.049361\n",
      "epoch 70 loss_pde:0.02060541, loss_ib:0.00283908\n",
      "epoch 70 loss_pde:0.02059635, loss_ib:0.00283885\n",
      "epoch 70 loss_pde:0.02059552, loss_ib:0.00283724\n",
      "epoch 70 loss_pde:0.02058946, loss_ib:0.00283586\n",
      "epoch 70 loss_pde:0.02058743, loss_ib:0.00283385\n",
      "epoch 70 loss_pde:0.02059514, loss_ib:0.00283073\n",
      "epoch 70 loss_pde:0.02059142, loss_ib:0.00282949\n",
      "epoch 70 loss_pde:0.02059090, loss_ib:0.00282828\n",
      "epoch 70 loss_pde:0.02058121, loss_ib:0.00282750\n",
      "epoch 70 loss_pde:0.02056045, loss_ib:0.00282726\n",
      "epoch 70 loss_pde:0.02053148, loss_ib:0.00282805\n",
      "epoch 70 loss_pde:0.02051031, loss_ib:0.00282867\n",
      "epoch 70 loss_pde:0.02047906, loss_ib:0.00283006\n",
      "epoch 70 loss_pde:0.02044678, loss_ib:0.00283137\n",
      "epoch 70 loss_pde:0.02041187, loss_ib:0.00283290\n",
      "epoch 70 loss_pde:0.02038161, loss_ib:0.00283404\n",
      "epoch 70 loss_pde:0.02035179, loss_ib:0.00283514\n",
      "epoch 70 loss_pde:0.02033548, loss_ib:0.00283492\n",
      "epoch 70 loss_pde:0.02032110, loss_ib:0.00283438\n",
      "epoch 70 loss_pde:0.02033756, loss_ib:0.00283061\n",
      "epoch 70: loss 0.048996\n",
      "epoch 71 loss_pde:0.02034036, loss_ib:0.00282840\n",
      "epoch 71 loss_pde:0.02035722, loss_ib:0.00282537\n",
      "epoch 71 loss_pde:0.02037623, loss_ib:0.00282206\n",
      "epoch 71 loss_pde:0.02040253, loss_ib:0.00281801\n",
      "epoch 71 loss_pde:0.02043121, loss_ib:0.00281352\n",
      "epoch 71 loss_pde:0.02046260, loss_ib:0.00280884\n",
      "epoch 71 loss_pde:0.02048461, loss_ib:0.00280503\n",
      "epoch 71 loss_pde:0.02050042, loss_ib:0.00280175\n",
      "epoch 71 loss_pde:0.02050579, loss_ib:0.00279924\n",
      "epoch 71 loss_pde:0.02049900, loss_ib:0.00279826\n",
      "epoch 71 loss_pde:0.02048915, loss_ib:0.00279738\n",
      "epoch 71 loss_pde:0.02046860, loss_ib:0.00279702\n",
      "epoch 71 loss_pde:0.02044768, loss_ib:0.00279674\n",
      "epoch 71 loss_pde:0.02042898, loss_ib:0.00279631\n",
      "epoch 71 loss_pde:0.02041828, loss_ib:0.00279607\n",
      "epoch 71 loss_pde:0.02041081, loss_ib:0.00279516\n",
      "epoch 71 loss_pde:0.02040520, loss_ib:0.00279465\n",
      "epoch 71 loss_pde:0.02040221, loss_ib:0.00279404\n",
      "epoch 71 loss_pde:0.02040067, loss_ib:0.00279303\n",
      "epoch 71 loss_pde:0.02039409, loss_ib:0.00279183\n",
      "epoch 71: loss 0.048624\n",
      "epoch 72 loss_pde:0.02038730, loss_ib:0.00279019\n",
      "epoch 72 loss_pde:0.02037285, loss_ib:0.00278957\n",
      "epoch 72 loss_pde:0.02037931, loss_ib:0.00278739\n",
      "epoch 72 loss_pde:0.02036802, loss_ib:0.00278699\n",
      "epoch 72 loss_pde:0.02035201, loss_ib:0.00278735\n",
      "epoch 72 loss_pde:0.02033852, loss_ib:0.00278735\n",
      "epoch 72 loss_pde:0.02032200, loss_ib:0.00278757\n",
      "epoch 72 loss_pde:0.02031367, loss_ib:0.00278649\n",
      "epoch 72 loss_pde:0.02030599, loss_ib:0.00278466\n",
      "epoch 72 loss_pde:0.02029776, loss_ib:0.00278330\n",
      "epoch 72 loss_pde:0.02031158, loss_ib:0.00277988\n",
      "epoch 72 loss_pde:0.02031592, loss_ib:0.00277842\n",
      "epoch 72 loss_pde:0.02032586, loss_ib:0.00277604\n",
      "epoch 72 loss_pde:0.02032836, loss_ib:0.00277410\n",
      "epoch 72 loss_pde:0.02033357, loss_ib:0.00277143\n",
      "epoch 72 loss_pde:0.02033295, loss_ib:0.00276878\n",
      "epoch 72 loss_pde:0.02033504, loss_ib:0.00276563\n",
      "epoch 72 loss_pde:0.02032647, loss_ib:0.00276381\n",
      "epoch 72 loss_pde:0.02032447, loss_ib:0.00276205\n",
      "epoch 72 loss_pde:0.02030297, loss_ib:0.00276246\n",
      "epoch 72: loss 0.048289\n",
      "epoch 73 loss_pde:0.02030005, loss_ib:0.00276153\n",
      "epoch 73 loss_pde:0.02029254, loss_ib:0.00276131\n",
      "epoch 73 loss_pde:0.02028712, loss_ib:0.00276024\n",
      "epoch 73 loss_pde:0.02027363, loss_ib:0.00275894\n",
      "epoch 73 loss_pde:0.02028479, loss_ib:0.00275542\n",
      "epoch 73 loss_pde:0.02028132, loss_ib:0.00275423\n",
      "epoch 73 loss_pde:0.02027943, loss_ib:0.00275303\n",
      "epoch 73 loss_pde:0.02027813, loss_ib:0.00275197\n",
      "epoch 73 loss_pde:0.02027277, loss_ib:0.00275138\n",
      "epoch 73 loss_pde:0.02026599, loss_ib:0.00275076\n",
      "epoch 73 loss_pde:0.02024403, loss_ib:0.00275135\n",
      "epoch 73 loss_pde:0.02023500, loss_ib:0.00275062\n",
      "epoch 73 loss_pde:0.02022109, loss_ib:0.00275041\n",
      "epoch 73 loss_pde:0.02020824, loss_ib:0.00274977\n",
      "epoch 73 loss_pde:0.02019297, loss_ib:0.00274945\n",
      "epoch 73 loss_pde:0.02018687, loss_ib:0.00274846\n",
      "epoch 73 loss_pde:0.02017819, loss_ib:0.00274799\n",
      "epoch 73 loss_pde:0.02018058, loss_ib:0.00274660\n",
      "epoch 73 loss_pde:0.02018338, loss_ib:0.00274471\n",
      "epoch 73 loss_pde:0.02020176, loss_ib:0.00274039\n",
      "epoch 73: loss 0.047915\n",
      "epoch 74 loss_pde:0.02022110, loss_ib:0.00273582\n",
      "epoch 74 loss_pde:0.02024164, loss_ib:0.00273198\n",
      "epoch 74 loss_pde:0.02025674, loss_ib:0.00272838\n",
      "epoch 74 loss_pde:0.02030732, loss_ib:0.00272156\n",
      "epoch 74 loss_pde:0.02030711, loss_ib:0.00271972\n",
      "epoch 74 loss_pde:0.02030106, loss_ib:0.00271914\n",
      "epoch 74 loss_pde:0.02028778, loss_ib:0.00271885\n",
      "epoch 74 loss_pde:0.02027250, loss_ib:0.00271884\n",
      "epoch 74 loss_pde:0.02025648, loss_ib:0.00271891\n",
      "epoch 74 loss_pde:0.02023871, loss_ib:0.00271887\n",
      "epoch 74 loss_pde:0.02021998, loss_ib:0.00271824\n",
      "epoch 74 loss_pde:0.02019304, loss_ib:0.00271828\n",
      "epoch 74 loss_pde:0.02017129, loss_ib:0.00271872\n",
      "epoch 74 loss_pde:0.02015922, loss_ib:0.00271840\n",
      "epoch 74 loss_pde:0.02014818, loss_ib:0.00271827\n",
      "epoch 74 loss_pde:0.02013903, loss_ib:0.00271789\n",
      "epoch 74 loss_pde:0.02012922, loss_ib:0.00271724\n",
      "epoch 74 loss_pde:0.02012008, loss_ib:0.00271592\n",
      "epoch 74 loss_pde:0.02011422, loss_ib:0.00271430\n",
      "epoch 74 loss_pde:0.02010762, loss_ib:0.00271273\n",
      "epoch 74: loss 0.047579\n",
      "epoch 75 loss_pde:0.02009862, loss_ib:0.00271017\n",
      "epoch 75 loss_pde:0.02009377, loss_ib:0.00270694\n",
      "epoch 75 loss_pde:0.02007865, loss_ib:0.00270577\n",
      "epoch 75 loss_pde:0.02008338, loss_ib:0.00270373\n",
      "epoch 75 loss_pde:0.02008170, loss_ib:0.00270253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 75 loss_pde:0.02008993, loss_ib:0.00270052\n",
      "epoch 75 loss_pde:0.02009309, loss_ib:0.00269885\n",
      "epoch 75 loss_pde:0.02009735, loss_ib:0.00269682\n",
      "epoch 75 loss_pde:0.02010371, loss_ib:0.00269445\n",
      "epoch 75 loss_pde:0.02010064, loss_ib:0.00269336\n",
      "epoch 75 loss_pde:0.02009369, loss_ib:0.00269216\n",
      "epoch 75 loss_pde:0.02008101, loss_ib:0.00269087\n",
      "epoch 75 loss_pde:0.02007232, loss_ib:0.00268927\n",
      "epoch 75 loss_pde:0.02006100, loss_ib:0.00268850\n",
      "epoch 75 loss_pde:0.02004990, loss_ib:0.00268826\n",
      "epoch 75 loss_pde:0.02003676, loss_ib:0.00268782\n",
      "epoch 75 loss_pde:0.02001719, loss_ib:0.00268714\n",
      "epoch 75 loss_pde:0.01999807, loss_ib:0.00268580\n",
      "epoch 75 loss_pde:0.01997884, loss_ib:0.00268455\n",
      "epoch 75 loss_pde:0.01996855, loss_ib:0.00268316\n",
      "epoch 75: loss 0.047200\n",
      "epoch 76 loss_pde:0.01995705, loss_ib:0.00268172\n",
      "epoch 76 loss_pde:0.01996933, loss_ib:0.00267821\n",
      "epoch 76 loss_pde:0.01997113, loss_ib:0.00267674\n",
      "epoch 76 loss_pde:0.01997245, loss_ib:0.00267504\n",
      "epoch 76 loss_pde:0.01997138, loss_ib:0.00267242\n",
      "epoch 76 loss_pde:0.01994654, loss_ib:0.00267116\n",
      "epoch 76 loss_pde:0.01994201, loss_ib:0.00266876\n",
      "epoch 76 loss_pde:0.01993040, loss_ib:0.00266760\n",
      "epoch 76 loss_pde:0.01992952, loss_ib:0.00266606\n",
      "epoch 76 loss_pde:0.01992108, loss_ib:0.00266550\n",
      "epoch 76 loss_pde:0.01991274, loss_ib:0.00266482\n",
      "epoch 76 loss_pde:0.01990321, loss_ib:0.00266438\n",
      "epoch 76 loss_pde:0.01989149, loss_ib:0.00266357\n",
      "epoch 76 loss_pde:0.01988094, loss_ib:0.00266160\n",
      "epoch 76 loss_pde:0.01986996, loss_ib:0.00265966\n",
      "epoch 76 loss_pde:0.01986904, loss_ib:0.00265757\n",
      "epoch 76 loss_pde:0.01986991, loss_ib:0.00265541\n",
      "epoch 76 loss_pde:0.01986862, loss_ib:0.00265347\n",
      "epoch 76 loss_pde:0.01987818, loss_ib:0.00265087\n",
      "epoch 76 loss_pde:0.01987384, loss_ib:0.00265015\n",
      "epoch 76: loss 0.046774\n",
      "epoch 77 loss_pde:0.01987284, loss_ib:0.00264857\n",
      "epoch 77 loss_pde:0.01985587, loss_ib:0.00264827\n",
      "epoch 77 loss_pde:0.01985494, loss_ib:0.00264641\n",
      "epoch 77 loss_pde:0.01983454, loss_ib:0.00264693\n",
      "epoch 77 loss_pde:0.01981330, loss_ib:0.00264709\n",
      "epoch 77 loss_pde:0.01977763, loss_ib:0.00264883\n",
      "epoch 77 loss_pde:0.01976210, loss_ib:0.00264918\n",
      "epoch 77 loss_pde:0.01975085, loss_ib:0.00264923\n",
      "epoch 77 loss_pde:0.01973536, loss_ib:0.00264906\n",
      "epoch 77 loss_pde:0.01971611, loss_ib:0.00264833\n",
      "epoch 77 loss_pde:0.01969496, loss_ib:0.00264745\n",
      "epoch 77 loss_pde:0.01967198, loss_ib:0.00264690\n",
      "epoch 77 loss_pde:0.01966034, loss_ib:0.00264525\n",
      "epoch 77 loss_pde:0.01965155, loss_ib:0.00264444\n",
      "epoch 77 loss_pde:0.01963776, loss_ib:0.00264383\n",
      "epoch 77 loss_pde:0.01962213, loss_ib:0.00264334\n",
      "epoch 77 loss_pde:0.01960167, loss_ib:0.00264353\n",
      "epoch 77 loss_pde:0.01958306, loss_ib:0.00264353\n",
      "epoch 77 loss_pde:0.01956152, loss_ib:0.00264400\n",
      "epoch 77 loss_pde:0.01954684, loss_ib:0.00264369\n",
      "epoch 77: loss 0.046359\n",
      "epoch 78 loss_pde:0.01952227, loss_ib:0.00264434\n",
      "epoch 78 loss_pde:0.01951551, loss_ib:0.00264357\n",
      "epoch 78 loss_pde:0.01951279, loss_ib:0.00264265\n",
      "epoch 78 loss_pde:0.01950846, loss_ib:0.00264142\n",
      "epoch 78 loss_pde:0.01949797, loss_ib:0.00264024\n",
      "epoch 78 loss_pde:0.01948965, loss_ib:0.00263814\n",
      "epoch 78 loss_pde:0.01946495, loss_ib:0.00263790\n",
      "epoch 78 loss_pde:0.01944907, loss_ib:0.00263772\n",
      "epoch 78 loss_pde:0.01942956, loss_ib:0.00263774\n",
      "epoch 78 loss_pde:0.01941007, loss_ib:0.00263780\n",
      "epoch 78 loss_pde:0.01938493, loss_ib:0.00263868\n",
      "epoch 78 loss_pde:0.01936088, loss_ib:0.00263959\n",
      "epoch 78 loss_pde:0.01932295, loss_ib:0.00264135\n",
      "epoch 78 loss_pde:0.01929251, loss_ib:0.00264199\n",
      "epoch 78 loss_pde:0.01925418, loss_ib:0.00264388\n",
      "epoch 78 loss_pde:0.01922919, loss_ib:0.00264443\n",
      "epoch 78 loss_pde:0.01920808, loss_ib:0.00264399\n",
      "epoch 78 loss_pde:0.01920700, loss_ib:0.00264133\n",
      "epoch 78 loss_pde:0.01921226, loss_ib:0.00263885\n",
      "epoch 78 loss_pde:0.01922884, loss_ib:0.00263577\n",
      "epoch 78: loss 0.045966\n",
      "epoch 79 loss_pde:0.01924440, loss_ib:0.00263262\n",
      "epoch 79 loss_pde:0.01926686, loss_ib:0.00262813\n",
      "epoch 79 loss_pde:0.01928906, loss_ib:0.00262392\n",
      "epoch 79 loss_pde:0.01930776, loss_ib:0.00262020\n",
      "epoch 79 loss_pde:0.01931873, loss_ib:0.00261747\n",
      "epoch 79 loss_pde:0.01931757, loss_ib:0.00261611\n",
      "epoch 79 loss_pde:0.01935349, loss_ib:0.00261198\n",
      "epoch 79 loss_pde:0.01933126, loss_ib:0.00261251\n",
      "epoch 79 loss_pde:0.01930606, loss_ib:0.00261368\n",
      "epoch 79 loss_pde:0.01928139, loss_ib:0.00261497\n",
      "epoch 79 loss_pde:0.01925948, loss_ib:0.00261604\n",
      "epoch 79 loss_pde:0.01924047, loss_ib:0.00261665\n",
      "epoch 79 loss_pde:0.01922436, loss_ib:0.00261653\n",
      "epoch 79 loss_pde:0.01921025, loss_ib:0.00261541\n",
      "epoch 79 loss_pde:0.01920070, loss_ib:0.00261574\n",
      "epoch 79 loss_pde:0.01920403, loss_ib:0.00261229\n",
      "epoch 79 loss_pde:0.01921396, loss_ib:0.00260902\n",
      "epoch 79 loss_pde:0.01922617, loss_ib:0.00260612\n",
      "epoch 79 loss_pde:0.01923839, loss_ib:0.00260361\n",
      "epoch 79 loss_pde:0.01924695, loss_ib:0.00260161\n",
      "epoch 79: loss 0.045571\n",
      "epoch 80 loss_pde:0.01925082, loss_ib:0.00259978\n",
      "epoch 80 loss_pde:0.01925948, loss_ib:0.00259721\n",
      "epoch 80 loss_pde:0.01925854, loss_ib:0.00259573\n",
      "epoch 80 loss_pde:0.01924959, loss_ib:0.00259491\n",
      "epoch 80 loss_pde:0.01924022, loss_ib:0.00259419\n",
      "epoch 80 loss_pde:0.01922917, loss_ib:0.00259379\n",
      "epoch 80 loss_pde:0.01922843, loss_ib:0.00259242\n",
      "epoch 80 loss_pde:0.01922208, loss_ib:0.00259161\n",
      "epoch 80 loss_pde:0.01922448, loss_ib:0.00259026\n",
      "epoch 80 loss_pde:0.01922499, loss_ib:0.00258851\n",
      "epoch 80 loss_pde:0.01923262, loss_ib:0.00258544\n",
      "epoch 80 loss_pde:0.01923472, loss_ib:0.00258320\n",
      "epoch 80 loss_pde:0.01923793, loss_ib:0.00258109\n",
      "epoch 80 loss_pde:0.01923049, loss_ib:0.00258030\n",
      "epoch 80 loss_pde:0.01922692, loss_ib:0.00257899\n",
      "epoch 80 loss_pde:0.01918889, loss_ib:0.00258049\n",
      "epoch 80 loss_pde:0.01917703, loss_ib:0.00257999\n",
      "epoch 80 loss_pde:0.01916105, loss_ib:0.00257965\n",
      "epoch 80 loss_pde:0.01914790, loss_ib:0.00257923\n",
      "epoch 80 loss_pde:0.01913991, loss_ib:0.00257838\n",
      "epoch 80: loss 0.045249\n",
      "epoch 81 loss_pde:0.01913814, loss_ib:0.00257691\n",
      "epoch 81 loss_pde:0.01912190, loss_ib:0.00257758\n",
      "epoch 81 loss_pde:0.01913160, loss_ib:0.00257512\n",
      "epoch 81 loss_pde:0.01914008, loss_ib:0.00257277\n",
      "epoch 81 loss_pde:0.01915117, loss_ib:0.00257021\n",
      "epoch 81 loss_pde:0.01916092, loss_ib:0.00256782\n",
      "epoch 81 loss_pde:0.01917021, loss_ib:0.00256555\n",
      "epoch 81 loss_pde:0.01917829, loss_ib:0.00256335\n",
      "epoch 81 loss_pde:0.01918097, loss_ib:0.00256191\n",
      "epoch 81 loss_pde:0.01918052, loss_ib:0.00256016\n",
      "epoch 81 loss_pde:0.01920255, loss_ib:0.00255604\n",
      "epoch 81 loss_pde:0.01919050, loss_ib:0.00255540\n",
      "epoch 81 loss_pde:0.01917924, loss_ib:0.00255482\n",
      "epoch 81 loss_pde:0.01916610, loss_ib:0.00255471\n",
      "epoch 81 loss_pde:0.01915945, loss_ib:0.00255385\n",
      "epoch 81 loss_pde:0.01914769, loss_ib:0.00255310\n",
      "epoch 81 loss_pde:0.01914179, loss_ib:0.00255194\n",
      "epoch 81 loss_pde:0.01914103, loss_ib:0.00254988\n",
      "epoch 81 loss_pde:0.01912905, loss_ib:0.00254804\n",
      "epoch 81 loss_pde:0.01912921, loss_ib:0.00254495\n",
      "epoch 81: loss 0.044907\n",
      "epoch 82 loss_pde:0.01912352, loss_ib:0.00254374\n",
      "epoch 82 loss_pde:0.01911302, loss_ib:0.00254276\n",
      "epoch 82 loss_pde:0.01910824, loss_ib:0.00254178\n",
      "epoch 82 loss_pde:0.01909744, loss_ib:0.00254157\n",
      "epoch 82 loss_pde:0.01908326, loss_ib:0.00254168\n",
      "epoch 82 loss_pde:0.01906618, loss_ib:0.00254195\n",
      "epoch 82 loss_pde:0.01904683, loss_ib:0.00254259\n",
      "epoch 82 loss_pde:0.01902741, loss_ib:0.00254281\n",
      "epoch 82 loss_pde:0.01900128, loss_ib:0.00254341\n",
      "epoch 82 loss_pde:0.01898583, loss_ib:0.00254327\n",
      "epoch 82 loss_pde:0.01897758, loss_ib:0.00254288\n",
      "epoch 82 loss_pde:0.01897051, loss_ib:0.00254218\n",
      "epoch 82 loss_pde:0.01896436, loss_ib:0.00254096\n",
      "epoch 82 loss_pde:0.01895028, loss_ib:0.00254004\n",
      "epoch 82 loss_pde:0.01894448, loss_ib:0.00253803\n",
      "epoch 82 loss_pde:0.01893713, loss_ib:0.00253729\n",
      "epoch 82 loss_pde:0.01891082, loss_ib:0.00253659\n",
      "epoch 82 loss_pde:0.01886661, loss_ib:0.00253651\n",
      "epoch 82 loss_pde:0.01881518, loss_ib:0.00253817\n",
      "epoch 82 loss_pde:0.01877753, loss_ib:0.00253992\n",
      "epoch 82: loss 0.044561\n",
      "epoch 83 loss_pde:0.01874830, loss_ib:0.00254130\n",
      "epoch 83 loss_pde:0.01871118, loss_ib:0.00254319\n",
      "epoch 83 loss_pde:0.01867785, loss_ib:0.00254438\n",
      "epoch 83 loss_pde:0.01863272, loss_ib:0.00254790\n",
      "epoch 83 loss_pde:0.01862301, loss_ib:0.00254752\n",
      "epoch 83 loss_pde:0.01862341, loss_ib:0.00254615\n",
      "epoch 83 loss_pde:0.01862429, loss_ib:0.00254489\n",
      "epoch 83 loss_pde:0.01862891, loss_ib:0.00254337\n",
      "epoch 83 loss_pde:0.01863199, loss_ib:0.00254185\n",
      "epoch 83 loss_pde:0.01863234, loss_ib:0.00253996\n",
      "epoch 83 loss_pde:0.01862565, loss_ib:0.00253775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 83 loss_pde:0.01864826, loss_ib:0.00253336\n",
      "epoch 83 loss_pde:0.01863878, loss_ib:0.00253193\n",
      "epoch 83 loss_pde:0.01862718, loss_ib:0.00253116\n",
      "epoch 83 loss_pde:0.01861586, loss_ib:0.00253081\n",
      "epoch 83 loss_pde:0.01860670, loss_ib:0.00253048\n",
      "epoch 83 loss_pde:0.01859907, loss_ib:0.00252987\n",
      "epoch 83 loss_pde:0.01859375, loss_ib:0.00252859\n",
      "epoch 83 loss_pde:0.01859161, loss_ib:0.00252628\n",
      "epoch 83 loss_pde:0.01859619, loss_ib:0.00252274\n",
      "epoch 83: loss 0.044161\n",
      "epoch 84 loss_pde:0.01859489, loss_ib:0.00252008\n",
      "epoch 84 loss_pde:0.01859577, loss_ib:0.00251772\n",
      "epoch 84 loss_pde:0.01857975, loss_ib:0.00251707\n",
      "epoch 84 loss_pde:0.01857468, loss_ib:0.00251607\n",
      "epoch 84 loss_pde:0.01856413, loss_ib:0.00251540\n",
      "epoch 84 loss_pde:0.01854110, loss_ib:0.00251528\n",
      "epoch 84 loss_pde:0.01851627, loss_ib:0.00251539\n",
      "epoch 84 loss_pde:0.01850068, loss_ib:0.00251476\n",
      "epoch 84 loss_pde:0.01846748, loss_ib:0.00251579\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [29]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m tic \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m----> 4\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m toc \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotal training time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtoc \u001b[38;5;241m-\u001b[39m tic\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Input \u001b[0;32mIn [23]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Optimize loss function\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m loss_value \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(loss, \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m loss\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Print total loss\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/optim/optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/grad_mode.py:28\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m():\n\u001b[0;32m---> 28\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/optim/lbfgs.py:437\u001b[0m, in \u001b[0;36mLBFGS.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_iter \u001b[38;5;241m!=\u001b[39m max_iter:\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;66;03m# re-evaluate function only if not in last iteration\u001b[39;00m\n\u001b[1;32m    434\u001b[0m     \u001b[38;5;66;03m# the reason we do this: in a stochastic setting,\u001b[39;00m\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;66;03m# no use to re-evaluate that function here\u001b[39;00m\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[0;32m--> 437\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    438\u001b[0m     flat_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gather_flat_grad()\n\u001b[1;32m    439\u001b[0m     opt_cond \u001b[38;5;241m=\u001b[39m flat_grad\u001b[38;5;241m.\u001b[39mabs()\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m tolerance_grad\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/grad_mode.py:28\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m():\n\u001b[0;32m---> 28\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [23]\u001b[0m, in \u001b[0;36mtrain.<locals>.closure\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Print iteration, loss of PDE and ICs\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m loss_pde:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_pde\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.8f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, loss_ib:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_ib\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.8f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 29\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_tensor.py:307\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    300\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    301\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    305\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    306\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 307\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:154\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m--> 154\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "optimizer = torch.optim.LBFGS(model.parameters(),lr=0.5,max_iter=20)\n",
    "epochs = 1000\n",
    "tic = time.time()\n",
    "for epoch in range(1, epochs+1):\n",
    "    train(epoch)\n",
    "toc = time.time()\n",
    "print(f'Total training time: {toc - tic}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device1 = torch.device(\"cpu\")\n",
    "#model.load_state_dict(torch.load(PATH))\n",
    "#torch.save(model,'2DRiemanncase8.pt')\n",
    "#model = torch.save('backcorner.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'naca0012NS_time005.pth'\n",
    "model = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 后处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on the whole computational domain\n",
    "for i in range(50):\n",
    "#u = to_numpy(model(x_test))\n",
    "#x = to_numpy(x_test)\n",
    "    x = np.linspace(0.0, 1.0, 401)                                   # Partitioned spatial axis\n",
    "    y = np.linspace(0.0, 1.0, 400)                                   # Partitioned spatial axis\n",
    "    t = np.linspace(0.20, 0.20, 1)                                        # Partitioned time axis\n",
    "    t_grid, x_grid,y_grid = np.meshgrid(t,x,y)                                    # (t,x) in [0,0.2]x[a,b]\n",
    "    T = t_grid.flatten()[:, None]                                        # Vectorized t_grid\n",
    "    X = x_grid.flatten()[:, None]                                         # Vectorized x_grid\n",
    "    Y = y_grid.flatten()[:, None]                                         # Vectorized x_grid\n",
    "    x_test = np.hstack((T, X, Y))                                            # Vectorized whole domain\n",
    "    x_test = torch.tensor(x_test, dtype=torch.float32).to(device1)\n",
    "    u = to_numpy(model(x_test))\n",
    "    #Xp,Yp = meshgrid(x,y)\n",
    "    \n",
    "    ue = np.zeros((401,400))\n",
    "    for i in range(0,401):\n",
    "        for j in range(0,400):\n",
    "            ue[i,j] = u[j*400+i,3]\n",
    "            \n",
    "    plt.figure()\n",
    "    plt.contourf(x_grid[:,0,:],y_grid[:,0,:],ue,50)\n",
    "    #plt.pcolor(x_grid[:,0,:],y_grid[:,0,:],ue)\n",
    "    #plt.colorbar(x_grid[:,0,:],y_grid[:,0,:],ue)\n",
    "    #plt.scatter(x_int_train[:,1],)\n",
    "    #plt.pcolor(x[:],u[:,1])\n",
    "    #plt.pcolor(x[:],u[:,2])\n",
    "    ax = plt.gca()\n",
    "    ax.set_aspect(1)\n",
    "    plt.savefig(\"test_rasterization.png\", dpi=150)\n",
    "    plt.show()\n",
    " \n",
    "    #    uo = ue.flatten()[:,None]\n",
    "    #    uxy= np.hstack((X, Y,uo))                                            # Vectorized whole domain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for k in range(50):\n",
    "    Nd = 600\n",
    "    T = 2.0/50*k\n",
    "    t = np.linspace(T, T, 1)                                   # Partitioned spatial axis\n",
    "    x = np.linspace(0.0,Lx,Nd)                                   # Partitioned spatial axis\n",
    "    y = np.linspace(0.0,Ly,Nd)                                   # Partitioned spatial axis\n",
    "    t_grid,x_grid,y_grid = np.meshgrid(t,x,y)                                    # (t,x) in [0,0.2]x[a,b]\n",
    "    T = t_grid.flatten()[:, None]                                         # Vectorized x_grid\n",
    "    X = x_grid.flatten()[:, None]                                         # Vectorized x_grid\n",
    "    Y = y_grid.flatten()[:, None]                                         # Vectorized x_grid\n",
    "    x_test = np.hstack((T,X,Y))                                            # Vectorized whole domain\n",
    "    x_test = torch.tensor(x_test, dtype=torch.float32).to(device)\n",
    "    u = to_numpy(model(x_test))\n",
    "    #Xp,Yp = meshgrid(x,y)\n",
    "    \n",
    "    \n",
    "    x_test = np.hstack((T,X,Y))                                            # Vectorized whole domain\n",
    "    ue = np.zeros((Nd,Nd))\n",
    "    for j in range(0,Nd):\n",
    "        for i in range(0,Nd):\n",
    "            ue[i,j] = u[i*Nd+j,0]\n",
    "            x1 = x_test[i*Nd+j,1] \n",
    "            y1 = x_test[i*Nd+j,2] \n",
    "            if (((x1 - rx)>0) and ((x1-rx)<1)):\n",
    "                yd1,yd2,dy1,dy2 = Naca0012data(x1-rx)\n",
    "                if (((y1-ry)>yd2) and ((y1-ry)<yd1)):\n",
    "                    ue[i,j] = 0.0\n",
    "                \n",
    "    uo1 = ue.flatten()[:,None]\n",
    "    ue = np.zeros((Nd,Nd))\n",
    "    for j in range(0,Nd):\n",
    "        for i in range(0,Nd):\n",
    "            ue[i,j] = u[i*Nd+j,1]\n",
    "            x1 = x_test[i*Nd+j,1] \n",
    "            y1 = x_test[i*Nd+j,2] \n",
    "            if (((x1 - rx)>0) and ((x1-rx)<1)):\n",
    "                yd1,yd2,dy1,dy2 = Naca0012data(x1-rx)\n",
    "                if (((y1-ry)>yd2) and ((y1-ry)<yd1)):\n",
    "                    ue[i,j] = 0.0\n",
    "    uo2 = ue.flatten()[:,None]\n",
    "    ue = np.zeros((Nd,Nd))\n",
    "    for j in range(0,Nd):\n",
    "        for i in range(0,Nd):\n",
    "            ue[i,j] = u[i*Nd+j,2]\n",
    "            x1 = x_test[i*Nd+j,1] \n",
    "            y1 = x_test[i*Nd+j,2] \n",
    "            if (((x1 - rx)>0) and ((x1-rx)<1)):\n",
    "                yd1,yd2,dy1,dy2 = Naca0012data(x1-rx)\n",
    "                if (((y1-ry)>yd2) and ((y1-ry)<yd1)):\n",
    "                    ue[i,j] = 0.0\n",
    "            \n",
    "    uo3 = ue.flatten()[:,None]\n",
    "    ue = np.zeros((Nd,Nd))\n",
    "    for j in range(0,Nd):\n",
    "        for i in range(0,Nd):\n",
    "            ue[i,j] = u[i*Nd+j,3]\n",
    "            x1 = x_test[i*Nd+j,1] \n",
    "            y1 = x_test[i*Nd+j,2] \n",
    "            if (((x1 - rx)>0) and ((x1-rx)<1)):\n",
    "                yd1,yd2,dy1,dy2 = Naca0012data(x1-rx)\n",
    "                if (((y1-ry)>yd2) and ((y1-ry)<yd1)):\n",
    "                    ue[i,j] = 0.0\n",
    "    uo4 = ue.flatten()[:,None]\n",
    "    filename = \"naca{}.dat\".format(k) \n",
    "    uxy= np.hstack((X, Y,uo1,uo2,uo3,uo4)) \n",
    "    f = open(filename,'w')\n",
    "    f.write(var2)\n",
    "    np.savetxt(f, uxy)# Vectorized whole domain\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nd = 600\n",
    "T = 0.5\n",
    "t = np.linspace(T, T, 1)                                   # Partitioned spatial axis\n",
    "x = np.linspace(0.0,Lx,Nd)                                   # Partitioned spatial axis\n",
    "y = np.linspace(0.0,Ly,Nd)                                   # Partitioned spatial axis\n",
    "t_grid,x_grid,y_grid = np.meshgrid(t,x,y)                                    # (t,x) in [0,0.2]x[a,b]\n",
    "T = t_grid.flatten()[:, None]                                         # Vectorized x_grid\n",
    "X = x_grid.flatten()[:, None]                                         # Vectorized x_grid\n",
    "Y = y_grid.flatten()[:, None]                                         # Vectorized x_grid\n",
    "x_test = np.hstack((T,X,Y))                                            # Vectorized whole domain\n",
    "x_test = torch.tensor(x_test, dtype=torch.float32).to(device)\n",
    "u = to_numpy(model(x_test))\n",
    "#Xp,Yp = meshgrid(x,y)\n",
    "\n",
    "\n",
    "x_test = np.hstack((T,X,Y))                                            # Vectorized whole domain\n",
    "ue = np.zeros((Nd,Nd))\n",
    "for j in range(0,Nd):\n",
    "    for i in range(0,Nd):\n",
    "        ue[i,j] = u[i*Nd+j,0]\n",
    "        x1 = x_test[i*Nd+j,1] \n",
    "        y1 = x_test[i*Nd+j,2] \n",
    "        if ((x1 - rx)**2 +(y1-ry)**2< rd**2):\n",
    "            ue[i,j] = 0.0\n",
    "            \n",
    "uo1 = ue.flatten()[:,None]\n",
    "ue = np.zeros((Nd,Nd))\n",
    "for j in range(0,Nd):\n",
    "    for i in range(0,Nd):\n",
    "        ue[i,j] = u[i*Nd+j,1]\n",
    "        x1 = x_test[i*Nd+j,1] \n",
    "        y1 = x_test[i*Nd+j,2] \n",
    "        if ((x1 - rx)**2 +(y1-ry)**2< rd**2):\n",
    "            ue[i,j] = 0.0\n",
    "uo2 = ue.flatten()[:,None]\n",
    "ue = np.zeros((Nd,Nd))\n",
    "for j in range(0,Nd):\n",
    "    for i in range(0,Nd):\n",
    "        ue[i,j] = u[i*Nd+j,2]\n",
    "        x1 = x_test[i*Nd+j,1] \n",
    "        y1 = x_test[i*Nd+j,2] \n",
    "        if ((x1 - rx)**2 +(y1-ry)**2< rd**2):\n",
    "            ue[i,j] = 0.0\n",
    "        \n",
    "uo3 = ue.flatten()[:,None]\n",
    "ue = np.zeros((Nd,Nd))\n",
    "for j in range(0,Nd):\n",
    "    for i in range(0,Nd):\n",
    "        ue[i,j] = u[i*Nd+j,1]\n",
    "        x1 = x_test[i*Nd+j,1] \n",
    "        y1 = x_test[i*Nd+j,2] \n",
    "        if (((x1 - rx)>0) and ((x1-rx)<1)):\n",
    "            yd1,yd2,dy1,dy2 = Naca0012data(x1-rx)\n",
    "            if (((y1-ry)>yd2) and ((y1-ry)<yd1)):\n",
    "              #  print(x1-rx,yd1,dy1,yd2,dy2)\n",
    "                ue[i,j] = 0.0\n",
    "#uo4 = ue.flatten()[:,None]\n",
    "#filename = \"temp{}.dat\".format(44) \n",
    "#uxy= np.hstack((X, Y,uo1,uo2,uo3,uo4)) \n",
    "#f = open(filename,'w')\n",
    "#f.write(var2)\n",
    "#np.savetxt(f, uxy)# Vectorized whole domain\n",
    "#f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "var2 = \"\"\"TITLE=\"Euler\"\n",
    " VARIABLES=\"x\",\"y\",\"rou\"，\"u\",\"v\",\"p\"\n",
    "ZONE I=         600 J=         600 F=POINT\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (4215644540.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_704458/4215644540.py\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    #        print(var2)\u001b[0m\n\u001b[0m                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "filename = '1.dat'\n",
    "with open(filename, 'r', encoding='utf-8') as fileobj:\n",
    "    for line in fileobj:\n",
    "#        print(var2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.contourf(x_grid[:,0,:],y_grid[:,0,:],ue,60)\n",
    "#plt.pcolor(x_grid[:,0,:],y_grid[:,0,:],ue)\n",
    "#plt.colorbar(x_grid[:,0,:],y_grid[:,0,:],ue)\n",
    "#plt.scatter(x_int_train[:,1],)\n",
    "#plt.pcolor(x[:],u[:,1])\n",
    "#plt.pcolor(x[:],u[:,2])\n",
    "ax = plt.gca()\n",
    "ax.set_aspect(1)\n",
    "plt.show()\n",
    "plt.savefig(\"Riemann2.pdf\", dpi=150)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "            #fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n",
    "#surf = ax.plot_surface(x_grid[:,0,:], y_grid[:,0,:], ue, cmap=cm.coolwarm,\n",
    "#                       linewidth=0, antialiased=False)\n",
    "#\n",
    "## Customize the z axis.\n",
    "#ax.set_zlim(0, 1.01)\n",
    "#ax.zaxis.set_major_locator(LinearLocator(10))\n",
    "## A StrMethodFormatter is used automatically\n",
    "#ax.zaxis.set_major_formatter('{x:.02f}')\n",
    "#\n",
    "## Add a color bar which maps values to colors.\n",
    "#fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlimits = np.array([[0.,Tend],[0.0, 3], [0.0,3]])  #interal\n",
    "sampling = LHS(xlimits=xlimits)\n",
    "x_int_train = sampling(num_int)\n",
    "A = []\n",
    "for i in range(num_int):\n",
    "    x = x_int_train[i,1]\n",
    "    y = x_int_train[i,2]\n",
    "    if (x< 0.5 and y< 1.5):\n",
    "        A.append(i)\n",
    "x_int_train = np.delete(x_int_train,A,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(x_int_train[1:1000,1],x_int_train[1:1000,2])\n",
    "#plt.pcolor(x_grid[:,0,:],y_grid[:,0,:],ue)\n",
    "#plt.colorbar(x_grid[:,0,:],y_grid[:,0,:],ue)\n",
    "#plt.scatter(x_int_train[:,1],)\n",
    "#plt.pcolor(x[:],u[:,1])\n",
    "#plt.pcolor(x[:],u[:,2])\n",
    "ax = plt.gca()\n",
    "ax.set_aspect(1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model,'2DRiemanncase8.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device1 = torch.device(\"cpu\")\n",
    "#model.load_state_dict(torch.load(PATH))\n",
    "#model = model.to(device1)\n",
    "#torch.save(model,'2DRiemanncase8.pt')\n",
    "model = torch.load('2DRiemanncase8.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
