{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import time\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import arange, meshgrid\n",
    "#\n",
    "from smt.sampling_methods import LHS\n",
    "# Seeds\n",
    "torch.manual_seed(123)\n",
    "np.random.seed(123)\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    def closure():\n",
    "        optimizer.zero_grad()                                                     # Optimizer\n",
    "      #  loss_bdR = model.bd_OX(x_bcR_train) \n",
    "        loss_pde = model.loss_pde(x_int_train)                                    # Loss function of PDE\n",
    "        loss_bdL = model.loss_bc(x_bcL_train, rho_bcL_train,u_bcL_train,v_bcL_train,p_bcL_train)   # Loss function of IC\n",
    "        loss_bdR1 = model.loss_bcR(x_bcR1_train)   # Loss function of IC\n",
    "        loss_bdR2 = model.loss_bcR(x_bcR2_train)   # Loss function of IC\n",
    "        loss_bdD = model.loss_bcD(x_bcD_train)   # Loss function of IC\n",
    "        #loss_cut = model.loss_bc1(x_cut_train, rho_cut_train,u_cut_train,v_cut_train,p_cut_train)   # Loss function of IC\n",
    "      #  loss_bdD = model.loss_bcOY(x_bcD_train) \n",
    "      #  loss_bdU = model.loss_bcOY(x_bcU_train) \n",
    "        loss_bdI = model.bd_B(x_bcI_train, sin_bcI_train,cos_bcI_train)  \n",
    "        loss_bd = 10*(loss_bdL + loss_bdI + loss_bdR1 + loss_bdR2)# + loss_bdL +loss_bdU + loss_bdD)\n",
    "        #loss_bd = 10*(loss_bdL)# + loss_bdL +loss_bdU + loss_bdD)\n",
    "        loss = loss_pde + loss_bd# + loss_bdR + loss_bdD +loss_bdI)                                         # Total loss function G(theta)\n",
    "        #loss = loss_pde + 10*(loss_bdL)                                         # Total loss function G(theta)\n",
    "\n",
    "        # Print iteration, loss of PDE and ICs\n",
    "        print(f'epoch {epoch} loss_pde:{loss_pde:.8f}, loss_bd:{loss_bd:.8f}')\n",
    "        loss.backward()\n",
    "        return loss\n",
    "\n",
    "    # Optimize loss function\n",
    "    loss = optimizer.step(closure)\n",
    "    loss_value = loss.item() if not isinstance(loss, float) else loss\n",
    "    # Print total loss\n",
    "    print(f'epoch {epoch}: loss {loss_value:.6f}')\n",
    "    \n",
    "# Calculate gradients using torch.autograd.grad\n",
    "def gradients(outputs, inputs):\n",
    "    return torch.autograd.grad(outputs, inputs,grad_outputs=torch.ones_like(outputs), create_graph=True)\n",
    "\n",
    "# Convert torch tensor into np.array\n",
    "def to_numpy(input):\n",
    "    if isinstance(input, torch.Tensor):\n",
    "        return input.detach().cpu().numpy()\n",
    "    elif isinstance(input, np.ndarray):\n",
    "        return input\n",
    "    else:\n",
    "        raise TypeError('Unknown type of input, expected torch.Tensor or ' \\\n",
    "                        'np.ndarray, but got {}'.format(type(input)))\n",
    "\n",
    "# Initial conditions\n",
    "def BC_L(x):\n",
    "    N =x.shape[0]\n",
    "    rho_init = np.zeros((x.shape[0]))                                              # rho - initial condition\n",
    "    u_init = np.zeros((x.shape[0]))                                                # u - initial condition\n",
    "    v_init = np.zeros((x.shape[0]))                                                # u - initial condition\n",
    "    p_init = np.zeros((x.shape[0]))                                                # p - initial condition\n",
    "    \n",
    "    gamma = 1.4\n",
    "    ms = 3.0\n",
    "    #rho1 = 4.2\n",
    "    #p1 = 3.0\n",
    "    #v1 = 0.0\n",
    "    #u1 = 2.0 #ms*np.sqrt(gamma)\n",
    "    rho1 = 1.0\n",
    "    p1 = 0.71429\n",
    "    v1 = 0.0\n",
    "    #u1 = ms*npsqrt(gamma)\n",
    "    u1 = u_set_in \n",
    "    # rho, p - initial condition\n",
    "    for i in range(N):\n",
    "        rho_init[i] = rho1\n",
    "        u_init[i] = u1\n",
    "        v_init[i] = v1\n",
    "        p_init[i] = p1\n",
    "\n",
    "    return rho_init, u_init, v_init,p_init\n",
    "def BC_R(x):\n",
    "    N =x.shape[0]\n",
    "    rho_init = np.zeros((x.shape[0]))                                              # rho - initial condition\n",
    "    u_init = np.zeros((x.shape[0]))                                                # u - initial condition\n",
    "    v_init = np.zeros((x.shape[0]))                                                # u - initial condition\n",
    "    p_init = np.zeros((x.shape[0]))                                                # p - initial condition\n",
    "    \n",
    "    gamma = 1.4\n",
    "    ms = 3.0\n",
    "    rho1 = 1.0\n",
    "    p1 = 1.0\n",
    "    v1 = 0.0\n",
    "    u1 = 0\n",
    "    # rho, p - initial condition\n",
    "    for i in range(N):\n",
    "        rho_init[i] = rho1\n",
    "        u_init[i] = u1\n",
    "        v_init[i] = v1\n",
    "        p_init[i] = p1\n",
    "\n",
    "    return rho_init, u_init, v_init,p_init\n",
    "def BC_Cut(x):\n",
    "    N =x.shape[0]\n",
    "    rho_init = np.zeros((x.shape[0]))                                              # rho - initial condition\n",
    "    u_init = np.zeros((x.shape[0]))                                                # u - initial condition\n",
    "    v_init = np.zeros((x.shape[0]))                                                # u - initial condition\n",
    "    p_init = np.zeros((x.shape[0]))                                                # p - initial condition\n",
    "    \n",
    "    gamma = 1.4\n",
    "    ms = 3.0\n",
    "    rho1 = 1.0\n",
    "    p1 = 1.0\n",
    "    v1 = 0.0\n",
    "    u1 = 0\n",
    "    # rho, p - initial condition\n",
    "    for i in range(N):\n",
    "        rho_init[i] = 10.01\n",
    "        u_init[i] =  0\n",
    "        v_init[i] = 0\n",
    "        p_init[i] = 10.01\n",
    "\n",
    "    return rho_init, u_init, v_init,p_init\n",
    " \n",
    "    \n",
    "class DNN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DNN, self).__init__()\n",
    "        self.net = nn.Sequential()                                                  # Define neural network\n",
    "        self.net.add_module('Linear_layer_1', nn.Linear(2, 90))                     # First linear layer\n",
    "        self.net.add_module('Tanh_layer_1', nn.Tanh())                              # First activation Layer\n",
    "\n",
    "        for num in range(2, 7):                                                     # Number of layers (2 through 7)\n",
    "            self.net.add_module('Linear_layer_%d' % (num), nn.Linear(90, 90))       # Linear layer\n",
    "            self.net.add_module('Tanh_layer_%d' % (num), nn.Tanh())                 # Activation Layer\n",
    "        self.net.add_module('Linear_layer_final', nn.Linear(90, 4))                 # Output Layer\n",
    "\n",
    "    # Forward Feed\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "    def bd_B(self,x,sin,cos):\n",
    "        yb = self.net(x)\n",
    "        rhob,pb,ub,vb = yb[:, 0:1], yb[:, 1:2], yb[:, 2:3],yb[:,3:]\n",
    "        drhob_g = gradients(rhob, x)[0]                                      # Gradient [u_t, u_x]\n",
    "        rhob_x, rhob_y = drhob_g[:, :1], drhob_g[:, 1:2]                            # Partial derivatives u_t, u_x\n",
    "        dub_g = gradients(ub, x)[0]                                      # Gradient [u_t, u_x]\n",
    "        ub_x, ub_y = dub_g[:, :1], dub_g[:, 1:2]                            # Partial derivatives u_t, u_x\n",
    "        dvb_g = gradients(vb, x)[0]                                      # Gradient [u_t, u_x]\n",
    "        vb_x, vb_y = dvb_g[:, :1], dvb_g[:, 1:2]                            # Partial derivatives u_t, u_x\n",
    "        dpb_g = gradients(pb, x)[0]                                      # Gradient [p_t, p_x]\n",
    "        pb_x, pb_y = dpb_g[:, :1], dpb_g[:, 1:2]                            # Partial derivatives p_t, p_x\n",
    "        \n",
    "        deltau = ub_x + vb_y\n",
    "        #lam = 0.1*(abs(deltau) - deltau) + 1\n",
    "        #lam = (deltau) - deltau) + 1\n",
    "        \n",
    "        fb = (((ub*cos + vb*sin))**2).mean()# +\\\n",
    "            #(((pb_x*cos + pb_y*sin))**2).mean()# +\\\n",
    "            #(((rhob_x*cos + rhob_y*sin))**2).mean()\n",
    "        return fb\n",
    "    def bd_OY(self,x):\n",
    "        y = self.net(x)\n",
    "        rho,p,u,v = y[:, 0:1], y[:, 1:2], y[:, 2:3],y[:,3:]\n",
    "        \n",
    "        drho_g = gradients(rho, x)[0]                                  # Gradient [rho_t, rho_x]\n",
    "        rho_x,rho_y = drho_g[:, :1], drho_g[:, 1:2]                    # Partial derivatives rho_t, rho_x\n",
    "        du_g = gradients(u, x)[0]                                      # Gradient [u_t, u_x]\n",
    "        u_x, u_y = du_g[:, :1], du_g[:, 1:2]                            # Partial derivatives u_t, u_x\n",
    "        dv_g = gradients(v, x)[0]                                      # Gradient [u_t, u_x]\n",
    "        v_x, v_y = dv_g[:, :1], dv_g[:, 1:2]                            # Partial derivatives u_t, u_x\n",
    "        dp_g = gradients(p, x)[0]                                      # Gradient [p_t, p_x]\n",
    "        p_x, p_y = dp_g[:, :1], dp_g[:, 1:2]                            # Partial derivatives p_t, p_x\n",
    "        \n",
    "        deltau = u_x + v_y\n",
    "        lam = 0.1*(abs(deltau) - deltau) + 1\n",
    "        \n",
    "        f = ((( u_y)/lam)**2).mean() +\\\n",
    "            ((( v_y)/lam)**2).mean() +\\\n",
    "            ((( p_y)/lam)**2).mean() +\\\n",
    "            ((( rho_y)/lam)**2).mean()\n",
    "        return f\n",
    "    \n",
    "    def bd_OX(self,x):\n",
    "        y = self.net(x)\n",
    "        rho,p,u,v = y[:, 0:1], y[:, 1:2], y[:, 2:3],y[:,3:]\n",
    "        \n",
    "        drho_g = gradients(rho, x)[0]                                  # Gradient [rho_t, rho_x]\n",
    "        rho_x,rho_y = drho_g[:, :1], drho_g[:, 1:2]                    # Partial derivatives rho_t, rho_x\n",
    "        du_g = gradients(u, x)[0]                                      # Gradient [u_t, u_x]\n",
    "        u_x, u_y = du_g[:, :1], du_g[:, 1:2]                            # Partial derivatives u_t, u_x\n",
    "        dv_g = gradients(v, x)[0]                                      # Gradient [u_t, u_x]\n",
    "        v_x, v_y = dv_g[:, :1], dv_g[:, 1:2]                            # Partial derivatives u_t, u_x\n",
    "        dp_g = gradients(p, x)[0]                                      # Gradient [p_t, p_x]\n",
    "        p_x, p_y = dp_g[:, :1], dp_g[:, 1:2]                            # Partial derivatives p_t, p_x\n",
    "        \n",
    "        deltau = u_x + v_y\n",
    "        lam = 0.01*(abs(deltau) - deltau) + 1\n",
    "        \n",
    "        f = ((( u_x)/lam)**2).mean() +\\\n",
    "            ((( v_x)/lam)**2).mean() +\\\n",
    "            ((( p_x)/lam)**2).mean() +\\\n",
    "            ((( rho_x)/lam)**2).mean()\n",
    "        return f\n",
    "        \n",
    "    # Loss function for PDE\n",
    "    def loss_pde(self, x):\n",
    "        y = self.net(x)                                                # Neural network\n",
    "        rho,p,u,v = y[:, 0:1], y[:, 1:2], y[:, 2:3],y[:,3:]\n",
    "        \n",
    "        gamma = 1.4                                                    # Heat Capacity Ratio\n",
    "\n",
    "        eps = 1e-8\n",
    "        \n",
    "        yL = self.net(x_intL_train)\n",
    "        yR = self.net(x_intR_train)\n",
    "        yU = self.net(x_intU_train)\n",
    "        yD = self.net(x_intD_train)\n",
    "        rhoL,pL,uL,vL = yL[:, 0:1], yL[:, 1:2], yL[:, 2:3],yL[:,3:]\n",
    "        rhoR,pR,uR,vR = yR[:, 0:1], yR[:, 1:2], yR[:, 2:3],yR[:,3:]\n",
    "        rhoU,pU,uU,vU = yU[:, 0:1], yU[:, 1:2], yU[:, 2:3],yU[:,3:]\n",
    "        rhoD,pD,uD,vD = yD[:, 0:1], yD[:, 1:2], yD[:, 2:3],yD[:,3:]\n",
    "        \n",
    "        E = p/rho/0.4 + 0.5*(u**2+v**2)\n",
    "        EL = pL/rhoL/0.4 + 0.5*(uL**2+vL**2)\n",
    "        ER = pR/rhoR/0.4 + 0.5*(uR**2+vR**2)\n",
    "        EU = pU/rhoU/0.4 + 0.5*(uU**2+vU**2)\n",
    "        ED = pD/rhoD/0.4 + 0.5*(uD**2+vD**2)\n",
    "        cL = torch.sqrt(1.4*abs(pL)/(abs(rhoL)+eps))\n",
    "        cR = torch.sqrt(1.4*abs(pR)/(abs(rhoR)+eps))\n",
    "        c = torch.sqrt(1.4*abs(p)/(abs(rho)+eps))\n",
    "        \n",
    "        smax = torch.max(cL+uL,cR+uR)\n",
    "        smin = torch.min(-cL+uL,-cR+uR)\n",
    " \n",
    "        # Gradients and partial derivatives\n",
    "        drho_g = gradients(rho, x)[0]                            \n",
    "        rho_x,rho_y = drho_g[:, :1], drho_g[:, 1:2]\n",
    "        du_g = gradients(u, x)[0]                                \n",
    "        u_x, u_y = du_g[:, :1], du_g[:, 1:2]         \n",
    "        dv_g = gradients(v, x)[0]                    \n",
    "        v_x, v_y = dv_g[:, :1], dv_g[:, 1:2]         \n",
    "        dp_g = gradients(p, x)[0]                    \n",
    "        p_x, p_y = dp_g[:, :1], dp_g[:, 1:2]         \n",
    "        \n",
    "       # du_gg = gradients(u_x, x)[0]                                      # Gradient [u_t, u_x]\n",
    "       # u_xx, u_xy = du_gg[:, :1], du_gg[:, 1:2]                            # Partial derivatives u_t, u_x\n",
    "       # \n",
    "       # dv_gg = gradients(v_y, x)[0]                                      # Gradient [u_t, u_x]\n",
    "       # v_yx, v_yy = dv_gg[:, :1], dv_gg[:, 1:2]                            # Partial derivatives u_t, u_x\n",
    " \n",
    "        s1 = (u-uL)**2 *rho*rhoL - (p-pL)*(rho - rhoL)\n",
    "        s2 = (E-EL) *(u-uL)*rho*rhoL - (p*u-pL*uL)*(rho - rhoL)\n",
    "        s3 = (v-vD)**2 *rho*rhoD - (p-pD)*(rho - rhoD)\n",
    "        s4 = (E-ED)*(v-vD)*rho*rhoD - (p*v-pD*vD)*(rho -rhoD)\n",
    "        s5 = (u-uR)**2 *rho*rhoR - (p-pR)*(rho - rhoR)\n",
    "        s6 = (E-ER) *(u-uR)*rho*rhoR - (p*u-pR*uR)*(rho - rhoR)\n",
    "        s7 = (v-vU)**2 *rho*rhoU - (p-pU)*(rho - rhoU)\n",
    "        s8 = (E-EU)*(v-vU)*rho*rhoU - (p*v-pU*vU)*(rho -rhoU)\n",
    "        \n",
    "        deltau = u_x + v_y\n",
    "        lam = 0.1*(abs(deltau) - deltau) + 1\n",
    "        lam =  1\n",
    "\n",
    "      #  vis = -0.1*(u_xx + v_yy)\n",
    "        f = (((rho*deltau+u*rho_x + v*rho_y)/lam)**2).mean() +\\\n",
    "            (((rho*u*u_x+rho*v*u_y+p_x )/lam)**2).mean() +\\\n",
    "            (((rho*u*v_x+rho*v*v_y+p_y )/lam)**2).mean() +\\\n",
    "            (((u*p_x+v*p_y+1.4*p*deltau )/lam)**2).mean()# +\\\n",
    "           # 10*((abs(rho-1) - (rho-1))**2).mean()   + \\\n",
    "           # 10*((abs(p-0.7) - (p-0.7))**2).mean() +\\\n",
    "           # 10*(s1**2).mean() +\\\n",
    "           # 10*(s2**2).mean() +\\\n",
    "           # 10*(s3**2).mean() +\\\n",
    "           # 10*(s4**2).mean() +\\\n",
    "           # 10*(s5**2).mean() +\\\n",
    "           # 10*(s6**2).mean() +\\\n",
    "           # 10*(s7**2).mean() +\\\n",
    "           # 10*(s8**2).mean() \n",
    "            \n",
    "            #((abs(smax-u-c) -(smax - u-c))**2).mean() +\\\n",
    "            #((abs(smin-u+c)+(smin-u+c))**2).mean() \n",
    "           # 0.1*((abs(eta_t+phi_x)+eta_t+phi_x)).mean()   + \\\n",
    "           # (abs(rho_t)).mean() + (abs(U3_t)).mean() # \n",
    "        return f\n",
    "\n",
    "    # Loss function for initial condition\n",
    "    def loss_bc(self, x_ic, rho_ic, u_ic, v_ic,p_ic):\n",
    "        U_ic = self.net(x_ic)                                                      # Initial condition\n",
    "        rho_ic_nn, p_ic_nn,u_ic_nn,v_ic_nn = U_ic[:, 0], U_ic[:, 1], U_ic[:, 2],U_ic[:,3]            # rho, u, p - initial condition\n",
    "\n",
    "        # Loss function for the initial condition\n",
    "        loss_ics = ((u_ic_nn - u_ic) ** 2).mean() + \\\n",
    "               ((rho_ic_nn- rho_ic) ** 2).mean()  + \\\n",
    "               ((p_ic_nn - p_ic) ** 2).mean() +\\\n",
    "               ((v_ic_nn - v_ic) ** 2).mean()\n",
    "\n",
    "        return loss_ics\n",
    "\n",
    "    def loss_bcR(self,x):\n",
    "        U = self.net(x)                                                      # Initial condition\n",
    "        rho, p,u,v = U[:, 0], U[:, 1], U[:, 2],U[:,3]            # rho, u, p - initial condition\n",
    "        E = p/0.4 + 0.5*rho*(u**2 + v**2)\n",
    "\n",
    "        E0 = 0.71429/0.4 + 0.5*(u_set_in**2)\n",
    "        # Loss function for the initial condition\n",
    "        loss_ics = ((rho*u*0.8 - u_set_in).mean())**2 #+ \\\n",
    "                   #((E*u*0.8 - E0*3.0).mean())**2\n",
    "        return loss_ics\n",
    "\n",
    "    def loss_bcD(self,x):\n",
    "        U = self.net(x)                                                      # Initial condition\n",
    "        rho, p,u,v = U[:, 0], U[:, 1], U[:, 2],U[:,3]            # rho, u, p - initial condition\n",
    "\n",
    "        # Loss function for the initial condition\n",
    "        loss_ics = (v**2).mean() #+ \\\n",
    "                   #abs((E*u*0.8 - E0*3.0/2).mean())          \n",
    "\n",
    "        return loss_ics\n",
    "    def loss_bc1(self, x_ic, rho_ic, u_ic, v_ic,p_ic):\n",
    "        U_ic = self.net(x_ic)                                                      # Initial condition\n",
    "        rho_ic_nn, p_ic_nn,u_ic_nn,v_ic_nn = U_ic[:, 0], U_ic[:, 1], U_ic[:, 2],U_ic[:,3]            # rho, u, p - initial condition\n",
    "\n",
    "        # Loss function for the initial condition\n",
    "        loss_ics = ((rho_ic_nn- rho_ic) ** 2).mean()  + \\\n",
    "               ((p_ic_nn - p_ic) ** 2).mean() \n",
    "\n",
    "        return loss_ics\n",
    "    \n",
    "# Solve Euler equations using PINNs\n",
    "# def main():\n",
    "  # Initialization\n",
    "def BD_circle(xc,yc,r,n):\n",
    "    x = np.zeros((n,2)) \n",
    "    sin = np.zeros((n,1)) \n",
    "    cos = np.zeros((n,1)) \n",
    "    dth = np.pi/n\n",
    "    for i in range(n):\n",
    "        the = dth*i\n",
    "        xd = np.cos(the + np.pi/2)\n",
    "        yd = np.sin(the + np.pi/2)\n",
    "    \n",
    "        x[i,0] = xc  + xd*r\n",
    "        x[i,1] = yc + yd*r\n",
    "        cos[i,0] = xd \n",
    "        sin[i,0] = yd\n",
    "\n",
    "        #cos[i,0] = 1\n",
    "        #sin[i,0] = 0\n",
    "    return x, sin,cos\n",
    "def BD_front(n1,n2):\n",
    "    x = np.zeros((n1+n2,2)) \n",
    "    for i in range(n1):\n",
    "        x[i,0] = 1.0\n",
    "        x[i,1] = i*(0.2/n1)\n",
    "    for i in range(n1,n2):\n",
    "        x[i,0] = 1.0 + (3.0/n1)\n",
    "        x[i,1] = 0.2\n",
    "    return x\n",
    "def Pertur(x, dx):\n",
    "    N =x.shape[0]\n",
    "    xL = np.zeros((N,2))\n",
    "    xR = np.zeros((N,2))\n",
    "    xU = np.zeros((N,2))\n",
    "    xD = np.zeros((N,2))\n",
    "    \n",
    "    for i in range(N):\n",
    "        \n",
    "        xL[i,0] = x[i,0] - dx\n",
    "        xR[i,0] = x[i,0] + dx\n",
    "        xU[i,0] = x[i,0]\n",
    "        xD[i,0] = x[i,0]\n",
    "        \n",
    "        xL[i,1] = x[i,1] \n",
    "        xR[i,1] = x[i,1]\n",
    "        xU[i,1] = x[i,1] + dx\n",
    "        xD[i,1] = x[i,1] - dx\n",
    "        \n",
    "    return xL,xR,xU,xD\n",
    " \n",
    "device = torch.device('cuda')                                          # Run on CPU\n",
    "lr = 0.0011                                                           # Learning rate\n",
    "num_i_train = 1000                                                # Random sampled points from IC0\n",
    "num_f_train = 20000                                                # Random sampled points in interior\n",
    "num_test = 1000                                                 # Random sampled points in interior\n",
    "\n",
    "u_set_in = 3.5\n",
    "#id_ic = np.random.choice(num_x, num_i_train, replace=False)           # Random sample numbering for IC\n",
    "\n",
    "xlimits = np.array([[0.0, 1.0], [0.0,2.0]])\n",
    "sampling = LHS(xlimits=xlimits)\n",
    "x_int_train = sampling(num_f_train)\n",
    "\n",
    "A = []\n",
    "for i in range(num_f_train):\n",
    "    x = x_int_train[i,0]\n",
    "    y = x_int_train[i,1]\n",
    "    if ((x-1)**2 + (y-1)**2 ) < 0.04:\n",
    "        A.append(i)\n",
    "x_int_train = np.delete(x_int_train,A,axis=0)\n",
    "\n",
    "xlimits = np.array([[0.0, 0.8], [0.75,1.25]])\n",
    "sampling = LHS(xlimits=xlimits)\n",
    "x_add_train = sampling(10000)\n",
    "x_int_train = np.vstack((x_int_train,x_add_train))\n",
    "\n",
    "\n",
    "\n",
    "x_cut_train = np.zeros((num_i_train,2))\n",
    "\n",
    "x_cut_train = np.zeros((num_i_train,2))\n",
    "for i in range(num_i_train):\n",
    "    theta = np.random.rand()*np.pi + np.pi/2\n",
    "    r = np.random.rand()*0.3\n",
    "    x_cut_train[i,0] = r*np.cos(theta) + 1\n",
    "    x_cut_train[i,1] = r*np.sin(theta) + 1\n",
    "\n",
    "\n",
    "\n",
    "xlimits_ic = np.array([[0.0, 0.1], [0.0,2.0]])\n",
    "sampling_ic = LHS(xlimits=xlimits_ic)\n",
    "x_bcL_train =  sampling_ic(10000)                                           # Vectorized whole domain\n",
    "\n",
    "xlimits_ic = np.array([[1.0, 1.0], [0.0,0.8]])\n",
    "sampling_ic = LHS(xlimits=xlimits_ic)\n",
    "x_bcR1_train =  sampling_ic(num_i_train)                                           # Vectorized whole domain\n",
    "\n",
    "\n",
    "xlimits_ic = np.array([[0.0, 1.0], [0.0,0.8]])\n",
    "sampling_ic = LHS(xlimits=xlimits_ic)\n",
    "x_bcD_train =  sampling_ic(num_i_train)                                           # Vectorized whole domain\n",
    "\n",
    "xlimits_ic = np.array([[1.0, 1.0], [1.2,2.0]])\n",
    "sampling_ic = LHS(xlimits=xlimits_ic)\n",
    "x_bcR2_train =  sampling_ic(num_i_train)                                           # Vectorized whole domain\n",
    "\n",
    "#xlimits_ic = np.array([[0.0, 10.0], [10.0,10.0]])\n",
    "#sampling_ic = LHS(xlimits=xlimits_ic)\n",
    "#x_bcU_train =  sampling_ic(num_i_train)                                           # Vectorized whole domain\n",
    "#xlimits_ic = np.array([[0.0, 10.0], [0.0,0.0]])\n",
    "#sampling_ic = LHS(xlimits=xlimits_ic)\n",
    "#x_bcD_train =  sampling_ic(num_i_train)                                           # Vectorized whole domain\n",
    "\n",
    "x_bcI_train,sin_bcI_train,cos_bcI_train = BD_circle(1.0,1.0,0.2,1000)\n",
    "\n",
    "x_intL_train,x_intR_train,x_intU_train,x_intD_train = Pertur(x_int_train, 0.01)\n",
    "\n",
    "rho_bcL_train, u_bcL_train,v_bcL_train, p_bcL_train = BC_L(x_bcL_train)  \n",
    "#rho_bcR_train, u_bcR_train,v_bcR_train, p_bcR_train = BC_L(x_bcR_train)  \n",
    "rho_cut_train, u_cut_train,v_cut_train, p_cut_train = BC_Cut(x_cut_train)  \n",
    "\n",
    "xlimits_test = np.array([[0.2, 0.2], [0.0, 1.0],[0.0, 1.0]])\n",
    "sampling_test = LHS(xlimits=xlimits_test)\n",
    "x_test =  sampling_test(num_test)                                           # Vectorized whole domain\n",
    "\n",
    "x_int_train = torch.tensor(x_int_train, requires_grad=True, dtype=torch.float32).to(device)\n",
    "\n",
    "rho_bcL_train = torch.tensor(rho_bcL_train, dtype=torch.float32).to(device)\n",
    "u_bcL_train = torch.tensor(u_bcL_train, dtype=torch.float32).to(device)\n",
    "v_bcL_train = torch.tensor(v_bcL_train, dtype=torch.float32).to(device)\n",
    "p_bcL_train = torch.tensor(p_bcL_train, dtype=torch.float32).to(device)\n",
    "\n",
    "#rho_bcR_train = torch.tensor(rho_bcR_train, dtype=torch.float32).to(device)\n",
    "#u_bcR_train = torch.tensor(u_bcR_train, dtype=torch.float32).to(device)\n",
    "#v_bcR_train = torch.tensor(v_bcR_train, dtype=torch.float32).to(device)\n",
    "#p_bcR_train = torch.tensor(p_bcR_train, dtype=torch.float32).to(device)\n",
    "\n",
    "x_bcD_train = torch.tensor(x_bcD_train,requires_grad=True, dtype=torch.float32).to(device)\n",
    "#x_bcU_train = torch.tensor(x_bcU_train, requires_grad=True, dtype=torch.float32).to(device)\n",
    "x_bcR1_train = torch.tensor(x_bcR1_train, requires_grad=True, dtype=torch.float32).to(device)\n",
    "x_bcR2_train = torch.tensor(x_bcR2_train, requires_grad=True, dtype=torch.float32).to(device)\n",
    "x_bcL_train = torch.tensor(x_bcL_train, dtype=torch.float32).to(device)\n",
    "x_bcI_train = torch.tensor(x_bcI_train, requires_grad=True, dtype=torch.float32).to(device)\n",
    "sin_bcI_train = torch.tensor(sin_bcI_train, dtype=torch.float32).to(device)\n",
    "cos_bcI_train = torch.tensor(cos_bcI_train, dtype=torch.float32).to(device)\n",
    "\n",
    "rho_cut_train = torch.tensor(rho_cut_train, dtype=torch.float32).to(device)\n",
    "u_cut_train = torch.tensor(u_cut_train, dtype=torch.float32).to(device)\n",
    "v_cut_train = torch.tensor(v_cut_train, dtype=torch.float32).to(device)\n",
    "p_cut_train = torch.tensor(p_cut_train, dtype=torch.float32).to(device)\n",
    "x_cut_train = torch.tensor(x_cut_train, dtype=torch.float32).to(device)\n",
    "\n",
    "\n",
    "x_intL_train = torch.tensor(x_intL_train,dtype=torch.float32).to(device)\n",
    "x_intR_train = torch.tensor(x_intR_train,dtype=torch.float32).to(device)\n",
    "x_intU_train = torch.tensor(x_intU_train,dtype=torch.float32).to(device)\n",
    "x_intD_train = torch.tensor(x_intD_train,dtype=torch.float32).to(device)\n",
    "\n",
    "print('Start training...')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 loss_pde:0.13287732, loss_bd:0.18572591\n",
      "epoch 1: loss 0.318603\n",
      "epoch 2 loss_pde:1.65014434, loss_bd:0.57050413\n",
      "epoch 2: loss 2.220649\n",
      "epoch 3 loss_pde:0.21881199, loss_bd:0.24776056\n",
      "epoch 3: loss 0.466573\n",
      "epoch 4 loss_pde:0.51872963, loss_bd:0.22754461\n",
      "epoch 4: loss 0.746274\n",
      "epoch 5 loss_pde:0.87673420, loss_bd:0.43685988\n",
      "epoch 5: loss 1.313594\n",
      "epoch 6 loss_pde:0.64287567, loss_bd:0.49892598\n",
      "epoch 6: loss 1.141802\n",
      "epoch 7 loss_pde:0.30602247, loss_bd:0.37747622\n",
      "epoch 7: loss 0.683499\n",
      "epoch 8 loss_pde:0.18562397, loss_bd:0.23450373\n",
      "epoch 8: loss 0.420128\n",
      "epoch 9 loss_pde:0.26920536, loss_bd:0.20162264\n",
      "epoch 9: loss 0.470828\n",
      "epoch 10 loss_pde:0.38085341, loss_bd:0.28924531\n",
      "epoch 10: loss 0.670099\n",
      "epoch 11 loss_pde:0.39287937, loss_bd:0.38334715\n",
      "epoch 11: loss 0.776227\n",
      "epoch 12 loss_pde:0.31088591, loss_bd:0.38152307\n",
      "epoch 12: loss 0.692409\n",
      "epoch 13 loss_pde:0.20864253, loss_bd:0.29778564\n",
      "epoch 13: loss 0.506428\n",
      "epoch 14 loss_pde:0.15134829, loss_bd:0.21416546\n",
      "epoch 14: loss 0.365514\n",
      "epoch 15 loss_pde:0.16291210, loss_bd:0.19788535\n",
      "epoch 15: loss 0.360797\n",
      "epoch 16 loss_pde:0.21124023, loss_bd:0.24974817\n",
      "epoch 16: loss 0.460988\n",
      "epoch 17 loss_pde:0.23790729, loss_bd:0.30960757\n",
      "epoch 17: loss 0.547515\n",
      "epoch 18 loss_pde:0.21737126, loss_bd:0.32131970\n",
      "epoch 18: loss 0.538691\n",
      "epoch 19 loss_pde:0.17285860, loss_bd:0.28209746\n",
      "epoch 19: loss 0.454956\n",
      "epoch 20 loss_pde:0.14169875, loss_bd:0.22973147\n",
      "epoch 20: loss 0.371430\n",
      "epoch 21 loss_pde:0.14023334, loss_bd:0.20301005\n",
      "epoch 21: loss 0.343243\n",
      "epoch 22 loss_pde:0.15818566, loss_bd:0.21214759\n",
      "epoch 22: loss 0.370333\n",
      "epoch 23 loss_pde:0.17668089, loss_bd:0.23652425\n",
      "epoch 23: loss 0.413205\n",
      "epoch 24 loss_pde:0.18368277, loss_bd:0.24810804\n",
      "epoch 24: loss 0.431791\n",
      "epoch 25 loss_pde:0.17610531, loss_bd:0.23644251\n",
      "epoch 25: loss 0.412548\n",
      "epoch 26 loss_pde:0.15889224, loss_bd:0.21279545\n",
      "epoch 26: loss 0.371688\n",
      "epoch 27 loss_pde:0.14437023, loss_bd:0.19598944\n",
      "epoch 27: loss 0.340360\n",
      "epoch 28 loss_pde:0.14277169, loss_bd:0.19629902\n",
      "epoch 28: loss 0.339071\n",
      "epoch 29 loss_pde:0.15111119, loss_bd:0.20921135\n",
      "epoch 29: loss 0.360323\n",
      "epoch 30 loss_pde:0.15633649, loss_bd:0.22138554\n",
      "epoch 30: loss 0.377722\n",
      "epoch 31 loss_pde:0.15048024, loss_bd:0.22247554\n",
      "epoch 31: loss 0.372956\n",
      "epoch 32 loss_pde:0.13908210, loss_bd:0.21270096\n",
      "epoch 32: loss 0.351783\n",
      "epoch 33 loss_pde:0.13324164, loss_bd:0.20075916\n",
      "epoch 33: loss 0.334001\n",
      "epoch 34 loss_pde:0.13683051, loss_bd:0.19524938\n",
      "epoch 34: loss 0.332080\n",
      "epoch 35 loss_pde:0.14402124, loss_bd:0.19749700\n",
      "epoch 35: loss 0.341518\n",
      "epoch 36 loss_pde:0.14734775, loss_bd:0.20177141\n",
      "epoch 36: loss 0.349119\n",
      "epoch 37 loss_pde:0.14510153, loss_bd:0.20191966\n",
      "epoch 37: loss 0.347021\n",
      "epoch 38 loss_pde:0.14062627, loss_bd:0.19734553\n",
      "epoch 38: loss 0.337972\n",
      "epoch 39 loss_pde:0.13703160, loss_bd:0.19260733\n",
      "epoch 39: loss 0.329639\n",
      "epoch 40 loss_pde:0.13537188, loss_bd:0.19205591\n",
      "epoch 40: loss 0.327428\n",
      "epoch 41 loss_pde:0.13579562, loss_bd:0.19558242\n",
      "epoch 41: loss 0.331378\n",
      "epoch 42 loss_pde:0.13687311, loss_bd:0.19928397\n",
      "epoch 42: loss 0.336157\n",
      "epoch 43 loss_pde:0.13594085, loss_bd:0.19961280\n",
      "epoch 43: loss 0.335554\n",
      "epoch 44 loss_pde:0.13262899, loss_bd:0.19659781\n",
      "epoch 44: loss 0.329227\n",
      "epoch 45 loss_pde:0.13027102, loss_bd:0.19316919\n",
      "epoch 45: loss 0.323440\n",
      "epoch 46 loss_pde:0.13161159, loss_bd:0.19179103\n",
      "epoch 46: loss 0.323403\n",
      "epoch 47 loss_pde:0.13497710, loss_bd:0.19218597\n",
      "epoch 47: loss 0.327163\n",
      "epoch 48 loss_pde:0.13680948, loss_bd:0.19234967\n",
      "epoch 48: loss 0.329159\n",
      "epoch 49 loss_pde:0.13596612, loss_bd:0.19118395\n",
      "epoch 49: loss 0.327150\n",
      "epoch 50 loss_pde:0.13392009, loss_bd:0.18957388\n",
      "epoch 50: loss 0.323494\n",
      "epoch 51 loss_pde:0.13233861, loss_bd:0.18911617\n",
      "epoch 51: loss 0.321455\n",
      "epoch 52 loss_pde:0.13184832, loss_bd:0.19033879\n",
      "epoch 52: loss 0.322187\n",
      "epoch 53 loss_pde:0.13205889, loss_bd:0.19221526\n",
      "epoch 53: loss 0.324274\n",
      "epoch 54 loss_pde:0.13187759, loss_bd:0.19314629\n",
      "epoch 54: loss 0.325024\n",
      "epoch 55 loss_pde:0.13085517, loss_bd:0.19239037\n",
      "epoch 55: loss 0.323246\n",
      "epoch 56 loss_pde:0.13012636, loss_bd:0.19063592\n",
      "epoch 56: loss 0.320762\n",
      "epoch 57 loss_pde:0.13092004, loss_bd:0.18918407\n",
      "epoch 57: loss 0.320104\n",
      "epoch 58 loss_pde:0.13270165, loss_bd:0.18865299\n",
      "epoch 58: loss 0.321355\n",
      "epoch 59 loss_pde:0.13383994, loss_bd:0.18862493\n",
      "epoch 59: loss 0.322465\n",
      "epoch 60 loss_pde:0.13355863, loss_bd:0.18848637\n",
      "epoch 60: loss 0.322045\n",
      "epoch 61 loss_pde:0.13238835, loss_bd:0.18827261\n",
      "epoch 61: loss 0.320661\n",
      "epoch 62 loss_pde:0.13123381, loss_bd:0.18848160\n",
      "epoch 62: loss 0.319715\n",
      "epoch 63 loss_pde:0.13064960, loss_bd:0.18926290\n",
      "epoch 63: loss 0.319912\n",
      "epoch 64 loss_pde:0.13062419, loss_bd:0.19009711\n",
      "epoch 64: loss 0.320721\n",
      "epoch 65 loss_pde:0.13068397, loss_bd:0.19032733\n",
      "epoch 65: loss 0.321011\n",
      "epoch 66 loss_pde:0.13049833, loss_bd:0.18983418\n",
      "epoch 66: loss 0.320333\n",
      "epoch 67 loss_pde:0.13039716, loss_bd:0.18906015\n",
      "epoch 67: loss 0.319457\n",
      "epoch 68 loss_pde:0.13087463, loss_bd:0.18845555\n",
      "epoch 68: loss 0.319330\n",
      "epoch 69 loss_pde:0.13174741, loss_bd:0.18808164\n",
      "epoch 69: loss 0.319829\n",
      "epoch 70 loss_pde:0.13231972, loss_bd:0.18778645\n",
      "epoch 70: loss 0.320106\n",
      "epoch 71 loss_pde:0.13220900, loss_bd:0.18758447\n",
      "epoch 71: loss 0.319793\n",
      "epoch 72 loss_pde:0.13160962, loss_bd:0.18768199\n",
      "epoch 72: loss 0.319292\n",
      "epoch 73 loss_pde:0.13096011, loss_bd:0.18815726\n",
      "epoch 73: loss 0.319117\n",
      "epoch 74 loss_pde:0.13057783, loss_bd:0.18876097\n",
      "epoch 74: loss 0.319339\n",
      "epoch 75 loss_pde:0.13046452, loss_bd:0.18909985\n",
      "epoch 75: loss 0.319564\n",
      "epoch 76 loss_pde:0.13044506, loss_bd:0.18898758\n",
      "epoch 76: loss 0.319433\n",
      "epoch 77 loss_pde:0.13051808, loss_bd:0.18857114\n",
      "epoch 77: loss 0.319089\n",
      "epoch 78 loss_pde:0.13082695, loss_bd:0.18813384\n",
      "epoch 78: loss 0.318961\n",
      "epoch 79 loss_pde:0.13128141, loss_bd:0.18783793\n",
      "epoch 79: loss 0.319119\n",
      "epoch 80 loss_pde:0.13155106, loss_bd:0.18768592\n",
      "epoch 80: loss 0.319237\n",
      "epoch 81 loss_pde:0.13146193, loss_bd:0.18765749\n",
      "epoch 81: loss 0.319119\n",
      "epoch 82 loss_pde:0.13114841, loss_bd:0.18777528\n",
      "epoch 82: loss 0.318924\n",
      "epoch 83 loss_pde:0.13084751, loss_bd:0.18802088\n",
      "epoch 83: loss 0.318868\n",
      "epoch 84 loss_pde:0.13069820, loss_bd:0.18826419\n",
      "epoch 84: loss 0.318962\n",
      "epoch 85 loss_pde:0.13068332, loss_bd:0.18834664\n",
      "epoch 85: loss 0.319030\n",
      "epoch 86 loss_pde:0.13071981, loss_bd:0.18822515\n",
      "epoch 86: loss 0.318945\n",
      "epoch 87 loss_pde:0.13080813, loss_bd:0.18799809\n",
      "epoch 87: loss 0.318806\n",
      "epoch 88 loss_pde:0.13099404, loss_bd:0.18779355\n",
      "epoch 88: loss 0.318788\n",
      "epoch 89 loss_pde:0.13120110, loss_bd:0.18766758\n",
      "epoch 89: loss 0.318869\n",
      "epoch 90 loss_pde:0.13127045, loss_bd:0.18761829\n",
      "epoch 90: loss 0.318889\n",
      "epoch 91 loss_pde:0.13116057, loss_bd:0.18764743\n",
      "epoch 91: loss 0.318808\n",
      "epoch 92 loss_pde:0.13097456, loss_bd:0.18775961\n",
      "epoch 92: loss 0.318734\n",
      "epoch 93 loss_pde:0.13082974, loss_bd:0.18791141\n",
      "epoch 93: loss 0.318741\n",
      "epoch 94 loss_pde:0.13077904, loss_bd:0.18800607\n",
      "epoch 94: loss 0.318785\n",
      "epoch 95 loss_pde:0.13081923, loss_bd:0.18796296\n",
      "epoch 95: loss 0.318782\n",
      "epoch 96 loss_pde:0.13093434, loss_bd:0.18778798\n",
      "epoch 96: loss 0.318722\n",
      "epoch 97 loss_pde:0.13111526, loss_bd:0.18756506\n",
      "epoch 97: loss 0.318680\n",
      "epoch 98 loss_pde:0.13131103, loss_bd:0.18738791\n",
      "epoch 98: loss 0.318699\n",
      "epoch 99 loss_pde:0.13141796, loss_bd:0.18730623\n",
      "epoch 99: loss 0.318724\n",
      "epoch 100 loss_pde:0.13137817, loss_bd:0.18732411\n",
      "epoch 100: loss 0.318702\n",
      "epoch 101 loss_pde:0.13124093, loss_bd:0.18741897\n",
      "epoch 101: loss 0.318660\n",
      "epoch 102 loss_pde:0.13109997, loss_bd:0.18754572\n",
      "epoch 102: loss 0.318646\n",
      "epoch 103 loss_pde:0.13101932, loss_bd:0.18764001\n",
      "epoch 103: loss 0.318659\n",
      "epoch 104 loss_pde:0.13101879, loss_bd:0.18764618\n",
      "epoch 104: loss 0.318665\n",
      "epoch 105 loss_pde:0.13108765, loss_bd:0.18755648\n",
      "epoch 105: loss 0.318644\n",
      "epoch 106 loss_pde:0.13120244, loss_bd:0.18741585\n",
      "epoch 106: loss 0.318618\n",
      "epoch 107 loss_pde:0.13132873, loss_bd:0.18728581\n",
      "epoch 107: loss 0.318615\n",
      "epoch 108 loss_pde:0.13141942, loss_bd:0.18720314\n",
      "epoch 108: loss 0.318623\n",
      "epoch 109 loss_pde:0.13144293, loss_bd:0.18717362\n",
      "epoch 109: loss 0.318617\n",
      "epoch 110 loss_pde:0.13140930, loss_bd:0.18718910\n",
      "epoch 110: loss 0.318598\n",
      "epoch 111 loss_pde:0.13135263, loss_bd:0.18723452\n",
      "epoch 111: loss 0.318587\n",
      "epoch 112 loss_pde:0.13130456, loss_bd:0.18728228\n",
      "epoch 112: loss 0.318587\n",
      "epoch 113 loss_pde:0.13128754, loss_bd:0.18729851\n",
      "epoch 113: loss 0.318586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 114 loss_pde:0.13131239, loss_bd:0.18726450\n",
      "epoch 114: loss 0.318577\n",
      "epoch 115 loss_pde:0.13137297, loss_bd:0.18719187\n",
      "epoch 115: loss 0.318565\n",
      "epoch 116 loss_pde:0.13144672, loss_bd:0.18711282\n",
      "epoch 116: loss 0.318560\n",
      "epoch 117 loss_pde:0.13150018, loss_bd:0.18705858\n",
      "epoch 117: loss 0.318559\n",
      "epoch 118 loss_pde:0.13151026, loss_bd:0.18704325\n",
      "epoch 118: loss 0.318554\n",
      "epoch 119 loss_pde:0.13148454, loss_bd:0.18705976\n",
      "epoch 119: loss 0.318544\n",
      "epoch 120 loss_pde:0.13145214, loss_bd:0.18708542\n",
      "epoch 120: loss 0.318538\n",
      "epoch 121 loss_pde:0.13143997, loss_bd:0.18709460\n",
      "epoch 121: loss 0.318535\n",
      "epoch 122 loss_pde:0.13145812, loss_bd:0.18707289\n",
      "epoch 122: loss 0.318531\n",
      "epoch 123 loss_pde:0.13149869, loss_bd:0.18702589\n",
      "epoch 123: loss 0.318525\n",
      "epoch 124 loss_pde:0.13154484, loss_bd:0.18697269\n",
      "epoch 124: loss 0.318518\n",
      "epoch 125 loss_pde:0.13158162, loss_bd:0.18693163\n",
      "epoch 125: loss 0.318513\n",
      "epoch 126 loss_pde:0.13160026, loss_bd:0.18690997\n",
      "epoch 126: loss 0.318510\n",
      "epoch 127 loss_pde:0.13159992, loss_bd:0.18690525\n",
      "epoch 127: loss 0.318505\n",
      "epoch 128 loss_pde:0.13158803, loss_bd:0.18691090\n",
      "epoch 128: loss 0.318499\n",
      "epoch 129 loss_pde:0.13157494, loss_bd:0.18691920\n",
      "epoch 129: loss 0.318494\n",
      "epoch 130 loss_pde:0.13157022, loss_bd:0.18692045\n",
      "epoch 130: loss 0.318491\n",
      "epoch 131 loss_pde:0.13158122, loss_bd:0.18690547\n",
      "epoch 131: loss 0.318487\n",
      "epoch 132 loss_pde:0.13160865, loss_bd:0.18687266\n",
      "epoch 132: loss 0.318481\n",
      "epoch 133 loss_pde:0.13164485, loss_bd:0.18683138\n",
      "epoch 133: loss 0.318476\n",
      "epoch 134 loss_pde:0.13167599, loss_bd:0.18679655\n",
      "epoch 134: loss 0.318473\n",
      "epoch 135 loss_pde:0.13169026, loss_bd:0.18677863\n",
      "epoch 135: loss 0.318469\n",
      "epoch 136 loss_pde:0.13168651, loss_bd:0.18677770\n",
      "epoch 136: loss 0.318464\n",
      "epoch 137 loss_pde:0.13167550, loss_bd:0.18678388\n",
      "epoch 137: loss 0.318459\n",
      "epoch 138 loss_pde:0.13167059, loss_bd:0.18678492\n",
      "epoch 138: loss 0.318456\n",
      "epoch 139 loss_pde:0.13167837, loss_bd:0.18677351\n",
      "epoch 139: loss 0.318452\n",
      "epoch 140 loss_pde:0.13169681, loss_bd:0.18675101\n",
      "epoch 140: loss 0.318448\n",
      "epoch 141 loss_pde:0.13171904, loss_bd:0.18672425\n",
      "epoch 141: loss 0.318443\n",
      "epoch 142 loss_pde:0.13173914, loss_bd:0.18670006\n",
      "epoch 142: loss 0.318439\n",
      "epoch 143 loss_pde:0.13175392, loss_bd:0.18668176\n",
      "epoch 143: loss 0.318436\n",
      "epoch 144 loss_pde:0.13176213, loss_bd:0.18666974\n",
      "epoch 144: loss 0.318432\n",
      "epoch 145 loss_pde:0.13176447, loss_bd:0.18666324\n",
      "epoch 145: loss 0.318428\n",
      "epoch 146 loss_pde:0.13176408, loss_bd:0.18665960\n",
      "epoch 146: loss 0.318424\n",
      "epoch 147 loss_pde:0.13176578, loss_bd:0.18665433\n",
      "epoch 147: loss 0.318420\n",
      "epoch 148 loss_pde:0.13177353, loss_bd:0.18664299\n",
      "epoch 148: loss 0.318417\n",
      "epoch 149 loss_pde:0.13178813, loss_bd:0.18662450\n",
      "epoch 149: loss 0.318413\n",
      "epoch 150 loss_pde:0.13180593, loss_bd:0.18660277\n",
      "epoch 150: loss 0.318409\n",
      "epoch 151 loss_pde:0.13182160, loss_bd:0.18658352\n",
      "epoch 151: loss 0.318405\n",
      "epoch 152 loss_pde:0.13183136, loss_bd:0.18657023\n",
      "epoch 152: loss 0.318402\n",
      "epoch 153 loss_pde:0.13183534, loss_bd:0.18656252\n",
      "epoch 153: loss 0.318398\n",
      "epoch 154 loss_pde:0.13183741, loss_bd:0.18655676\n",
      "epoch 154: loss 0.318394\n",
      "epoch 155 loss_pde:0.13184157, loss_bd:0.18654907\n",
      "epoch 155: loss 0.318391\n",
      "epoch 156 loss_pde:0.13184966, loss_bd:0.18653749\n",
      "epoch 156: loss 0.318387\n",
      "epoch 157 loss_pde:0.13186093, loss_bd:0.18652262\n",
      "epoch 157: loss 0.318384\n",
      "epoch 158 loss_pde:0.13187356, loss_bd:0.18650635\n",
      "epoch 158: loss 0.318380\n",
      "epoch 159 loss_pde:0.13188593, loss_bd:0.18649058\n",
      "epoch 159: loss 0.318377\n",
      "epoch 160 loss_pde:0.13189630, loss_bd:0.18647674\n",
      "epoch 160: loss 0.318373\n",
      "epoch 161 loss_pde:0.13190416, loss_bd:0.18646541\n",
      "epoch 161: loss 0.318370\n",
      "epoch 162 loss_pde:0.13190946, loss_bd:0.18645659\n",
      "epoch 162: loss 0.318366\n",
      "epoch 163 loss_pde:0.13191409, loss_bd:0.18644856\n",
      "epoch 163: loss 0.318363\n",
      "epoch 164 loss_pde:0.13192004, loss_bd:0.18643925\n",
      "epoch 164: loss 0.318359\n",
      "epoch 165 loss_pde:0.13192886, loss_bd:0.18642703\n",
      "epoch 165: loss 0.318356\n",
      "epoch 166 loss_pde:0.13194025, loss_bd:0.18641232\n",
      "epoch 166: loss 0.318353\n",
      "epoch 167 loss_pde:0.13195211, loss_bd:0.18639708\n",
      "epoch 167: loss 0.318349\n",
      "epoch 168 loss_pde:0.13196222, loss_bd:0.18638361\n",
      "epoch 168: loss 0.318346\n",
      "epoch 169 loss_pde:0.13196938, loss_bd:0.18637311\n",
      "epoch 169: loss 0.318343\n",
      "epoch 170 loss_pde:0.13197426, loss_bd:0.18636493\n",
      "epoch 170: loss 0.318339\n",
      "epoch 171 loss_pde:0.13197884, loss_bd:0.18635708\n",
      "epoch 171: loss 0.318336\n",
      "epoch 172 loss_pde:0.13198486, loss_bd:0.18634774\n",
      "epoch 172: loss 0.318333\n",
      "epoch 173 loss_pde:0.13199317, loss_bd:0.18633619\n",
      "epoch 173: loss 0.318329\n",
      "epoch 174 loss_pde:0.13200310, loss_bd:0.18632300\n",
      "epoch 174: loss 0.318326\n",
      "epoch 175 loss_pde:0.13201317, loss_bd:0.18630964\n",
      "epoch 175: loss 0.318323\n",
      "epoch 176 loss_pde:0.13202211, loss_bd:0.18629748\n",
      "epoch 176: loss 0.318320\n",
      "epoch 177 loss_pde:0.13202916, loss_bd:0.18628724\n",
      "epoch 177: loss 0.318316\n",
      "epoch 178 loss_pde:0.13203458, loss_bd:0.18627860\n",
      "epoch 178: loss 0.318313\n",
      "epoch 179 loss_pde:0.13203919, loss_bd:0.18627086\n",
      "epoch 179: loss 0.318310\n",
      "epoch 180 loss_pde:0.13204433, loss_bd:0.18626250\n",
      "epoch 180: loss 0.318307\n"
     ]
    }
   ],
   "source": [
    "lr = 0.0001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "#optimizer = torch.optim.LBFGS(model.parameters(),lr=lr,max_iter=5000)\n",
    "epochs = 20000\n",
    "tic = time.time()\n",
    "for epoch in range(1, epochs+1):\n",
    "    train(epoch)\n",
    "toc = time.time()\n",
    "print(f'Total training time: {toc - tic}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-50-516bfdcd13bd>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [50]\u001b[0;36m\u001b[0m\n\u001b[0;31m    x = np.linspace(0.0, 3.0, 401)                                   # Partitioned spatial axis\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on the whole computational domain\n",
    "#for i in range(50):\n",
    "#u = to_numpy(model(x_test))\n",
    "#x = to_numpy(x_test)\n",
    "    x = np.linspace(0.0, 3.0, 401)                                   # Partitioned spatial axis\n",
    "    y = np.linspace(0.0, 2.0, 400)                                   # Partitioned spatial axis\n",
    "    x_grid,y_grid = np.meshgrid(x,y)                                    # (t,x) in [0,0.2]x[a,b]\n",
    "    X = x_grid.flatten()[:, None]                                         # Vectorized x_grid\n",
    "    Y = y_grid.flatten()[:, None]                                         # Vectorized x_grid\n",
    "    x_test = np.hstack((X, Y))                                            # Vectorized whole domain\n",
    "    x_test = torch.tensor(x_test, dtype=torch.float32).to(device)\n",
    "    u = to_numpy(model(x_test))\n",
    "    #Xp,Yp = meshgrid(x,y)\n",
    "    \n",
    "    uxy= np.hstack((X, Y,u)) \n",
    "    \n",
    "    ue = np.zeros((401,400))\n",
    "    for i in range(0,401):\n",
    "        for j in range(0,400):\n",
    "            ue[i,j] = u[j*400+i,0]\n",
    "            \n",
    "    plt.figure()\n",
    "    plt.contourf(x_grid[:,:],y_grid[:,:],ue,50)\n",
    "    #plt.pcolor(x_grid[:,0,:],y_grid[:,0,:],ue)\n",
    "    #plt.colorbar(x_grid[:,0,:],y_grid[:,0,:],ue)\n",
    "    #plt.scatter(x_int_train[:,1],)\n",
    "    #plt.pcolor(x[:],u[:,1])\n",
    "    #plt.pcolor(x[:],u[:,2])\n",
    "    ax = plt.gca()\n",
    "    ax.set_aspect(1)\n",
    "    #plt.savefig(\"test_rasterization.png\", dpi=150)\n",
    "    plt.show()\n",
    " \n",
    "    #    uo = ue.flatten()[:,None]\n",
    "    #    uxy= np.hstack((X, Y,uo))                                            # Vectorized whole domain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0.0, 1.0, 400)                                   # Partitioned spatial axis\n",
    "y = np.linspace(0.0, 2.0, 400)                                   # Partitioned spatial axis\n",
    "x_grid,y_grid = np.meshgrid(x,y)                                    # (t,x) in [0,0.2]x[a,b]\n",
    "X = x_grid.flatten()[:, None]                                         # Vectorized x_grid\n",
    "Y = y_grid.flatten()[:, None]                                         # Vectorized x_grid\n",
    "x_test = np.hstack((X,Y))                                            # Vectorized whole domain\n",
    "x_test = torch.tensor(x_test, dtype=torch.float32).to(device)\n",
    "u = to_numpy(model(x_test))\n",
    "#Xp,Yp = meshgrid(x,y)\n",
    "\n",
    "\n",
    "x_test = np.hstack((X,Y))                                            # Vectorized whole domain\n",
    "ue = np.zeros((400,400))\n",
    "for j in range(0,400):\n",
    "    for i in range(0,400):\n",
    "        ue[i,j] = u[i*400+j,1]\n",
    "        x1 = x_test[i*400+j,0] \n",
    "        y1 = x_test[i*400+j,1] \n",
    "       # if ((x1-1.0)**2+(x2-1.0)**2<0.04):\n",
    "       #     ue[i,j] = 1.0\n",
    "          #  print(ue[i,j])\n",
    "uo = ue.flatten()[:,None]\n",
    "\n",
    "\n",
    "#for j in range(0,400):\n",
    "#    for i in range(0,400):\n",
    "#        x1 = x_grid[i,j] \n",
    "#        y1 = y_grid[i,j] \n",
    "#        if ((x1-1.0)**2+(x2-1.0)**2 < 0.04):\n",
    "#            ue[i,j] = 1.0\n",
    "#uxy= np.hstack((X, Y,uo))                                            # Vectorized whole domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('Stablerho.txt', uxy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQQAAAGiCAYAAAABTbj5AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAA9hAAAPYQGoP6dpAABT3UlEQVR4nO29e5QU5Z3//65LX4bLNBBkLskIGA1uDBeDMsHVr/p14sByiOwlApsEZNVk/Zls3Ikxko0QY85ijGvQyEouILo5EXVj8Gx0iToJ8jWiHLlsgjEeYUkAw4xKwgwzMN1dVc/vj+qqrqquqq6qrq6u7vm8zmmmu/rp6qe7q158Pp/nqSqOMcZAEAQBgK91BwiCiA8kBIIgdEgIBEHokBAIgtAhIRAEoUNCIAhCh4RAEIQOCYEgCB0SAkEQOiQEgiB0fAlh3bp1uPjiizF+/HhMmTIFS5YswZtvvln2dU8++STOP/98pNNpzJw5E88++6zpecYY1qxZg7a2NjQ1NaGrqwtvvfWWv09CEETF+BLCiy++iJtvvhmvvPIKnn/+eeTzeVx99dUYHh52fM3LL7+M5cuX4/rrr8e+ffuwZMkSLFmyBAcOHNDb3HPPPXjggQewceNGvPrqqxg7diy6u7sxMjIS/JMRBOEfVgHvvPMOA8BefPFFxzbXXnstW7RokWlZZ2cn+9znPscYY0xRFNba2sq+/e1v68+fPHmSpVIp9thjj1XSPYIgfCJWIpOBgQEAwKRJkxzb7Nq1Cz09PaZl3d3d2LZtGwDg8OHD6OvrQ1dXl/58JpNBZ2cndu3ahWXLlpWsM5vNIpvN6o8VRcGf/vQnvO997wPHcZV8JIKIJYwxnDp1Cu3t7eD56pX+AgtBURTccsst+Mu//Et85CMfcWzX19eHlpYW07KWlhb09fXpz2vLnNpYWbduHe68886gXSeIuuXo0aP4wAc+ULX1BxbCzTffjAMHDuCll14Ksz+eWL16tSnqGBgYwNlnn40rPnADRD4ZeX8IotpISg47jv0Q48ePr+r7BBLC5z//efzsZz/Dzp07y9qqtbUV/f39pmX9/f1obW3Vn9eWtbW1mdrMmTPHdp2pVAqpVKpkucgnIfKlywmiUah2SuwrGWGM4fOf/zx++tOf4he/+AWmT59e9jXz589Hb2+vadnzzz+P+fPnAwCmT5+O1tZWU5vBwUG8+uqrehuCIKLBV4Rw880348c//jGefvppjB8/Xs/xM5kMmpqaAAArVqzA+9//fqxbtw4A8MUvfhGXX345/u3f/g2LFi3C1q1b8dprr+H73/8+ANV4t9xyC775zW/ivPPOw/Tp03HHHXegvb0dS5YsCfGjEgRRDl9CeOihhwAAV1xxhWn5ww8/jOuuuw4AcOTIEVMV9JJLLsGPf/xjfO1rX8NXv/pVnHfeedi2bZupEHnbbbdheHgYn/3sZ3Hy5Elceuml2L59O9LpdMCPRRBEEDjG6v8kq4ODg8hkMug6+/+jGgLRkEhKFi8c+XcMDAygubm5au9DxzIQBKFDQiAIQoeEQBCEDgmBIAgdEgJBEDokBIIgdEgIBEHokBAIgtAhIRAEoUNCIAhCh4RAEIQOCYEgCB0SAkEQOo0lBElWbwRBBKKxhKBBYiCIQDSmEDRIDAThi8YWggaJgSA8UdGFWuoOoxREoXb9IIiYMjoiBDsoaiCIEkavEDRIDAShM7pSBjconSAIihBsoaiBGKVQhOAGRQ3EKIMiBK9Q1ECMAkgIfiExEA0MpQxBoXSCaEAoQggDihqIBoEihDChqIGoc0gI1YLkQNQhlDJEAaUURJ1AEUKUUNRAxByKEGoFRQ1EDKEIodZQ1EDECBJCnCA5EDWGUoa4QikFUQN8C2Hnzp1YvHgx2tvbwXEctm3b5tr+uuuuA8dxJbcLLrhAb/P1r3+95Pnzzz/f94dpSDQxkByICPAthOHhYcyePRsbNmzw1P7+++/H8ePH9dvRo0cxadIkfPKTnzS1u+CCC0ztXnrpJb9da3xIDkSV8V1DWLhwIRYuXOi5fSaTQSaT0R9v27YNf/7zn7Fq1SpzR0QRra2tfrszeqF6A1EFIq8hbNq0CV1dXZg6dapp+VtvvYX29nacc845+NSnPoUjR444riObzWJwcNB0G9VQ5ECERKRC+OMf/4j//u//xg033GBa3tnZiS1btmD79u146KGHcPjwYVx22WU4deqU7XrWrVunRx6ZTAYdHR1RdL8+IDEQFRCpEB555BFMmDABS5YsMS1fuHAhPvnJT2LWrFno7u7Gs88+i5MnT+KJJ56wXc/q1asxMDCg344ePRpB7+sMihqIAEQ2D4Exhs2bN+Mzn/kMksmka9sJEybgQx/6EA4ePGj7fCqVQiqVqkY3GxOqNxAeiSxCePHFF3Hw4EFcf/31ZdsODQ3h0KFDaGtri6BnowyKHAgXfAthaGgI+/fvx/79+wEAhw8fxv79+/Ui4OrVq7FixYqS123atAmdnZ34yEc+UvLcrbfeihdffBG///3v8fLLL+Ov//qvIQgCli9f7rd7hB9IDoQF3ynDa6+9hiuvvFJ/3NPTAwBYuXIltmzZguPHj5eMEAwMDOAnP/kJ7r//ftt1Hjt2DMuXL8eJEydw1lln4dJLL8Urr7yCs846y2/3iKBoUqCUYlTDMcZYrTtRKYODg8hkMuhq/xxEnmoLoUFyiA2SksULR/4dAwMDaG5urtr70MFNhDNUjBx1kBAIb5AcRgV0tCPhHypENiwUIRDBoaih4aAIgQgHihoaAooQiHChqKGuoQiBqB4UNdQdFCEQ1YeihrqBIgQiWihqiDUkBKI2kBhiCQmBqC0khlhBQiDiAYkhFpAQiHhBYqgpJAQinpAUagIJgYgvFC1EDgmBiD8khcggIRD1AUULkUBCIOoLkkJVISEQ9QdJoWqQEIj6hKRQFUgIRP1CUggdEgJR35AUQoWEQNQ/JIXQICEQjQFJIRRICARB6DSWECQZkKRa94KoFRQlVExjnkLNKgWxMT8mQYTN6NhTjIIgOTQ2kkznbayA0bd3kBwIwpHRvUeQHBoTihICQ3uBBsmBIEgItpAciFEKbe3lIDkQowjawv1AciAaHNqqg0JyIBoQ3zMVd+7cicWLF6O9vR0cx2Hbtm2u7Xfs2AGO40pufX19pnYbNmzAtGnTkE6n0dnZid27d/vtWu2QpOKNIOoY30IYHh7G7NmzsWHDBl+ve/PNN3H8+HH9NmXKFP25xx9/HD09PVi7di327t2L2bNno7u7G++8847f7tUekgNRx/iOdRcuXIiFCxf6fqMpU6ZgwoQJts/dd999uPHGG7Fq1SoAwMaNG/HMM89g8+bNuP32232/V2ygtIKoMyI7uGnOnDloa2vDxz/+cfzqV7/Sl+dyOezZswddXV3FTvE8urq6sGvXLtt1ZbNZDA4Omm6xhyIHog6ouhDa2tqwceNG/OQnP8FPfvITdHR04IorrsDevXsBAO+99x5kWUZLS4vpdS0tLSV1Bo1169Yhk8not46Ojmp/jHAhMRAxpepx7IwZMzBjxgz98SWXXIJDhw7hO9/5Dv7jP/4j0DpXr16Nnp4e/fHg4GD9SQGglIKIHTXZCufNm4eXXnoJADB58mQIgoD+/n5Tm/7+frS2ttq+PpVKIZVKVb2fkUJyCA86jiEwNTlByv79+9HW1gYASCaTmDt3Lnp7e/XnFUVBb28v5s+fX4vu1R6qNxA1wvd/RUNDQzh48KD++PDhw9i/fz8mTZqEs88+G6tXr8bbb7+NRx99FACwfv16TJ8+HRdccAFGRkbwwx/+EL/4xS/w3HPP6evo6enBypUrcdFFF2HevHlYv349hoeH9VEHz0gykPT7iWKOJgWKGrxB0UFF+N7KXnvtNVx55ZX6Yy2XX7lyJbZs2YLjx4/jyJEj+vO5XA5f+tKX8Pbbb2PMmDGYNWsWXnjhBdM6li5dinfffRdr1qxBX18f5syZg+3bt5cUGj1hPY1Wo2wglFKMbiL6zTnGGIvknarI4OAgMpkMuiatgsiXCREaRRAAicGORvp9Af03lpQsXvjfBzAwMIDm5ubqvV3V1hxXGimCoKjBTD3/lho1/h1pK2oUQVCtoX6J0W8Wn57EhXoXxGgVQ739TjH9feLZqzhhFEQ9bXSUTsSLOvkN6qOXcaFeo4dGjxri+juE8X1rn02O5jM26BYSEfUmiEYUQxy/80q+X5vPw0QejItmDmEDbRkxoF4E0YhiqDVBv0uHbYSJBgEIApiYCLZ+v92J5F1GK3GvP9R7naHW32nVJcCrN4GHJHDB3stv1yJ5l4hgsgSmmEMrLi4betyjB4oavBHk+/EoAOMyJSWCiTzklABpDI8zKUoZQoHZHCAUC0nENXqoFzFE/Z35/T4CSICJvCoCgYM0VkS+iUduPIdchsOZVDQHusX8V68OsZNEHKOHehFDNQlBAiYBAK4SkFMC5BQHaQyP3DgeuWYglwFyExXIiWygj+CXUfxrm4mVJOIUPcRRDNX8Tvx8Ti9RAGCSgC4CgYeSEqCIPJQUj3wTD2kMh9w4DvmxQL4ZyGcUsOY8xjSPYDxO4migD+SPGP3K8cMqiZoIIi7RQxzFECZeP5ePVEBbbpSAVhdgghoJyCkO+SYOeYMIpHFFEYxL53DWmCGkckOBP5ofGvTXrQ6xiCJqHT3UWgxhfuYIJWBMCeSkKgKpiUN+HJAfC0hjiyJIjcshM2YEmfQZTEqdwfjECJL8qcAf0w8khAqpqSRqGT3UQgxhfT4vfQ4gAe2vUQJaSiAnuYIMCiIYC0hNqgjyGRkYKyE1Loex6RyaUyOYmD6DcWIW4xMjaBZHkFKiObN4QwmB5fNgXHG8lktEM5mjpB+1SjVqET1IUv2kEQFFEEQCakRQjAbkJCA1cZDTQH4cII1lkMYpwFgJiSYJ48eOoDk1grGJHMYlchifGMF4cQQZ8QxSfB5JYaTij++FOvklg8HyedvlUYuiJoKIUg5RRAuVfIZy/Qo4OsAE4301JWAiZ4oG1IgAkNKA3GQQQZOMxNg8mtI5jE9nMSF1xiSCFC8hI55GmpOQ4vNgnP22HDYNLQQnai2KyNOMqOQQt2jBrS8BJQDAMRooRgVqWiAnVRFI4wA5XSqCdEJCJq2KQEsPVBGoUUFGOIM0l0eaz0Phz4TznZQhRr9e7amlKCKLIqoth2pEC377WYkIfEiAiRwUAaa0QLFEBEYRCGkZqXQO49I5jEnkStIDa1SgySDN5aDwFCHEBjtRVFsSkQiimnIIK1rw0y8fIvBaFyjed44G1BugJM2pgdxkFkE6KZnqBEle0ouGaT5fkECpDNJ8HjKlDPEm6mii6oLQ5BCmGCqVgte+OL1HlaIBJsIkAjmtjhjIaUBuMotATEoYm86hKZHHmIIIxolZpASpbFSQLkQFaS4PmaOpy/6RZKA2Aws6UUUTVRNE2FFDNesKHkVQSTRgTQuYAL1QqCTsRcAlFCSb8kglJIxPZzEmkUOSl/X0IMlLnqKCNFcUgkRCCIiXqx1FXPiKQhJVEURYUUMQKZR7T7v1eRCBUzSg3i9NCxQ9NVDrA3LSXgQsqYBPy7oI0glJrxMkBbkkKtBkkBHUYqGbDNKcBImzzDmpEo0nBC+4SSMiWVRbEkZBVCyHaqQTbri9T8gisIsGNCEoglkEUlqtE2giUJIMSpMMPi0jkZSRSEhIJyVTepDkZT0qMI4g2EUFAHQZFIUgIc3JiGbi8mgVghvlIowqCqNakghNDpWIwWuU4LRuPyIImBb4FQGXUCAmFb1gKPKKp6jArwwAIMUp5b+7ECAh+MVJGFUShVUSlQoiFDlUK2LwKgOPIjBGA/qyMoVCRSgOHcqJogiUJIPcxMCSCpCWISYViEnJlB4kBNk2KignA2Px0E4GaU5BNGMMJITwiEgUYUYRFctBkkM8vsBmPT5F4LU+4DZioCRdRJBUClGBOT1I8HJJVGCXIgAIJAP1L51CrTGIQBRGSUQuhzCk4FMGXiKCMESgpBiUBNNHDsSkDF5QHKOCJC+5pghAafFQX2aQgYZRBnlEcwlWEkKtsBNFCJIII8XQ5OBZDF6k4GOugKmtnQgAT8cW+BWBUhhBkMcoUBJqVMAlFAiF9EDgFduowEuKAHiXQZqTSyKDFJ2G3T9KXoLiM7TiEzH6CqogiUqiByZJ4UrBirW9Q1QQlQiUpFow1NID3pAeiIKCdEJCQpD1eQV2KYIXGWikDbMPjTKwkuJ45EBFxUhQ8t4nfNREHlZJVCCIIHLwHS3Y4WGEoGxUEIII1ElFhSggaUkPkgxMcI4KtBGERCEi0KYeB5GBdZ6BhlvdIMXRlZtiRzl5RCKMkAShycGPGEKbDelBBk5Rgd/pxdqogVUEekRQqBOwpAJOYHpUoNUKREHRUwS7eoGTDDQ0GeiPjfdt6gbF56JNFTRICCHiJIyqiqLCi634EUMgKZQZKXCVgUNUoIkAgH4OArd5BEoCBTHYpwdMYHpUwAtKSYrgRwYaKUMUAMAxVbCSjmi+gRMkhAiwE0VVJFGBHFg+X9kcB7/Dhh5SBL9HHhpFoAnAWCdgvGX0QGB6rYAXWEmKoMnAWDy0k4ERY6oAwDE6MLZX/zpPTU5zAtUQGp2qSyLAeQkqloIbZWRgFxV4OSGJot+HqWDIeHN6wAT7WgEnMD1FsJNBQpBtZWDEGh0A1tpADkGIOl0ASAixwiiJ0OTgUwyBpOBj6LCcDMqdp1A7RZkicI4FQ2N6YIwKGA9TiuBJBrz9/9zGVEHDrhbghFvaUEt8K2jnzp1YvHgx2tvbwXEctm3b5tr+qaeewsc//nGcddZZaG5uxvz58/Hzn//c1ObrX/86OI4z3c4//3y/XYvXEGKFKHnJ1whIWbwcBRoW5WQgCoBYvJgpBMF0cVMlJRZuQuHU5YJ65uIUh3xT4RTmY9TrGeinKtPOUpTUzlRUKCI2mQuHMI0ilJdBuegAQEntoBqkuGiO6/cthOHhYcyePRsbNmzw1H7nzp34+Mc/jmeffRZ79uzBlVdeicWLF2Pfvn2mdhdccAGOHz+u31566SW/XQOgSsF6q2dCF0MF2BYUPQ4fmmSA0qhASYvOpzDXzlNouLCJ8byFctowy7DJXDRUkmYZgGd68VCTgSio+bmdDOyw1g2CMMJqfOIOB3zvLQsXLsTChQs9t1+/fr3p8b/+67/i6aefxn/913/hwgsvLHZEFNHa2uppndlsFtls8Vp3g4Pu56y3SiEuO5gflLwUidyc0oXAMrAsqzQ90IYRtbTAWCtgglYvYHrNQK8XCAzgmTq0aJGBKCgQefuinRYdWAlDCgAwwkRPqUaWRZNiRF61UBQFp06dwqRJk0zL33rrLbS3t+Occ87Bpz71KRw5csRxHevWrUMmk9FvHR0dvvpQj1FErGUgiiXpgL7csCxoeuAWFUhNpTJQUsxFBsxWBsbowI6U4E0AWaX0OxxhSfWvzXPFNqV1mCyLfggyciHce++9GBoawrXXXqsv6+zsxJYtW7B9+3Y89NBDOHz4MC677DKcOmV/+arVq1djYGBAvx09erTiftlJotayCLUPZYqKFcnAulwThE2twGt6UDyTsWVOgWWCkSYCowwYD1cZlMMtXdDIKqLhvp0E3Hb+hGObERb9yIKRSLf2H//4x7jzzjvx9NNPY8qUKfpyYwoya9YsdHZ2YurUqXjiiSdw/fXXl6wnlUohlUpF0meg/P/OYaUgVZGPh9EFzzJwShHK1QoCpAfanAImFEcQmKBGBUzQhhFR2PkL6YFYlAGEwtGBFhl4TRWCYBf+jygJpPk8RlgSaS6nP7Z73QgT9PkII4zpsxVHmAygwQ5/3rp1K2644QY8+eST6Orqcm07YcIEfOhDH8LBgwf9vYkoANGces5ELFOOMEUAeIsKDI/19ADFOQXlTltmnFNgrBXoUYBRBIK7DJigisALWrrgRlYW9bQhq4h6DUHbwbNKQh9h0O6PsATSXL5ECgCKz+l/izIZYbw+YzHLlEjnI0TyTo899hhWrVqFxx57DIsWLSrbfmhoCIcOHUJbW5v/N9PDVcNtNOHxM/uKCqypgHG5h/RASYne0oPCyUzlJFeSIpjmFniVgUt0EAbGtEFjhImF58xpwYj+OGm4X5o6GGsJIyyacyAY8b23DA0Nmf7nPnz4MPbv349Jkybh7LPPxurVq/H222/j0UcfBaCmCStXrsT999+Pzs5O9PX1AQCampqQyWQAALfeeisWL16MqVOn4o9//CPWrl0LQRCwfPnyMD6j8w4S5dh8NfEoPbcJR67pgdeIADAVDAHvJzO1iwq0HV9LF4zL/MjA9isrI4WcLOh1hJwi6iMN2n23KEH7394tUoCCwv1ipADANkqIEt8RwmuvvYYLL7xQHzLs6enBhRdeiDVr1gAAjh8/bhoh+P73vw9JknDzzTejra1Nv33xi1/U2xw7dgzLly/HjBkzcO211+J973sfXnnlFZx11lmVfj537KKJeogqfPaVSyRsZcCJon4rrruCiCCVMKUHTqMHdkVDLSowpghywr8M7NCig5Kv0Wf9ICur30XOVFBU72v/6xcjA/+RgvY664hDlinIsmhyYY6xGsQlITM4OIhMJoP/O2YZRC4Z3RtXO8KoQEy+ogGg4ohAW+alTqAVDbWogOkFxOJOb4oSjMuT6hwDJqqbrZfowC5dMBYUjTUE43RlLULQHxdmK2r3geJ8BO2vVjAsngtBsjzOm9rZnT3Jek7FNMchNwRMP/84BgYG0NzcbPezhkLM/yv0B5dIgLOZ4ul02bWKiVEkUe74g4onFvkQAQBP6QEz3TfLQEmWRgXahCMNNxloOEUHbuQUwfEYBq24aE0dtL/G9EHtQOGFCsqmD4A5WnA7ArJaxGeLriJuO0vVZFFlvByA5HjuAg+HIfsVAQBfUYFdrcApKjDKwDUyMGCdcyAY0gOvRUWtjqAJQpeAixSMWOsK+hHMPExSAAqpBF960JM6L0HBKYUOf46EcjtWHITh5+hDTxIAPE0zBsIXgVtUULEMbAqJRimEMbpQTgoAbKMFY7ERKA47qveLw5Hac+pf9fdIcxKGSAj+4QQRHK9+JOu1DgOv0++JST0KJKzzDngWABAoGtAeez3FeSUiUNfpLANtEp+bDDScogP9IxvqB3boUYElSjBiJwUAjimE3TwFU7qgJDCChD76oJHm8hhW6NqOFeF2qq+wZGH7vtU6wQg8nOg0xPMSaI/thg8B+xqBkwjU13uPCrT2QWRgTBXsooNKogRr6gCUSsGIVQoAbIckoQAjhcuWp/k8RuRiYTzN5TCCBE7LJISqUW7HqqYwvOD5vIUhnLbM69WPvIpAfU3lUYG+3Di0CPiWgV3tIIzpynYCsEsfjJjqCYBeU8iiGD2MyNp8hMJjPWIgIfhHFBDGqedCO7twmPi5IrKHYw78nt4cgKMIAG/pgdrOe1QAmOsFahv/MnCKCpzSBSvGSUp2GKc168tsRh9KXmcSQbFeUPpcAoyd8dTXSonhll8hbjuOVIMDHYIQ5CpITpGA4bmg10JU2/gXgboe+/TA+JxTVKA+Nk860oYWtfvGuQbGv1YZWKMDowzsjmMw1guMMnAbklSfN6QTDqMPrhinCho8prBootbGE4IbXq4sFIU0/FzhqJJRAxgkAHiKBgBzWgDAV2qgLbeKwLTca1QA2KYIQGUyMGKUgdeIQcOulqAtB2AqNAIokYM1cjDNXwAscojmDEujSwheCPsS577e2+XnCPEaiAA8pQXa46AiMC0zisFmolHZqAAoSRHUv4priuA1MjAud4sArFilAKBEDOo6g8tBrTtEs102lhBEAeBdPlIcDmbyUp9wkpKXKyIDniSgPnZOCwCUjBqoy8IRgd7GRgTa/TCiAsCfDOyw1hCsaYMxIgCKYgAql4MmhlxExzk1lhDK0WDFQjcJGO9bawOAe1qgvrZyEeiPeePj8ukBUForAHyMItikCEFl4ESu8IHdxACURg127YzzF6yPNTFwiGaCXAz3kAbESxri4VgDkwCAshLQHhsloD7nnBYA5kKh3t6lWKg/9iACtU/2Q4lA+fTA+LeSqMANu8JhTi4IwCIQv2IASqMGNzEYD7GOgsYSgigAvMdcq9LiYdBag886gV0UYF1eLiXQ7lujAfW+c33A/Bj640pFoC33MpSo/vUvAiCYDDScRhOchiDdxKAud44a3MRQTCUoQqgu1S4eBplVCOcowPqcmwQA2NYGtOftogH1fjEaKD421wDsRg30+wFFwAGB6gTG+04iANxlkJeFstOXS5Y7RAvaa4DSwqSXdMJODNY0oto0lhBEUS0qVrN4GKQO4XXnB0wCsLbxIgHtsZ9oQH+NTX0AcBaBtVioPhcfEXgliBSAcMVgrTHYSSEKGksIGrUoHpaJOGx3fsCTAIDKJaDft0QD6mvt04JiO4eowEexUGtTDREA7jLIK0LZlKGcFADn4cigYigXLZhTCEoZ4oPP9MLrzm/X1q8E1OdLUwJ1uXs0ADinBdpr7I43KD42Dx9qr3cqFmoi8FsjMN4PehxCpVIAwhdDuWjBei7HKGgsIRSuChQFjju9sS8eXmcnAONyrxIAwosG9OU2bYKIAEDFxUIgnIOSylFOCoCH6ctlxOAlWrCTQhQ0lhBCouzObsXjzm+3zK8EgOApgXbfazSgtfFUHwA8zSNQ//oTARCODLxECYB3KQDusxrdhirLSaFWNJQQjHPzQ6dM5OEkES8CUJcHl4Da3j0l0Np4iQaA8AqFgPcJRdb7fvCyEwP+pAB4mMXoUQx+pVB6bATVEKIhQIrhFkG4CcD6vF8JqM85S6B0mbdoQF/mMy1Qn2eB6gOATUoQ0gVUyuFVCkB4YvAqBSNmKUSTCjeWEEKsIXhJG2xTAg8CACqTgLWdVQLa39CjAaC0UOgzLQCC7fiSwpdNG7xGCYA/KfhZd7lhynJSqHXq0FhCKIPv2oCH17kJQH3eHAUYl1UqgeIy6MuMEtCXBYwG1GX+0wL1cakIosCvFADvsxj9RAt+pBAnGkoIQWsInqIBh/U6CQCITgJAaUqg/Q01GgBc0wLrfb9IMu8aPXiJEoJQjWihXDrg1taplhAFjSUEgQscBRTX4f56NwEApamA+ppwJKA9Nu7cxfYwtbdr5zcaALyNFmh4lUG5Hb9S/EQJ+msilEKco4SGEoITYUYNTgIwPmcUgHF5FBLQ2ztIQH1/79GA9hgoiqAeiLsU/LTLyiI45GyfC5uGEgITvKcM3tIErmSZnQDU9QWTgNbWqwRKnnOQgPo3QDQAlIggSsJMG+IiBTvKRQm1Ki42lhAM1xHw9TqbHR8w7/x2baOWQEkbh7qA6TmHaMD42CqCoCgyX5I2yAofeWGxUsKWgp96Qq1pLCEInOPOXY5yOz9QFABQ/XTA2NZJAuY25gKhuY29CIDaRAGVUu0oAYguUjASB3E0lBAUkbfdsb1gJxI3AajL7GcMau0rlYBTW7dowNzOLALTMqIsfqXgFz/FRTofQgAqiRCMOz9Q3EG19arLilGA6XEVJGD6WyYl0PtpcGHcd/xqjzTUgqDnVXCiFnWExhOC6F8I1lmh1igAqI0EzMuN/8sX/tapDKKmknC+2lFC3GgoIchJDlyiuDNzcvkdo2QY0VQ0NC+rtgSMrzEts5EBER2jSQqNJYRUUQicBMCys/MFQSi29YLifbeRAfvllr8eRVByv0oHahKEV3xvgjt37sTixYvR3t4OjuOwbdu2sq/ZsWMHPvrRjyKVSuHcc8/Fli1bStps2LAB06ZNQzqdRmdnJ3bv3u23a5DSHORk4ZYy35gI033tpi1ThOJrmQDISXWHVh9zUEqWqzuzov1NFHL9whRiu1OPEY2PdrxDUKI6qtEJ30IYHh7G7NmzsWHDBk/tDx8+jEWLFuHKK6/E/v37ccstt+CGG27Az3/+c73N448/jp6eHqxduxZ79+7F7Nmz0d3djXfeecdX35hhp5aT0G+ssGPb3bSd3CgAJwkYd3K71AAorUcQjUG+Sj9srkKBhA3HGAtcgeI4Dj/96U+xZMkSxzZf+cpX8Mwzz+DAgQP6smXLluHkyZPYvn07AKCzsxMXX3wxHnzwQQCAoijo6OjAF77wBdx+++0l68xms8hms/rjwcFBdHR0YNY//CtEIe3cX5eagmnCkeE3UpzCe4dQ31N7D+vxUkPwXFR0GXY0zUPgXZ5D6dRlu2MX7JbZTUxyG2HwMvoQ5CCnSucJeK0jOL2P0yiDdejRdOVp7ZyKwzk8cdV/YGBgAM3NzZ76EYSqZ627du1CV1eXaVl3dzd27doFAMjlctizZ4+pDc/z6Orq0ttYWbduHTKZjH7r6OgAAHABf++gQ5UE4YdapwNeqLoQ+vr60NLSYlrW0tKCwcFBnDlzBu+99x5kWbZt09fXZ7vO1atXY2BgQL8dPXoUgBoBuN2c8DIaQRC1IKqzLWvU5ShDKpVCKpUqWc7L6s0vimCVQnE0QVuf2qaxioNhT1v2mi4Q8aXqQmhtbUV/f79pWX9/P5qbm9HU1ARBECAIgm2b1tZW3+9nlzbwZSIAzlo/kJlBAOpzWihl3byZAHBKMYfnZSosEvbU+jgFL1Q9ZZg/fz56e3tNy55//nnMnz8fAJBMJjF37lxTG0VR0Nvbq7fxCp9n4GUGIWu+cRJKljk+l9NuKNzUx3yOgZPVZXzhLycDfOEvp6g3oBipcHJRUEZRBa11EES18R0hDA0N4eDBg/rjw4cPY//+/Zg0aRLOPvtsrF69Gm+//TYeffRRAMA//uM/4sEHH8Rtt92Gf/iHf8AvfvELPPHEE3jmmWf0dfT09GDlypW46KKLMG/ePKxfvx7Dw8NYtWqVr74JWQZBMRzJ56E2IMjGCjwHaDuwPj1Ye8zpFXptpqKgpRBycUjSmFa4RhVyaXtjpBEaNI05FCodYagXfAvhtddew5VXXqk/7unpAQCsXLkSW7ZswfHjx3HkyBH9+enTp+OZZ57BP//zP+P+++/HBz7wAfzwhz9Ed3e33mbp0qV49913sWbNGvT19WHOnDnYvn17SaGxHJzMIDiMonJS6fKS4x4KqQKgikIThCJwECTmSQ6Kzc4uaDu6bP+8W21Cfb74fo1WxwhCFFdwioq4nUqtonkIcWFwcBCZTAZ/2XUnRLE4DyHIsQwAPJ/3QHvsdIxDuVOjW5+z/evxkGdzW8PntkQIrudC4J2fszt9mpdzKjoVFZ3mGjTqHATAvoZgJwRruyQvRTYPoS5HGZzgZAZB9vfDKzZlFE6LDgAIUkEQhejBKXJQ9CihWJDkNFlYIgM/UYP+t9BejRYAQNtBNUExS5vC8zLnLW3webakSs6uXG+MlnQBaDAhCFkZnOh9w2YCB16y37DtROEmB2PNoSgC5qneECSl0MTAFSSkP4YWNWiRCvMuBaKEsI5yDDrCQOdDqABOUsCVlPDsYSLvmFKUiEIqnmJNix7KySHseoP63u5RgyYHgNPFwClcYX1qPzj9MzIwmSumBgpX0TkV/UQMUZ8YJej/3H5kEFZ0UOuhycYSgqyA45w3NuMZmTmHyMBOFFZBaNGDqxxkLnBKYZWDAv8phWPUoIlBixq0xwIzScEki4DU86SkMM9/4PUYhjjQYEJg4KA4nnmZk8vLwioKO0Hw8CYHbYdSdy5tfcWJUJocAHc5cEHqDQ4phWPUoNUdNBHw5ghCkbm6ui6DRpD/uf3KoBFqBxoNJQQNp//9rRjFYScLJvBlBWGUg5ZaOMmhJKUwRQnl5QD4lINdSqE9D4McXKIGZogatE/KC8z2lOuNQNgyqKfoAGgwIXCSAo45pwJ27d3aWSVhJwjrZE9dELZyKKy3kL+r9631BrgWI4PIASiVA2CVA8xRQ0EOxqhB06DhSo7qv4LiWRBR1g/8/s8dds3Abz3AbrgxahpKCG64RQ0mCbhIwq8gKpeD+8zIIHIoV29QP29RDtCu8GQTNch6SmGRYmGn93uRlmrNQfBClDKIa3QANJoQZBn63GM7BPspfmUjBYfn3WoS6uuK6xFkWU0ZIpIDoEYAdnIALHKQOcMym3qDW9SgcJALKZEiC47phCaGqE6/7ic6iIMMykUHqYiihcYSQjncJi1ZZOE2CuH0fHlJhC+H4jI4FiQBZzm4FiMdUgp9ToNBDlCYa9Sgf828AklWn6v1dRmiThG8yqCWNJQQ/NYQTHiUhWtKYVuALC7jZMUwmqE+X7kc4Kkg6SYHwCIHl5TCOErBAMdCZLmowU4M5aIHr+mCl5037NmHYcqgJDoQpIiu/dxgQnDDy8iDozScZFEQhddRjZI+FQRhlAOgpgSAevHUSuWgrs/7UKZtMdKYUrgVImG+b1dr0MRgl1aIQuXRQ1gyqLYInF5bi0KikVEjBC/4lkaZqMKPKFyjB5TKwTgBCnCXg9skKKBUDq7FSLeoocwIRbl0QiMMMTgRlgzCFoG6vFQGKYGmLgdHLiS+lSDaFx41vI5W2MrCRhKcpJhSDuOIhjV6MMpB/9/fIgf7Q7XhKa3QZ0cWum6VgymlKFNrMKYTDLAVg5ZOOGEVg6QU7jukDhUf0RhwToGpTdB5CTGQAdBoQggDycdGZZFHWVk4RBTGA7DtBOFHDsbZkUHkUDJ12kYagHOtAfpxFKVDl1YxAOqchrDF4IRbdFCpDIJEBOpz3kQQVeGRhFAJXuQhlq8zWHcFTpZN9QkmFuc7sMKOoacXhkO1vcpBMQ1blk6CsqYUdsdUWCMHtS92YigeQ1ENMRil4HZR16AyqGQ+gV8RAKUy0K/L4PpO4dFYQpAkgHcP+auOaPlKy0lDtK81mM4B7SAIY/SgtnOXg93UaS8HXTkVIq1HYXoRg1sqAfgTg1204PdKz34vqgKEGw1o2EcFlDLUP5LHH1ETh50wbCRRepJ4ZzkY0wptKJMTOPeRCpMI3KMGJhTKgoHEUL7GAGjTo72dYNIqBu36iuXEUOkVlsq1Lz7vLRpwap/iJYAmJoWIn7pAtbAWK72Ko4DpRG+m4qOsFyt1ORjSCu1ALCZwriMVisCBB9OjBnWH12oNBjHAvgjpRwywzmUofCzrfeMxE8ahSm1KtB4lOKQRxmjB6yXd/cjAT4FQw6sEgOhmJxppLCFIcrArtUSBj3qDF1kYBaGfTM1BDlpB0i6lMB2N6eMcDkYxAN4iBoAzjUrAeo4G42cx3PcSLVjTCC9S8JpeuF170by8skhAb19DOTSWEILg83/qUPFabxAF135ykmyQiZY+mOXgmFJYoga7WoNiihCYoxicUgm9n/pp5rXd3aa+UGhbSbQQRqSg4UUGfoYMg0ogW/JsdWh8IdRyhy9Hub451RnKCaLw1xg5cJLgGjVYaw3GdMKuzuBFDNZlZkFw+slbtDTCLlrQzuSk3VdkDmFKwW8R0oj3IUPv/+PbLU/zeXB8PkAP/dNYQpDkcIovtag52E2IstvpRdGzIIyRg1rEc44a9HRCMqcTCphtncFNDIB5VMJYHtUFYRMtQCwmDMzwDCz3rSlEECkEwa146DRcaGrjQwxpiwCSJIQqE4dCoxG3/hhlYd3x7QRhgy4HQ0phFzVY0wk/YjAVHw1ddksjCr2zrS3YphCFU7spKJ65CfAuBQ0vqYP78KLzfzzG57xKwCoAtV1x2Yjju4VL4wshpB2fRZR6cNa6AuA4NKk+518Qpp3MGjXYpRMuBUgezFBjKI5KWCVgTCNMfZHdogWuJELwKwUrdlGCl8jBGB3YHY1o91zKRQzlBKC34wqvi+iCoI0lhApGGaLa4cvh1I8SUdilDYBZEGXkUBI1AKZ0Qm1jFgMTOPAwRwzG4qNxuJIVhi+Nu6V1mLI0YiiVAopLfElB/Zqco4QwcJKBqY1huVUEVgnoArC0UWiUoTqEveOzfDi5HZdIuL+PXY3AKAnjjh9EDoCjGKx1BqdUQheDLoLSNMI4TOm8e6q7vvFoSmtdwU4KAEyHVVtTBz8Y0wWn6MBNBuWGCY0iMErAPkrIg9mIoho0tBDC2PnD2uEreR8nWVg/H2c3KuFjbgNgEYOhzgCgrBjUnbw4j0E7u5MxjTDulnZphV5wNF2yjiupK9hJwTj6AJReF0KLErQUwfcQpAcZeEkT7GRgXpYveZ1CKYN/mCyBKcGupx7Vjh8Eu77ZScIoCE9ycIkarHUGb2IozmNQ0wdWkkboRcgytQVzT8pLAYBpSBJQT/KqfuxgUQJgP7LgVwbGNMGrDNKmZTkSQrWoyo4fZhpiV1S0wfo5rILwJAc3MUgSIIp6nSGQGFzSCLtooRIp6HMWAFM9Qf3YRRkErSVoO36UMkhzOf2xQoc/V05oO3+UBUen9yojCuNndZKDbc3BaZKTZbmbGNTn7UYlSsVgTCOs0QJnKTgqSWOHvEnBWk8A/J/xWasfeD1yUROAHxGULi+NDIykuGgi2IYSAsvnwTiufEM3YjLaUILd8KIDTnJwjRochzG9iaEoBx48FP0Sd9qIhFZfMKYR5aIFPmeuKyg+pGBMHdSPaZaBn0lK1uggiAzKicD4Oi0ysK4rChpKCL6J687vBesIggOaHMpGDX7FoK0HFjFo8xgqiBaUpH0KwUMtNipJ71LQMBYYTbMXfRYWjbjJIGhUYEwTjO2GA/XQP6NHCPW885fDWAtwIBIxAKUTnMqIQU6VRgvImVMIOWnuAp8zTJP2IYXiR/BfQ7BGB35lECQqMLWjYccKiUgASj7c9+ETFfwkHqKGisRQblTCbh6DixiQBJC1jxaMEhBy9nUFPsepNYeUsxRgc+Yl7fwJgJdzKXr7ff3KwC4qMC43yyC6CCHQGN2GDRswbdo0pNNpdHZ2Yvfu3Y5tr7jiCnAcV3JbtGiR3ua6664reX7BggX+OybJ6oZbBRkoecn2Vs33qYgy3wHL522LriVzNyS5KAHjd6st15YVHnOSAi6bB+TCfUkBn5UKNxlC4cZnFQg5BiGnQMgy9ZZjEHJqcVLIqSLgcwCfVyegcnLhcWEZJwOczIHPcuBznHoGJln9y2QOUDgosiohReYhK7xJBhp5RdDPsuQFa3TgRQZpLl82RbDKwPiaVFwjhMcffxw9PT3YuHEjOjs7sX79enR3d+PNN9/ElClTSto/9dRTyOWKBjxx4gRmz56NT37yk6Z2CxYswMMPP6w/TqVSfrsWKtXY2YP2IXDU4DGV8Dwq4XMeg/FYCX25YagSgG0aAZRGCzxKUwge5mIjn+OgJC0RAs85pg4a1ighJwsOZ0jyGC3YyEB/rkzh0CgD4/qGPL1z5fje0u677z7ceOONWLVqFQBg48aNeOaZZ7B582bcfvvtJe0nTZpkerx161aMGTOmRAipVAqtra1+uxMqcZCAHUpeqjyV8CkFQBWD41Cldb36cRHehyoFwLa+APCmNEIVRLHgqKUQilEMhWIjnyuczwGaFNQKhZ0UTKMOhsMzjcONOUV0FYE1OggqA2uKYFyXej+aeQi+UoZcLoc9e/agq6uruAKeR1dXF3bt2uVpHZs2bcKyZcswduxY0/IdO3ZgypQpmDFjBm666SacOHHCcR3ZbBaDg4OmW1CqGf6HRUUy0KgghbBNI4zrtaYRxveTiqmDlkbwI5JzGpFlSJyxphEMfCGd0NIFIW9IH3Jq9MDnCqmDxIHPG9KHPK8Kw5A+qF3jISm8Y7qQUwTL4+C/Qz3IAPAZIbz33nuQZRktLS2m5S0tLfjd735X9vW7d+/GgQMHsGnTJtPyBQsW4G/+5m8wffp0HDp0CF/96lexcOFC7Nq1C4LNJdzXrVuHO++800/XTcR557cSigxCoNJoAfBZeJQ4yCm1FsBJ6miENVoAihOZAOgjD3yucMAT1MgBycJMCIUVYgUeeYgA3LeDctddsOIUHdjNJfArgxTnf2QkCJFubZs2bcLMmTMxb9480/Jly5bp92fOnIlZs2bhgx/8IHbs2IGrrrqqZD2rV69GT0+P/nhwcBAdHR1l379eRFBLCTilD4CNFIDytQVNDIZl1jMhATCf2k0/GaygpxG8rAqCK9QQjBey1a9qrYlBUdesj0DkePV070lVBm5SsJuToEUGlV5azW6OQfG52kYGGr62vMmTJ0MQBPT395uW9/f3l83/h4eHsXXrVnzjG98o+z7nnHMOJk+ejIMHD9oKIZVK+S46xlkGkQjA4zESQIVSANyjhYIY7E7SomEsPMopoVhfkBnkJA9ek4IMKEnOFC0ABjEYrlqtJAHk+LJSyCulEal1CnNWEUM5C7JdAbH4nGy4ryCq+Yq+agjJZBJz585Fb2+vvkxRFPT29mL+/Pmur33yySeRzWbx6U9/uuz7HDt2DCdOnEBbW5uf7jkSFxnwCdH2VnV8yCAwdgdHGZ+zqS041Rc4SQEnq8ud6gviaYbEGQbxjFpbEM+otQVxRK0pCIXHwgin1xb4PAcux6tiyPOQc2o9IZ8X1XqCzONMPqEPQ+YUATntryIiK4sldYQRxf08FnYHKqmPXc6OZFoWTaqg4XtL6enpwcqVK3HRRRdh3rx5WL9+PYaHh/VRhxUrVuD9738/1q1bZ3rdpk2bsGTJErzvfe8zLR8aGsKdd96Jv/3bv0VraysOHTqE2267Deeeey66u7sr+GhFrDtdmIKIS45vS0AReDlZi+Op3pwiBePz1tSicKUqYxpRrr7ARA5ykiscVFVIHWSuGDkktAiisE6Z0+8zWZ3ExAQGGeqRmAAgKwoACWeQAGy+AuNoQ1ZLIwzLRpjoe0ZhueiguKzCY3Q84nuLWbp0Kd59912sWbMGfX19mDNnDrZv364XGo8cOQKeNwceb775Jl566SU899xzJesTBAG//vWv8cgjj+DkyZNob2/H1VdfjbvuuqtqcxFivRNXSoXRQDkZlMVOCkBpCqE9Z6ktAN7qC8bCo7G+oMqBgywDXFKd0CQXxMApXEEMDEzhoSQKl45TGCQAYhLIFlIIOynk+OJ3ax2KzCoJ27MdjSgJTwcoxSE6AACOMeZ8Vc06YXBwEJlMBv83+UmIXIUbdD0SUkrgRwa2EYKG3SnlgdJ+GttpzxmW6XMWhOL8BSYWzw6tXU9CSfH6ORzllHryVzlZrC8oSUBOqH+ZoEYPSpJBSTH1cnJJBUgq4BIKeIFBTEoQeAXppASRVzAmkUNCkJHkZYxL5JDkJaQESf1buKX5vGkuQoo3z0x0ShnciomaENIch1OnFJz/F/0YGBhAc3Oz83dfIQ38X2WDUoV6QMVRgRVrlKAvd0ghjM8ZRie0tAFysQip99lQeOQlRReDFkHwsholKIJ6JiVOBhS5eNFaY7SgoDhPgSUUACKUwqSldFLC6XwSYwoXZB/KJzHO5esynTqdJfQdfoQlTXMQnKjFyIIREkKcqXIxMKgIXKMDv5QZtrRLI3RRaMsKJ2exioGT1BmPcuEYCCVZiBwKaYR2LISWZiiFYyBkhYOS0OoKPFIJtV8JRQYSubJS0I7d1uRgTBu8phC1goQQF6IYCUDl0YBnGXiNEuza2gxbcpL6P7Zt4dFyynheUiCnBPCyokYEBjFwMgdWmO2o5NThSyXPFYuQCl+Y0cghL3MQksWzLaWTqhjyhWHIpM2kOR39LC+wjRK06KH4Vy1IjjChplECCSFKItrpjYSVDviOCpzqCICzFIyvc5jkZEwjAEvh0eZaElYxCDmmjk4UUgpOMUQLhTSCSzJztCBzEJNyoZs80oWIIafIhb8ixidsrq1kkAIU1EWUQEKoBjXY8TXCrgeEmh4YcTrgyk+0YFNfcBKDIqoTmqxikJMc+CSg5AE5pxYelTwHJcepKUZTMVpQZB5yslj8SxSEMC6Rw6l82lYKaUFCVklYIgb7KCEOkBCCUsOd3kjoBUFtvZV8PrfowAtuQ5eWaAEoHaYE7MWg1Rg0MQhZdVTCTQyczENJcVASPKS8oh8gZUohZAFjE+pOnuQlwCKGFJ83RQrGiMFceDSnDSOMR5pTMBLhQGA8tuo4EpMd3ki1dn59/WF8Zj8ycDss264GYY0WCu8XVAxMUFMENzHwOQ5yujAakSpEC3keUkJQZzimjSmEgHEFMeQESZ02qWGRQUkaEZMoIX5bfZTEcKfXqPbOD4ScDlQaFdjhJAUgsBis15Lg5FIxaPMYpCYOSt4wQpHmoGQ5yE3qMRFaGpFNShhJiBifzqpTnhNFMWR5qbiXOUhBKzyOMPU3r2WUEN89IgxivMMbiWLn198r7O+kEhF46YufOQ2F/jiJwRQtOIhByKlRgpDlIGijEEkOUpMqBrlJPS5CbuKg5HhITbJ6TERTMY3IK9rxDwLGiVkAQJYXkRERSApRUh97jFdEAeDi/ZGi3PmBKhYFK40I/PTLTQrWdbmIATAPV7pNiWaiJgQecsoQJYwAUlo9aEpOc5BzHOQmBbk8DyldTCPysqBHC8ZRiBSfAMTTvqSg0oDnQxhtRL3zA1UUAFCdtCAMysxtsI5KADbpREEMfFY2TYvmZehiYKI6MqFGEYCUK4jhjAC5ST16Mt/EQ8oJyDblVTEYooWsXBRDls8jxUT1PHIepCAxEkLdUIsdX3/vaqdFYUsgaH+dogT9eZdoAbAdrgTK1xm0AqSQZZBTWiqhphXiGTWVUM4AklEMTQqyeR5STsRIOoczyQTyqWK0kFNEjBdH1GgBQJZLADjtMVKoLiQEnzT0zq9RrUggkvMyuMxvAExiAAx1hsJJWtzSCWudQRE0KXAQRgA5bRWDgNNNArJpGSNpEePSOeTSZ2yjBQAYYXmkuQTSTD0Yyjx5qfxxEGFAQnCglju+3ocoi6LVTAfC+hzlogS9ncvp5y0zIr2mEwDAG46y5LPqMRNCDhCyvDpseUZNIZQkIKU5SGMNYhirnohlJC/idDqLoUTSFC0AcEwhAECmlCE6Rt3OD0RTD6j1iEYAMQDeowZjOsHOFOYxFP5KTRykEVUMcpMA6QyPfJOI/FgRZ5qSGD92BHlZwFAiiWxKRFYR0SyOIKskkBHOYITlMcEgB5lFM9owqoQQhx1fI3IBANEVBeM23FtuAhRg+m7cogbjmZyA0qhByKnnZhBP85DGaHIApCEO0riCGMYJ+NMZEcPjkhibzmE4n8TE9BmcEtOYnBoCUIgWAD2F0E4dX21i9suFQ5x2fKBGOz8Q/ahA3ERgpNxVrCwFSMA+agCchy75rAyW5SGnhMI1JTgwgUN+DK+OSgxzyI/lIA+KkMYKyI4IyI1NYLgpidP5JDLpM8gpIk4l0hgvjiArqtECcBoKRQj+4RIJcDE4Y1LNBAA0rgTC+lxlrmKltnGOGgDnlAIAmMzrj4WsOnQp5NTL0kljeCSH1HQiP5ZDfrgghnEJvNucwNDYJAbSTThrzJCeRoyICYwwEUklpudUJMzUdOcHajc3oJ4Lnh6ueam2K40aAIeUwiqHrPo8L/Bg2cLoRFYAEznkm3gkhjhIgxzy4zjkxwrID/E4PS6BM815DI0kcXJsE6aMGcIpMY1sSkQ6osMcSAg+qPnOD9R2clCj1T28RAt62zIphRc5FCY98VlBP/+jNMQj38QhP1gQQ7OA05kEzjSncKo5hUljTmNISmF8nmoINSUWOz9Q+9mBjZ7+eI0WTK9xlwPgklZkAT6r1hxEkYd4WkAyxUM+qdYa8uM4ZJtVMQxOSuBUcxP+1HwGzYyEEAmx2fE1ai0AjVp/L1F/D36iBdPr3NMKwJsclJSAxCke0ikeqQE1asidEJCdKODUxCQGUlRDCJ3Y7fxAfAQA1F4CGrX6ToJEC6bXB5cDn5XARB7isCoHOSVAOskj9yceuXE8To9JB+uTT2KyBYRLLHd8IF47v0bcvqs4fEeVigHwJAf9Stha7UFS2/FZHoLIQzwtInFKQFOKRyJNMxV9wwkiOD5GHykOG7cdcZMAEM/vKmgaUbIeyxwCD9EDJwngRyQIhbNKcxVeedorMdwy6pQ4btBG4igBjTh/d8aL1oZ5TIYRu8KkIXrgJAGinA3nvcsQ460kxsR5A9aIswCM1MN3qRFGKmG73jKCkBT1PPERUCdbTQ2ppw22XiQA1Nf3aqUaUYNp/aU7PxfRhV/raAuKgHrbSOtJAEbq7Xt2Q7Lk9tX6TWSKEKpLPW6U9SoAI/X4vfshKkFUifrqbVDqdSOss43JlXr9DSrFKgiNmP628exVJdTzhhfTjaRi6vk3qRZOorAS8TbRWFtgvW14jSoAjXr7PeKIJg6F5iE0Ho0uACMkg7pkFG2hNWI0SUCDZFC3BDqmcsOGDZg2bRrS6TQ6Ozuxe/dux7ZbtmwBx3GmWzptPlCDMYY1a9agra0NTU1N6OrqwltvvRWka7VHFM230QbJoK7xLYTHH38cPT09WLt2Lfbu3YvZs2eju7sb77zzjuNrmpubcfz4cf32hz/8wfT8PffcgwceeAAbN27Eq6++irFjx6K7uxsjIyMOa4wRo10ARkgGdY9vIdx333248cYbsWrVKnz4wx/Gxo0bMWbMGGzevNnxNRzHobW1Vb+1tLTozzHGsH79enzta1/DNddcg1mzZuHRRx/FH//4R2zbti3Qh6oqJAB7SAYNgS8h5HI57NmzB11dXcUV8Dy6urqwa9cux9cNDQ1h6tSp6OjowDXXXIPXX39df+7w4cPo6+szrTOTyaCzs9NxndlsFoODg6ZbVSEBEKMEX0J47733IMuy6X94AGhpaUFfX5/ta2bMmIHNmzfj6aefxo9+9CMoioJLLrkEx44dAwD9dX7WuW7dOmQyGf3W0dHh52OUh6IAf1B00DBU/URt8+fPx4oVKzBnzhxcfvnleOqpp3DWWWfhe9/7XuB1rl69GgMDA/rt6NGjlXeUBEAQ/oYdJ0+eDEEQ0N/fb1re39+P1tZWT+tIJBK48MILcfDgQQDQX9ff34+2tjbTOufMmWO7jlQqhVQq5afr9tDOTxAmfEUIyWQSc+fORW9vr75MURT09vZi/vz5ntYhyzJ+85vf6Dv/9OnT0draalrn4OAgXn31Vc/r9AVFAgThiO+9oqenBytXrsRFF12EefPmYf369RgeHsaqVasAACtWrMD73/9+rFu3DgDwjW98Ax/72Mdw7rnn4uTJk/j2t7+NP/zhD7jhhhsAqCMQt9xyC775zW/ivPPOw/Tp03HHHXegvb0dS5YsCelT0s5PEF7wvacsXboU7777LtasWYO+vj7MmTMH27dv14uCR44cAc8XA48///nPuPHGG9HX14eJEydi7ty5ePnll/HhD39Yb3PbbbdheHgYn/3sZ3Hy5Elceuml2L59e8kEJn+fjCRAEH7hGGOsfLN4Mzg4iEwmg64pN0BMjql1d0YfNMpQdSQlixeO/DsGBgbQ3NxctfeJ5nIwUUEbZm2wOeUXUZ80lhAIgqgIEgIRDhQlNAQkBIIgdEgIRHhQlFD3kBCIcCEp1DUkBIIgdEgIRPhQlFC3kBAIgtAhIRAEoUNCIAhCh4RAEIQOCYEgCB0SAhE+dJBZ3UJCIAhCh4RAhAtFB3UNCYEgCB0SAhEeFB3UPSQEIhxIBg0BCYGoHJJBw0BCICqDZNBQkBCI4JAMGg4SAhEMkkG0RHSdERIC4R+SQbREeNEhurwR4Q+SQXTU4OpjJATCOySDaKjhZQhJCER5SATREIPrkda+B0S8IRlUnxiIQCM+PSHiB8mgusRIBBrx6xFRe0gE1SWGItCIb8+I2kAyqB4xFoFG/HtIRAOJoHrUgQg06qenRPUgGVSHOhKBRv31mAgPEkF1qEMRaNRvz4nKIBmETx2LQCPQsQwbNmzAtGnTkE6n0dnZid27dzu2/cEPfoDLLrsMEydOxMSJE9HV1VXS/rrrrgPHcabbggULgnSNKIcokAzCRhQbQgZAACE8/vjj6Onpwdq1a7F3717Mnj0b3d3deOedd2zb79ixA8uXL8cvf/lL7Nq1Cx0dHbj66qvx9ttvm9otWLAAx48f12+PPfZYsE9EOEMiCJcGEoEGxxhjfl7Q2dmJiy++GA8++CAAQFEUdHR04Atf+AJuv/32sq+XZRkTJ07Egw8+iBUrVgBQI4STJ09i27Zt/j8BgMHBQWQyGXS1fw4inwq0joaGRBAeNRKApGTxwv8+gIGBATQ3N1ftfXxFCLlcDnv27EFXV1dxBTyPrq4u7Nq1y9M6Tp8+jXw+j0mTJpmW79ixA1OmTMGMGTNw00034cSJE47ryGazGBwcNN0IB0gG4dCA0YAdvoTw3nvvQZZltLS0mJa3tLSgr6/P0zq+8pWvoL293SSVBQsW4NFHH0Vvby++9a1v4cUXX8TChQshy7LtOtatW4dMJqPfOjo6/HyM0QHVCsJhlIhAI9JPevfdd2Pr1q3YsWMH0um0vnzZsmX6/ZkzZ2LWrFn44Ac/iB07duCqq64qWc/q1avR09OjPx4cHCQpaJAEKmcUCcCKrwhh8uTJEAQB/f39puX9/f1obW11fe29996Lu+++G8899xxmzZrl2vacc87B5MmTcfDgQdvnU6kUmpubTTcCJINKiWs0IAqAEM1v60sIyWQSc+fORW9vr75MURT09vZi/vz5jq+75557cNddd2H79u246KKLyr7PsWPHcOLECbS1tfnp3uiF0oPgaBKIqwgi/l19Dzv29PTgBz/4AR555BG88cYbuOmmmzA8PIxVq1YBAFasWIHVq1fr7b/1rW/hjjvuwObNmzFt2jT09fWhr68PQ0NDAIChoSF8+ctfxiuvvILf//736O3txTXXXINzzz0X3d3dIX3MBoVEEJy4SgCo6e/q+xtZunQp3n33XaxZswZ9fX2YM2cOtm/frhcajxw5Ap4veuahhx5CLpfD3/3d35nWs3btWnz961+HIAj49a9/jUceeQQnT55Ee3s7rr76atx1111IpWgI0RESQTDiLIEY4HseQhwZVfMQYrLh1BVxlQDg+feU5CxeeOs7VZ+HEONvijBBIvBPXEUQ498ypt8YYSLGG1DsiKsEgLr4HWP87RH1sAHFhriKoM5+w5h+i6OcOtuIakZcJQDU7W8Y4290FFKnG1HkxFUEDfD7xfSbHWU0wIZUdUgCkRDTb3kU0WAbVOiQCCIlpt/2KKBBN6hQIAmYYCIPxkVzofaYfvMNDInAGRKBDhOjEYCVmP4CDQiJwB6SgIlaiUAjpr9GA0EisIdEoFNrCRiJ6a/SAJAISiEJ6MRJAkZi+gvVMSSCUkgEOnEVgUZMf6k6hERghiSgE3cJGInpr1ZHkAjMxFEEJAHPxPDXqxNIBGZIBADqVwQaMfwVYw6JoAhJAED9S8BIDH/RmEIiKEIiiE4C+tmWo/l8MfxlYwaJQIUkAKAWIoiWGP7KMYFEoEIiaHgJGInhr11jSAQkgQKjSQQaMfzlawSJgESA0SkBIzHcAiKGRBA/ETRqNBBTCRiJ2ZYQIaNdBHGTAEDRQAyI4VZRZUgEte6BGYoGYkXMto4qMdolAIx6EVA04I2YbSUhM9pFMMolADRONBCV0GK2xYQEiaDWPTBD0UAgTJ9Drupb6cRsy6kQUQD4USwDEkH136SBogE7YrYFEb4Z5RIASARhErOtifDMKBcBSaA6xGyrIsoSJxFQNBCYuIlAI0ZbF+FInCQANGY0AIxqEWjEbEsjTMRJBBQNBCbuEjASoy2O0BnFIqBooLbEaMsb5cRJAgCJoALqUQQagXq+YcMGTJs2Del0Gp2dndi9e7dr+yeffBLnn38+0uk0Zs6ciWeffdb0PGMMa9asQVtbG5qamtDV1YW33norSNfqD1GMjwxEoXiLCCby1d+BBKF4qxLa56hnGQABhPD444+jp6cHa9euxd69ezF79mx0d3fjnXfesW3/8ssvY/ny5bj++uuxb98+LFmyBEuWLMGBAwf0Nvfccw8eeOABbNy4Ea+++irGjh2L7u5ujIyMBP9kcSeOIoiIyHaeKksAiEZoTOTBBK6q76HBMcaYnxd0dnbi4osvxoMPPggAUBQFHR0d+MIXvoDbb7+9pP3SpUsxPDyMn/3sZ/qyj33sY5gzZw42btwIxhja29vxpS99CbfeeisAYGBgAC0tLdiyZQuWLVtWss5sNotsNqs/HhgYwNlnn40rPnADRD7p5+NES1wEANTkIBxKC3y+h0ECkpzF/9t/H06ePIlMJlPFN/VBNptlgiCwn/70p6blK1asYJ/4xCdsX9PR0cG+853vmJatWbOGzZo1izHG2KFDhxgAtm/fPlOb//N//g/7p3/6J9t1rl27lgGgG91G3e3QoUN+dlnf+Pov67333oMsy2hpaTEtb2lpwe9+9zvb1/T19dm27+vr05/Xljm1sbJ69Wr09PToj0+ePImpU6fiyJEj1bVnFRgcHERHRweOHj2K5ubmWnfHM9TvaNGi4EmTJlX1fWIUw3onlUohlUqVLM9kMnX1Ixtpbm6uy75Tv6OF56ubqvha++TJkyEIAvr7+03L+/v70draavua1tZW1/baXz/rJAiiOvgSQjKZxNy5c9Hb26svUxQFvb29mD9/vu1r5s+fb2oPAM8//7zefvr06WhtbTW1GRwcxKuvvuq4ToIgqoTfosPWrVtZKpViW7ZsYb/97W/ZZz/7WTZhwgTW19fHGGPsM5/5DLv99tv19r/61a+YKIrs3nvvZW+88QZbu3YtSyQS7De/+Y3e5u6772YTJkxgTz/9NPv1r3/NrrnmGjZ9+nR25swZT30aGRlha9euZSMjI34/Ts2p175Tv6Mlqn77FgJjjH33u99lZ599Nksmk2zevHnslVde0Z+7/PLL2cqVK03tn3jiCfahD32IJZNJdsEFF7BnnnnG9LyiKOyOO+5gLS0tLJVKsauuuoq9+eabQbpGEEQF+J6HQBBE41Lf8ywJgggVEgJBEDokBIIgdEgIBEHoxFYI9XqItZ9+/+AHP8Bll12GiRMnYuLEiejq6ippf91114HjONNtwYIFNe33li1bSvqUTqdNbaL6vv32/YorrijpO8dxWLRokd6m2t/5zp07sXjxYrS3t4PjOGzbtq3sa3bs2IGPfvSjSKVSOPfcc7Fly5aSNn73GVtqO8hhz9atW1kymWSbN29mr7/+OrvxxhvZhAkTWH9/v237X/3qV0wQBHbPPfew3/72t+xrX/ua7VyHTCbDtm3bxv7nf/6HfeITn/A116Ea/f77v/97tmHDBrZv3z72xhtvsOuuu45lMhl27Ngxvc3KlSvZggUL2PHjx/Xbn/70p9D6HKTfDz/8MGtubjb1SZuHohHF9x2k7ydOnDD1+8CBA0wQBPbwww/rbar9nT/77LPsX/7lX9hTTz3FAJQcLGjlf//3f9mYMWNYT08P++1vf8u++93vMkEQ2Pbt2/U2fr8HJ2IphHnz5rGbb75ZfyzLMmtvb2fr1q2zbX/ttdeyRYsWmZZ1dnayz33uc4wxdZ5Da2sr+/a3v60/f/LkSZZKpdhjjz1Ws35bkSSJjR8/nj3yyCP6spUrV7JrrrkmtD7a4bffDz/8MMtkMo7ri+r7Zqzy7/w73/kOGz9+PBsaGtKXRfGda3gRwm233cYuuOAC07KlS5ey7u5u/XGl34NG7FKGXC6HPXv2oKurS1/G8zy6urqwa9cu29fs2rXL1B4Auru79faHDx9GX1+fqU0mk0FnZ6fjOqPot5XTp08jn8+XHNG2Y8cOTJkyBTNmzMBNN92EEydOhNLnSvo9NDSEqVOnoqOjA9dccw1ef/11/bkovu9K+m5k06ZNWLZsGcaOHWtaXs3v3C/ltu8wvgf9dZV3N1zcDrF2Ohy6GodYR9FvK1/5ylfQ3t5u+mEXLFiARx99FL29vfjWt76FF198EQsXLoQsh3OxvyD9njFjBjZv3oynn34aP/rRj6AoCi655BIcO3YMQDTfd9C+G9m9ezcOHDiAG264wbS82t+5X5y278HBQZw5cyaUbU+jLg9/bkTuvvtubN26FTt27DAV6IxnjJo5cyZmzZqFD37wg9ixYweuuuqqWnQV8+fPNx14dskll+Av/uIv8L3vfQ933XVXTfoUhE2bNmHmzJmYN2+eaXkcv/OoiF2EUK+HWAfpt8a9996Lu+++G8899xxmzZrl2vacc87B5MmTcfDgwYr7DFTWb41EIoELL7xQ71NUh7RX0vfh4WFs3boV119/fdn3Cfs794vT9t3c3IympqZQfkON2AmhXg+xDtJvQD3B7F133YXt27fjoosuKvs+x44dw4kTJ9DW1lbTfhuRZRm/+c1v9D5FdUh7JX1/8sknkc1m8elPf7rs+4T9nful3PYdxm+o46sEGRFxPMS6Gv2+++67WTKZZP/5n/9pGuI6deoUY4yxU6dOsVtvvZXt2rWLHT58mL3wwgvsox/9KDvvvPNCPQzWb7/vvPNO9vOf/5wdOnSI7dmzhy1btoyl02n2+uuvmz5btb/vIH3XuPTSS9nSpUtLlkfxnZ86dYrt27eP7du3jwFg9913H9u3bx/7wx/+wBhj7Pbbb2ef+cxn9PbasOOXv/xl9sYbb7ANGzbYDju6fQ9eiaUQGKvfQ6z99Hvq1Km2J9Jcu3YtY4yx06dPs6uvvpqdddZZLJFIsKlTp7Ibb7zR948cdr9vueUWvW1LSwv7q7/6K7Z3717T+qI8pN3vtvK73/2OAWDPPfdcybqi+M5/+ctf2v7uWj9XrlzJLr/88pLXzJkzhyWTSXbOOeeY5k1ouH0PXqHDnwmC0IldDYEgiNpBQiAIQoeEQBCEDgmBIAgdEgJBEDokBIIgdEgIBEHokBAIgtAhIRAEoUNCIAhCh4RAEITO/w8vw843j1PUwgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.contourf(x_grid[:,:],y_grid[:,:],ue,100)\n",
    "#plt.pcolor(x_grid[:,:],y_grid[:,:],ue)\n",
    "#plt.scatter(x_test[:,0],x_test[:,1],u[:,0],10)\n",
    "#plt.pcolor(x_grid[:,0,:],y_grid[:,0,:],ue)\n",
    "#plt.colorbar(x_grid[:,0,:],y_grid[:,0,:],ue)\n",
    "#plt.scatter(x_int_train[:,1],)\n",
    "#plt.pcolor(x[:],u[:,1])\n",
    "#plt.pcolor(x[:],u[:,2])\n",
    "ax = plt.gca()\n",
    "ax.set_aspect(1)\n",
    "plt.show()\n",
    "plt.savefig(\"Riemann2.pdf\", dpi=150)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n",
    "#surf = ax.plot_surface(x_grid[:,0,:], y_grid[:,0,:], ue, cmap=cm.coolwarm,\n",
    "#                       linewidth=0, antialiased=False)\n",
    "#\n",
    "## Customize the z axis.\n",
    "#ax.set_zlim(0, 1.01)\n",
    "#ax.zaxis.set_major_locator(LinearLocator(10))\n",
    "## A StrMethodFormatter is used automatically\n",
    "#ax.zaxis.set_major_formatter('{x:.02f}')\n",
    "#\n",
    "## Add a color bar which maps values to colors.\n",
    "#fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model,'2DRiemanncase8.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device1 = torch.device(\"cpu\")\n",
    "#model.load_state_dict(torch.load(PATH))\n",
    "#model = model.to(device1)\n",
    "#torch.save(model,'2DRiemanncase8.pt')\n",
    "model = torch.load('2DRiemanncase8.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
