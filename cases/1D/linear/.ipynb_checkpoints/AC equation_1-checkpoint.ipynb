{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Burgers equation\n",
    "\n",
    "This code is modified from the code https://github.com/alexpapados/Physics-Informed-Deep-Learning-Solid-and-Fluid-Mechanics\n",
    "\n",
    "The paper about this work can ref https://www.researchgate.net/publication/359480166_Discontinuity_Computing_with_Physics-Informed_Neural_Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import time\n",
    "import scipy.io\n",
    "torch.manual_seed(123)\n",
    "np.random.seed(123)\n",
    "from smt.sampling_methods import LHS\n",
    "#torch.set_default_tensor_type(torch.Tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    model.it = epoch\n",
    "    def closure():\n",
    "        optimizer.zero_grad()                                              \n",
    "        loss_pde = 0.001*model.loss_pde(x_int_ignore) + 0.1*model.loss_pde(x_int_covert) +\\\n",
    "                   100*model.loss_pde(x_int_memory) + model.loss_pde(x_int_overt)\n",
    "        \n",
    "        loss_ic = 0.001*model.loss_ic(x_ic_ignore,u_ic_ignore) +\\\n",
    "                   0.1*model.loss_ic(x_ic_covert,u_ic_covert) +\\\n",
    "                    100*model.loss_ic(x_ic_memory,u_ic_memory) +\\\n",
    "                          model.loss_ic(x_ic_overt,u_ic_overt)\n",
    "        \n",
    "        #loss_bd = model.loss_bd(x_bdL,u_bdL_in,mark_bd) #+model.loss_bd(x_bdR,u_bdR_in,mark_bd) \n",
    "        loss_bd =  0.001*model.loss_bd(x_bdL_ignore,u_bdL_ignore) +\\\n",
    "                    0.1*model.loss_bd(x_bdL_covert,u_bdL_covert) +\\\n",
    "                     100*model.loss_bd(x_bdL_memory,u_bdL_memory) +\\\n",
    "                        model.loss_bd(x_bdL_overt,u_bdL_overt) +\\\n",
    "                   0.001*model.loss_bd(x_bdR_ignore,u_bdR_ignore) +\\\n",
    "                    0.1*model.loss_bd(x_bdR_covert,u_bdR_covert) +\\\n",
    "                     100*model.loss_bd(x_bdR_memory,u_bdR_memory) +\\\n",
    "                         model.loss_bd(x_bdR_overt,u_bdR_overt)\n",
    "        \n",
    "         \n",
    "        loss = loss_pde + 10*loss_ic+ 10*loss_bd                                       \n",
    "\n",
    "        print(f'epoch {epoch} loss_pde:{loss_pde:.8f}, loss_ic:{loss_ic:.8f}, loss_bd:{loss_bd:.8f}')\n",
    "        #model.it = model.it + 1\n",
    "        #outputfile = open('loss_history_burgers.dat','a+')\n",
    "        #print(f'{model.it}  {loss_pde:.6f}  {loss_ic:.6f}  {loss:.6f}',file=outputfile)\n",
    "        #outputfile.close() \n",
    "        loss.backward()\n",
    "        return loss\n",
    "\n",
    "    loss = optimizer.step(closure)\n",
    "    loss_value = loss.item() if not isinstance(loss, float) else loss\n",
    "    print(f'epoch {epoch}: loss {loss_value:.6f}')\n",
    "    \n",
    "def gradients(outputs, inputs):\n",
    "    return torch.autograd.grad(outputs, inputs,grad_outputs=torch.ones_like(outputs), create_graph=True)\n",
    "\n",
    "def to_numpy(input):\n",
    "    if isinstance(input, torch.Tensor):\n",
    "        return input.detach().cpu().numpy()\n",
    "    elif isinstance(input, np.ndarray):\n",
    "        return input\n",
    "    else:\n",
    "        raise TypeError('Unknown type of input, expected torch.Tensor or ' \\\n",
    "                        'np.ndarray, but got {}'.format(type(input)))\n",
    "\n",
    "\n",
    "class DNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DNN, self).__init__()\n",
    "        self.net = nn.Sequential()                                                  \n",
    "        self.net.add_module('Linear_layer_1', nn.Linear(2, 128))                     \n",
    "        self.net.add_module('Tanh_layer_1', nn.Tanh())                              \n",
    "\n",
    "        for num in range(2, 4):                                                     \n",
    "            self.net.add_module('Linear_layer_%d' % (num), nn.Linear(128, 128))       \n",
    "            self.net.add_module('Tanh_layer_%d' % (num), nn.Tanh())                 \n",
    "        self.net.add_module('Linear_layer_final', nn.Linear(128, 1))                 \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "    def loss_pde(self, x):\n",
    "        y = self.net(x)                                                \n",
    "        du_g = gradients(y, x)[0]                                 \n",
    "        u_t,u_x = du_g[:, :1],du_g[:,1:]\n",
    "        dux_g = gradients(u_x,x)[0]\n",
    "        u_xx = dux_g[:,1:]\n",
    "        f = ((u_t -0.1*u_xx+5*y**3-5*y)**2).mean() \n",
    "        if (len(x) == 0) :\n",
    "            f = 0\n",
    "        return f\n",
    "    \n",
    "    def res_pde(self,x):\n",
    "        y = self.net(x)\n",
    "        Res = np.zeros((x.shape[0]))                                  \n",
    "        \n",
    "        u = y[:, 0:1]\n",
    "        du_g = gradients(u, x)[0]                                  \n",
    "        u_t,u_x = du_g[:, :1],du_g[:,1:]\n",
    "        Res = (u_t - u_x)**2 \n",
    "        return Res \n",
    "    \n",
    "    def lambda_pde(self,x):\n",
    "        y = self.net(x)\n",
    "        Res = np.zeros((x.shape[0]))                                  \n",
    "        \n",
    "        u = y[:, 0:1]\n",
    "        du_g = gradients(u, x)[0]                                  \n",
    "        u_t,u_x = du_g[:, :1],du_g[:,1:]\n",
    "        d = 0.1*(abs(u_x)-u_x) + 1\n",
    "        return  d\n",
    "\n",
    "    def loss_ic(self, x_ic, u_ic):\n",
    "        y_ic = self.net(x_ic)                                                      \n",
    "        loss_ics = ((y_ic - u_ic) ** 2).mean()\n",
    "        if (len(x_ic) == 0):\n",
    "            loss_ics = 0\n",
    "        return loss_ics\n",
    "    def loss_bd(self, x,u_in):\n",
    "        y = self.net(x)                                                      \n",
    "        loss_bds = ((y - u_in) ** 2).mean()\n",
    "        if (len(x) == 0):\n",
    "            loss_bds = 0\n",
    "        return loss_bds\n",
    "    def loss_bd1(self, x):\n",
    "        y = self.net(x)                                                      \n",
    "       # uR = yR[:,0:1]\n",
    "        du_g = gradients(y,x)[0]\n",
    "     #   duR_g = gradients(uR,xR)[0]\n",
    "        \n",
    "        u_x = du_g[:,1:]\n",
    "        \n",
    "        loss_bds = ((y + 1.0) ** 2 + (u_x)**2).mean()\n",
    "        \n",
    "        if (len(x) == 0):\n",
    "            loss_bds = 0\n",
    "        return loss_bds\n",
    "def BD_perid(X,T,N):\n",
    "    xL = np.zeros((N,2)) \n",
    "    xR = np.zeros((N,2)) \n",
    "    \n",
    "    for i in range(N):\n",
    "        t = np.random.rand()*T\n",
    "        xL[i,0] = t\n",
    "        xL[i,1] = -X \n",
    "        xR[i,0] = t\n",
    "        xR[i,1] = X\n",
    "    return xL,xR\n",
    "def BD(X):\n",
    "    u = np.zeros((X.shape[0])) \n",
    "    \n",
    "    for i in range(X.shape[0]):\n",
    "        x = X[i,1]\n",
    "        t = X[i,0]\n",
    "        u[i] =  -1.0\n",
    "    return u\n",
    "def IC1(x):\n",
    "    N = x.shape[0]\n",
    "    u = np.zeros((N))                                                \n",
    "    for i in range(N):\n",
    "        u[i] = np.sin(np.pi*(x[i,1]))\n",
    "    return u\n",
    "def IC(X):\n",
    "    N = X.shape[0]\n",
    "    u = np.zeros((X.shape[0]))                                                \n",
    "    for i in range(N):\n",
    "        x = X[i,1]\n",
    "        u[i] =x**2*np.cos(np.pi*x)\n",
    "    return u\n",
    "\n",
    "def Mark(x):\n",
    "    N =x.shape[0]\n",
    "    mark = np.zeros((N,2)) \n",
    "    \n",
    "    for i in range(N):\n",
    "        if (x[i,1] < 0):\n",
    "            mark[i,1] = 1.0\n",
    "        elif ( x[i,0]< 0.1 and x[i,1]< 0.3 ):\n",
    "            mark[i,1] = 1.0\n",
    "        elif ( x[i,0]< 0.1 and x[i,1] > 0.3 ):\n",
    "            mark[i,1] = 0.1\n",
    "        else:\n",
    "            mark[i,1] =  0.01 \n",
    "    return mark\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IgnoreAndAttention(X,xL,xR,tL,tR,num):\n",
    "    A_covert    = []\n",
    "    A_overt     = []\n",
    "    A_memory    = []\n",
    "    A_ignore    = []\n",
    "    for i in range(num):\n",
    "        x = X[i,1]\n",
    "        t = X[i,0]\n",
    "        if ((t - tL) < 0 ):\n",
    "            A_memory.append(i)\n",
    "        elif ((t - tL) >= 0 and (t- tR) <= 0 ):\n",
    "            if ((x - xL) >= 0 and (x- xR) <= 0):\n",
    "                A_overt.append(i)\n",
    "            else:\n",
    "                A_covert.append(i)\n",
    "        else:\n",
    "            A_ignore.append(i)\n",
    "            \n",
    "    x_ignore = np.zeros((len(A_ignore),2))\n",
    "    x_covert = np.zeros((len(A_covert),2))\n",
    "    x_overt = np.zeros((len(A_overt),2))\n",
    "    x_memory = np.zeros((len(A_memory),2))\n",
    "    for i in range( len(A_ignore)):\n",
    "        x_ignore[i,:] = X[A_ignore[i],:]\n",
    "    for i in range( len(A_memory)):\n",
    "        x_memory[i,:] = X[A_memory[i],:]\n",
    "        \n",
    "    for i in range( len(A_covert)):\n",
    "        x_covert[i,:] = X[A_covert[i],:]\n",
    "    for i in range( len(A_overt)):\n",
    "        x_overt[i,:] = X[A_overt[i],:]\n",
    "    \n",
    "    return x_memory, x_covert,x_overt,x_ignore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device('cuda')                                      \n",
    "num_x = 500                                                        \n",
    "num_t = 2000                                                        \n",
    "num_ic = 2000                                           \n",
    "num_int = 2000                                            \n",
    "Time = 0.0\n",
    "L = 1\n",
    "\n",
    "AT_xL = -L \n",
    "AT_xR = L\n",
    "AT_tL = 0\n",
    "AT_tR = 0.0\n",
    "\n",
    "xlimits = np.array([[0.,Time],[-L, L]])  #interal\n",
    "sampling = LHS(xlimits=xlimits)\n",
    "x_int= sampling(num_int)\n",
    "x_int_memory,x_int_covert,x_int_overt,x_int_ignore = IgnoreAndAttention(x_int,AT_xL,AT_xR,AT_tL,AT_tR,num_int)\n",
    "\n",
    "xlimits = np.array([[0.,0],[-L, L]])  #interal\n",
    "sampling = LHS(xlimits=xlimits)\n",
    "x_ic  = sampling(num_ic)\n",
    "x_ic_memory,x_ic_covert,x_ic_overt,x_ic_ignore = IgnoreAndAttention(x_ic,AT_xL,AT_xR,AT_tL,AT_tR,num_ic)\n",
    "\n",
    "u_ic_ignore = IC(x_ic_ignore)                 \n",
    "u_ic_memory = IC(x_ic_memory)                 \n",
    "u_ic_covert = IC(x_ic_covert)                 \n",
    "u_ic_overt = IC(x_ic_overt)                 \n",
    "\n",
    "xlimits = np.array([[0.,Time],[-1,-1]])  #interal\n",
    "sampling = LHS(xlimits=xlimits)\n",
    "x_bdL  = sampling(num_ic)\n",
    "\n",
    "xlimits = np.array([[0.,Time],[1,1]])  #interal\n",
    "sampling = LHS(xlimits=xlimits)\n",
    "x_bdR  = sampling(num_ic)\n",
    "\n",
    "#x_bdL,x_bdR = BD_perid(L,Time,num_t)                            \n",
    "x_bdL_memory,x_bdL_covert,x_bdL_overt,x_bdL_ignore = IgnoreAndAttention(x_bdL,AT_xL,AT_xR,AT_tL,AT_tR,num_t)\n",
    "x_bdR_memory,x_bdR_covert,x_bdR_overt,x_bdR_ignore = IgnoreAndAttention(x_bdR,AT_xL,AT_xR,AT_tL,AT_tR,num_t)\n",
    "\n",
    "#x_ic_memory,x_ic_covert,x_ic_overt,x_ic_ignore = IgnoreAndAttention(x_ic,AT_xL,AT_xR,AT_tL,AT_tR,num_ic)\n",
    "\n",
    "\n",
    "u_bdL_memory =  BD(x_bdL_memory)                            \n",
    "u_bdL_covert =  BD(x_bdL_covert)                            \n",
    "u_bdL_overt =   BD(x_bdL_overt)                            \n",
    "u_bdL_ignore =  BD(x_bdL_ignore)                            \n",
    "\n",
    "u_bdR_memory =  BD(x_bdR_memory)                            \n",
    "u_bdR_covert =  BD(x_bdR_covert)                            \n",
    "u_bdR_overt =   BD(x_bdR_overt)                            \n",
    "u_bdR_ignore =  BD(x_bdR_ignore)                            \n",
    "\n",
    "x_ic_memory    = torch.tensor(x_ic_memory, dtype=torch.float32).to(device)\n",
    "u_ic_memory    = torch.tensor(u_ic_memory, dtype=torch.float32).to(device)\n",
    "x_int_memory   = torch.tensor(x_int_memory, requires_grad=True, dtype=torch.float32).to(device)\n",
    "x_bdL_memory   = torch.tensor(x_bdL_memory, requires_grad=True,  dtype=torch.float32).to(device)\n",
    "x_bdR_memory   = torch.tensor(x_bdR_memory, requires_grad=True,  dtype=torch.float32).to(device)\n",
    "u_bdL_memory   = torch.tensor(u_bdL_memory, requires_grad=True,  dtype=torch.float32).to(device)\n",
    "u_bdR_memory   = torch.tensor(u_bdR_memory, requires_grad=True,  dtype=torch.float32).to(device)\n",
    "\n",
    "x_ic_covert    = torch.tensor(x_ic_covert, dtype=torch.float32).to(device)\n",
    "u_ic_covert    = torch.tensor(u_ic_covert, dtype=torch.float32).to(device)\n",
    "x_int_covert   = torch.tensor(x_int_covert, requires_grad=True, dtype=torch.float32).to(device)\n",
    "x_bdL_covert   = torch.tensor(x_bdL_covert, requires_grad=True, dtype=torch.float32).to(device)\n",
    "x_bdR_covert   = torch.tensor(x_bdR_covert, requires_grad=True,  dtype=torch.float32).to(device)\n",
    "u_bdL_covert   = torch.tensor(u_bdL_covert, requires_grad=True,  dtype=torch.float32).to(device)\n",
    "u_bdR_covert   = torch.tensor(u_bdR_covert, requires_grad=True,  dtype=torch.float32).to(device)\n",
    "\n",
    "x_ic_overt    = torch.tensor(x_ic_overt, dtype=torch.float32).to(device)\n",
    "u_ic_overt    = torch.tensor(u_ic_overt, dtype=torch.float32).to(device)\n",
    "x_int_overt   = torch.tensor(x_int_overt, requires_grad=True, dtype=torch.float32).to(device)\n",
    "x_bdL_overt   = torch.tensor(x_bdL_overt,requires_grad=True, dtype=torch.float32).to(device)\n",
    "x_bdR_overt   = torch.tensor(x_bdR_overt, requires_grad=True, dtype=torch.float32).to(device)\n",
    "u_bdL_overt   = torch.tensor(u_bdL_overt, requires_grad=True, dtype=torch.float32).to(device)\n",
    "u_bdR_overt   = torch.tensor(u_bdR_overt, requires_grad=True, dtype=torch.float32).to(device)\n",
    "\n",
    "\n",
    "x_ic_ignore    = torch.tensor(x_ic_ignore, dtype=torch.float32).to(device)\n",
    "u_ic_ignore    = torch.tensor(u_ic_ignore, dtype=torch.float32).to(device)\n",
    "x_int_ignore   = torch.tensor(x_int_ignore, requires_grad=True, dtype=torch.float32).to(device)\n",
    "x_bdL_ignore   = torch.tensor(x_bdL_ignore, requires_grad=True,  dtype=torch.float32).to(device)\n",
    "x_bdR_ignore   = torch.tensor(x_bdR_ignore, requires_grad=True,  dtype=torch.float32).to(device)\n",
    "u_bdL_ignore   = torch.tensor(u_bdL_ignore, requires_grad=True,  dtype=torch.float32).to(device)\n",
    "u_bdR_ignore   = torch.tensor(u_bdR_ignore, requires_grad=True,  dtype=torch.float32).to(device)\n",
    "\n",
    "model = DNN().to(device)\n",
    "\n",
    "lr = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "print('Start training...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 loss_pde:0.43162808, loss_ic:0.10707185, loss_bd:1.53765893\n",
      "epoch 1 loss_pde:0.44719315, loss_ic:0.10671659, loss_bd:1.52930248\n",
      "epoch 1 loss_pde:0.65567368, loss_ic:0.10336672, loss_bd:1.42646503\n",
      "epoch 1 loss_pde:0.89417827, loss_ic:0.10202567, loss_bd:1.32453060\n",
      "epoch 1 loss_pde:1.16333354, loss_ic:0.10290474, loss_bd:1.22059250\n",
      "epoch 1 loss_pde:1.46043634, loss_ic:0.10649813, loss_bd:1.11292279\n",
      "epoch 1 loss_pde:1.78375638, loss_ic:0.11376030, loss_bd:0.99822026\n",
      "epoch 1 loss_pde:2.13115048, loss_ic:0.12681609, loss_bd:0.87004042\n",
      "epoch 1 loss_pde:2.49020886, loss_ic:0.15174171, loss_bd:0.71265268\n",
      "epoch 1 loss_pde:2.68619347, loss_ic:0.22102256, loss_bd:0.46182209\n",
      "epoch 1 loss_pde:2.00963020, loss_ic:0.33503985, loss_bd:0.24411511\n",
      "epoch 1 loss_pde:0.48762056, loss_ic:0.51285332, loss_bd:0.07970592\n",
      "epoch 1 loss_pde:0.42221919, loss_ic:0.52253699, loss_bd:0.07386178\n",
      "epoch 1 loss_pde:0.39891633, loss_ic:0.52589339, loss_bd:0.07176220\n",
      "epoch 1 loss_pde:0.37555757, loss_ic:0.52906603, loss_bd:0.06968737\n",
      "epoch 1 loss_pde:0.35258424, loss_ic:0.53196615, loss_bd:0.06771379\n",
      "epoch 1 loss_pde:0.32980129, loss_ic:0.53459102, loss_bd:0.06585125\n",
      "epoch 1 loss_pde:0.30671602, loss_ic:0.53695989, loss_bd:0.06407915\n",
      "epoch 1 loss_pde:0.28291956, loss_ic:0.53905499, loss_bd:0.06238687\n",
      "epoch 1 loss_pde:0.25810042, loss_ic:0.54081219, loss_bd:0.06078009\n",
      "epoch 1: loss 16.878935\n",
      "epoch 2 loss_pde:0.23198454, loss_ic:0.54211193, loss_bd:0.05928179\n",
      "epoch 2 loss_pde:0.20427379, loss_ic:0.54274946, loss_bd:0.05793944\n",
      "epoch 2 loss_pde:0.17457621, loss_ic:0.54235423, loss_bd:0.05684799\n",
      "epoch 2 loss_pde:0.14229956, loss_ic:0.54015601, loss_bd:0.05622397\n",
      "epoch 2 loss_pde:0.10649385, loss_ic:0.53409797, loss_bd:0.05671652\n",
      "epoch 2 loss_pde:0.06623717, loss_ic:0.51520479, loss_bd:0.06168616\n",
      "epoch 2 loss_pde:0.11025710, loss_ic:0.28342935, loss_bd:0.23635435\n",
      "epoch 2 loss_pde:0.08606962, loss_ic:0.29739457, loss_bd:0.21429272\n",
      "epoch 2 loss_pde:0.08766475, loss_ic:0.30020007, loss_bd:0.20444857\n",
      "epoch 2 loss_pde:0.09494960, loss_ic:0.30206856, loss_bd:0.19240695\n",
      "epoch 2 loss_pde:0.10333347, loss_ic:0.30315241, loss_bd:0.18224853\n",
      "epoch 2 loss_pde:0.11092513, loss_ic:0.30397332, loss_bd:0.17368835\n",
      "epoch 2 loss_pde:0.11722417, loss_ic:0.30468741, loss_bd:0.16632330\n",
      "epoch 2 loss_pde:0.12214486, loss_ic:0.30532357, loss_bd:0.15988761\n",
      "epoch 2 loss_pde:0.12571879, loss_ic:0.30587724, loss_bd:0.15419707\n",
      "epoch 2 loss_pde:0.12800720, loss_ic:0.30633551, loss_bd:0.14911483\n",
      "epoch 2 loss_pde:0.12907362, loss_ic:0.30668372, loss_bd:0.14453447\n",
      "epoch 2 loss_pde:0.12897590, loss_ic:0.30690554, loss_bd:0.14037220\n",
      "epoch 2 loss_pde:0.12776691, loss_ic:0.30698201, loss_bd:0.13656093\n",
      "epoch 2 loss_pde:0.12549719, loss_ic:0.30689067, loss_bd:0.13304624\n",
      "epoch 2: loss 6.245922\n",
      "epoch 3 loss_pde:0.12222046, loss_ic:0.30660376, loss_bd:0.12978306\n",
      "epoch 3 loss_pde:0.11799950, loss_ic:0.30608672, loss_bd:0.12673268\n",
      "epoch 3 loss_pde:0.11291183, loss_ic:0.30529541, loss_bd:0.12385982\n",
      "epoch 3 loss_pde:0.10705492, loss_ic:0.30417225, loss_bd:0.12113002\n",
      "epoch 3 loss_pde:0.10055076, loss_ic:0.30263925, loss_bd:0.11850625\n",
      "epoch 3 loss_pde:0.09354910, loss_ic:0.30058661, loss_bd:0.11594556\n",
      "epoch 3 loss_pde:0.08623287, loss_ic:0.29784980, loss_bd:0.11339203\n",
      "epoch 3 loss_pde:0.07883289, loss_ic:0.29416469, loss_bd:0.11076668\n",
      "epoch 3 loss_pde:0.07166990, loss_ic:0.28906369, loss_bd:0.10794068\n",
      "epoch 3 loss_pde:0.06529775, loss_ic:0.28160363, loss_bd:0.10466157\n",
      "epoch 3 loss_pde:0.06108374, loss_ic:0.26944920, loss_bd:0.10026072\n",
      "epoch 3 loss_pde:0.06525266, loss_ic:0.24440329, loss_bd:0.09172183\n",
      "epoch 3 loss_pde:0.33976296, loss_ic:0.18450435, loss_bd:0.05189339\n",
      "epoch 3 loss_pde:0.32002559, loss_ic:0.18656272, loss_bd:0.04721721\n",
      "epoch 3 loss_pde:0.31399134, loss_ic:0.18776482, loss_bd:0.04460566\n",
      "epoch 3 loss_pde:0.30544803, loss_ic:0.18922268, loss_bd:0.04186698\n",
      "epoch 3 loss_pde:0.30136108, loss_ic:0.19088636, loss_bd:0.03851821\n",
      "epoch 3 loss_pde:0.29119766, loss_ic:0.19263978, loss_bd:0.03578937\n",
      "epoch 3 loss_pde:0.28814906, loss_ic:0.19444670, loss_bd:0.03249565\n",
      "epoch 3 loss_pde:0.27932996, loss_ic:0.19615853, loss_bd:0.03012079\n",
      "epoch 3: loss 4.486089\n",
      "epoch 4 loss_pde:0.27481925, loss_ic:0.19773157, loss_bd:0.02772096\n",
      "epoch 4 loss_pde:0.26831919, loss_ic:0.19918112, loss_bd:0.02583431\n",
      "epoch 4 loss_pde:0.26314917, loss_ic:0.20051882, loss_bd:0.02408424\n",
      "epoch 4 loss_pde:0.25748709, loss_ic:0.20175481, loss_bd:0.02260984\n",
      "epoch 4 loss_pde:0.25229651, loss_ic:0.20289148, loss_bd:0.02129455\n",
      "epoch 4 loss_pde:0.24707589, loss_ic:0.20393358, loss_bd:0.02016653\n",
      "epoch 4 loss_pde:0.24212381, loss_ic:0.20488435, loss_bd:0.01917986\n",
      "epoch 4 loss_pde:0.23730078, loss_ic:0.20574810, loss_bd:0.01833287\n",
      "epoch 4 loss_pde:0.23271073, loss_ic:0.20652869, loss_bd:0.01760211\n",
      "epoch 4 loss_pde:0.22831629, loss_ic:0.20723043, loss_bd:0.01697901\n",
      "epoch 4 loss_pde:0.22415751, loss_ic:0.20785743, loss_bd:0.01644863\n",
      "epoch 4 loss_pde:0.22022176, loss_ic:0.20841436, loss_bd:0.01600158\n",
      "epoch 4 loss_pde:0.21651997, loss_ic:0.20890588, loss_bd:0.01562659\n",
      "epoch 4 loss_pde:0.21303934, loss_ic:0.20933704, loss_bd:0.01531480\n",
      "epoch 4 loss_pde:0.20977248, loss_ic:0.20971285, loss_bd:0.01505726\n",
      "epoch 4 loss_pde:0.20669962, loss_ic:0.21003850, loss_bd:0.01484616\n",
      "epoch 4 loss_pde:0.20380121, loss_ic:0.21031862, loss_bd:0.01467449\n",
      "epoch 4 loss_pde:0.20105296, loss_ic:0.21055743, loss_bd:0.01453621\n",
      "epoch 4 loss_pde:0.19843131, loss_ic:0.21075834, loss_bd:0.01442634\n",
      "epoch 4 loss_pde:0.19591345, loss_ic:0.21092363, loss_bd:0.01434093\n",
      "epoch 4: loss 2.529344\n",
      "epoch 5 loss_pde:0.19348155, loss_ic:0.21105435, loss_bd:0.01427712\n",
      "epoch 5 loss_pde:0.19112287, loss_ic:0.21115036, loss_bd:0.01423302\n",
      "epoch 5 loss_pde:0.18883210, loss_ic:0.21121024, loss_bd:0.01420788\n",
      "epoch 5 loss_pde:0.18661235, loss_ic:0.21123181, loss_bd:0.01420129\n",
      "epoch 5 loss_pde:0.18447280, loss_ic:0.21121241, loss_bd:0.01421352\n",
      "epoch 5 loss_pde:0.18242979, loss_ic:0.21114968, loss_bd:0.01424479\n",
      "epoch 5 loss_pde:0.18050238, loss_ic:0.21104212, loss_bd:0.01429500\n",
      "epoch 5 loss_pde:0.17870970, loss_ic:0.21088982, loss_bd:0.01436319\n",
      "epoch 5 loss_pde:0.17706688, loss_ic:0.21069454, loss_bd:0.01444741\n",
      "epoch 5 loss_pde:0.17558387, loss_ic:0.21045969, loss_bd:0.01454428\n",
      "epoch 5 loss_pde:0.17426133, loss_ic:0.21018967, loss_bd:0.01464947\n",
      "epoch 5 loss_pde:0.17309421, loss_ic:0.20988944, loss_bd:0.01475753\n",
      "epoch 5 loss_pde:0.17207116, loss_ic:0.20956399, loss_bd:0.01486178\n",
      "epoch 5 loss_pde:0.17117643, loss_ic:0.20921868, loss_bd:0.01495424\n",
      "epoch 5 loss_pde:0.17039245, loss_ic:0.20885998, loss_bd:0.01502371\n",
      "epoch 5 loss_pde:0.16969535, loss_ic:0.20849732, loss_bd:0.01505385\n",
      "epoch 5 loss_pde:0.16905768, loss_ic:0.20814706, loss_bd:0.01501595\n",
      "epoch 5 loss_pde:0.16840874, loss_ic:0.20784527, loss_bd:0.01484553\n",
      "epoch 5 loss_pde:0.16706263, loss_ic:0.20756762, loss_bd:0.01443153\n",
      "epoch 5 loss_pde:0.16484460, loss_ic:0.20747244, loss_bd:0.01363048\n",
      "epoch 5: loss 2.446796\n",
      "epoch 6 loss_pde:0.16407731, loss_ic:0.20768183, loss_bd:0.01273742\n",
      "epoch 6 loss_pde:0.16402674, loss_ic:0.20782487, loss_bd:0.01220950\n",
      "epoch 6 loss_pde:0.16462508, loss_ic:0.20788243, loss_bd:0.01181940\n",
      "epoch 6 loss_pde:0.16543965, loss_ic:0.20785184, loss_bd:0.01153941\n",
      "epoch 6 loss_pde:0.16668716, loss_ic:0.20774494, loss_bd:0.01131265\n",
      "epoch 6 loss_pde:0.16804579, loss_ic:0.20755163, loss_bd:0.01117565\n",
      "epoch 6 loss_pde:0.16976124, loss_ic:0.20729601, loss_bd:0.01107688\n",
      "epoch 6 loss_pde:0.17133567, loss_ic:0.20697919, loss_bd:0.01106617\n",
      "epoch 6 loss_pde:0.17329027, loss_ic:0.20664312, loss_bd:0.01105043\n",
      "epoch 6 loss_pde:0.17482439, loss_ic:0.20629241, loss_bd:0.01110903\n",
      "epoch 6 loss_pde:0.17678472, loss_ic:0.20594828, loss_bd:0.01113218\n",
      "epoch 6 loss_pde:0.17819558, loss_ic:0.20560865, loss_bd:0.01122104\n",
      "epoch 6 loss_pde:0.17999940, loss_ic:0.20528378, loss_bd:0.01126677\n",
      "epoch 6 loss_pde:0.18131363, loss_ic:0.20497042, loss_bd:0.01136110\n",
      "epoch 6 loss_pde:0.18289694, loss_ic:0.20467116, loss_bd:0.01142168\n",
      "epoch 6 loss_pde:0.18413834, loss_ic:0.20438237, loss_bd:0.01151271\n",
      "epoch 6 loss_pde:0.18554582, loss_ic:0.20410262, loss_bd:0.01158190\n",
      "epoch 6 loss_pde:0.18673225, loss_ic:0.20382927, loss_bd:0.01166999\n",
      "epoch 6 loss_pde:0.18802907, loss_ic:0.20355979, loss_bd:0.01174423\n",
      "epoch 6 loss_pde:0.18918015, loss_ic:0.20329231, loss_bd:0.01183142\n",
      "epoch 6: loss 2.368270\n",
      "epoch 7 loss_pde:0.19041824, loss_ic:0.20302422, loss_bd:0.01190929\n",
      "epoch 7 loss_pde:0.19155391, loss_ic:0.20275418, loss_bd:0.01199738\n",
      "epoch 7 loss_pde:0.19277146, loss_ic:0.20247963, loss_bd:0.01207808\n",
      "epoch 7 loss_pde:0.19390842, loss_ic:0.20219949, loss_bd:0.01216794\n",
      "epoch 7 loss_pde:0.19513509, loss_ic:0.20191115, loss_bd:0.01225057\n",
      "epoch 7 loss_pde:0.19628529, loss_ic:0.20161413, loss_bd:0.01234197\n",
      "epoch 7 loss_pde:0.19754353, loss_ic:0.20130552, loss_bd:0.01242401\n",
      "epoch 7 loss_pde:0.19869365, loss_ic:0.20098710, loss_bd:0.01251506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7 loss_pde:0.20001653, loss_ic:0.20065050, loss_bd:0.01259106\n",
      "epoch 7 loss_pde:0.20111080, loss_ic:0.20030694, loss_bd:0.01267911\n",
      "epoch 7 loss_pde:0.20267396, loss_ic:0.19992572, loss_bd:0.01273093\n",
      "epoch 7 loss_pde:0.20350341, loss_ic:0.19955374, loss_bd:0.01281807\n",
      "epoch 7 loss_pde:0.20658833, loss_ic:0.19902310, loss_bd:0.01277193\n",
      "epoch 7 loss_pde:0.20714611, loss_ic:0.19861956, loss_bd:0.01282098\n",
      "epoch 7 loss_pde:0.20929712, loss_ic:0.19809476, loss_bd:0.01275644\n",
      "epoch 7 loss_pde:0.21008776, loss_ic:0.19758908, loss_bd:0.01274771\n",
      "epoch 7 loss_pde:0.21117830, loss_ic:0.19707094, loss_bd:0.01249991\n",
      "epoch 7 loss_pde:0.21156938, loss_ic:0.19657761, loss_bd:0.01214101\n",
      "epoch 7 loss_pde:0.21133208, loss_ic:0.19641477, loss_bd:0.01149356\n",
      "epoch 7 loss_pde:0.21059676, loss_ic:0.19638488, loss_bd:0.01090293\n",
      "epoch 7: loss 2.339753\n",
      "epoch 8 loss_pde:0.20965628, loss_ic:0.19634300, loss_bd:0.01038507\n",
      "epoch 8 loss_pde:0.20832588, loss_ic:0.19626269, loss_bd:0.00989820\n",
      "epoch 8 loss_pde:0.20669721, loss_ic:0.19614443, loss_bd:0.00945043\n",
      "epoch 8 loss_pde:0.20487702, loss_ic:0.19596909, loss_bd:0.00908560\n",
      "epoch 8 loss_pde:0.20286791, loss_ic:0.19573043, loss_bd:0.00882174\n",
      "epoch 8 loss_pde:0.20044056, loss_ic:0.19541916, loss_bd:0.00867198\n",
      "epoch 8 loss_pde:0.19774967, loss_ic:0.19499631, loss_bd:0.00862365\n",
      "epoch 8 loss_pde:0.19512141, loss_ic:0.19439977, loss_bd:0.00866314\n",
      "epoch 8 loss_pde:0.19233797, loss_ic:0.19376835, loss_bd:0.00860561\n",
      "epoch 8 loss_pde:0.18870252, loss_ic:0.19268225, loss_bd:0.00885143\n",
      "epoch 8 loss_pde:0.18602259, loss_ic:0.19188246, loss_bd:0.00864718\n",
      "epoch 8 loss_pde:0.18244834, loss_ic:0.19096488, loss_bd:0.00877243\n",
      "epoch 8 loss_pde:0.18294407, loss_ic:0.18914925, loss_bd:0.00918127\n",
      "epoch 8 loss_pde:0.17900024, loss_ic:0.18853526, loss_bd:0.00808199\n",
      "epoch 8 loss_pde:0.17654945, loss_ic:0.18690969, loss_bd:0.00836995\n",
      "epoch 8 loss_pde:0.17372788, loss_ic:0.18523514, loss_bd:0.00893346\n",
      "epoch 8 loss_pde:0.17089462, loss_ic:0.18343163, loss_bd:0.00969426\n",
      "epoch 8 loss_pde:0.16780649, loss_ic:0.18176903, loss_bd:0.01040701\n",
      "epoch 8 loss_pde:0.16515626, loss_ic:0.17999817, loss_bd:0.01133275\n",
      "epoch 8 loss_pde:0.16314304, loss_ic:0.17879800, loss_bd:0.01180967\n",
      "epoch 8: loss 2.276937\n",
      "epoch 9 loss_pde:0.16062935, loss_ic:0.17707893, loss_bd:0.01294111\n",
      "epoch 9 loss_pde:0.15952936, loss_ic:0.17616056, loss_bd:0.01334996\n",
      "epoch 9 loss_pde:0.15870503, loss_ic:0.17512214, loss_bd:0.01395591\n",
      "epoch 9 loss_pde:0.15805477, loss_ic:0.17428736, loss_bd:0.01438902\n",
      "epoch 9 loss_pde:0.15832597, loss_ic:0.17320840, loss_bd:0.01491518\n",
      "epoch 9 loss_pde:0.15802406, loss_ic:0.17226706, loss_bd:0.01543304\n",
      "epoch 9 loss_pde:0.15854555, loss_ic:0.17142269, loss_bd:0.01581725\n",
      "epoch 9 loss_pde:0.15916176, loss_ic:0.17054804, loss_bd:0.01620025\n",
      "epoch 9 loss_pde:0.16039667, loss_ic:0.16955833, loss_bd:0.01656182\n",
      "epoch 9 loss_pde:0.16202295, loss_ic:0.16840434, loss_bd:0.01693253\n",
      "epoch 9 loss_pde:0.16478971, loss_ic:0.16694801, loss_bd:0.01732916\n",
      "epoch 9 loss_pde:0.16867816, loss_ic:0.16516411, loss_bd:0.01759569\n",
      "epoch 9 loss_pde:0.17568819, loss_ic:0.16273420, loss_bd:0.01796771\n",
      "epoch 9 loss_pde:0.18410523, loss_ic:0.15947284, loss_bd:0.01860215\n",
      "epoch 9 loss_pde:0.20076002, loss_ic:0.15672739, loss_bd:0.01778727\n",
      "epoch 9 loss_pde:0.21812457, loss_ic:0.15222339, loss_bd:0.01816966\n",
      "epoch 9 loss_pde:0.23744643, loss_ic:0.15054558, loss_bd:0.01649866\n",
      "epoch 9 loss_pde:0.24615125, loss_ic:0.14907202, loss_bd:0.01617054\n",
      "epoch 9 loss_pde:0.25542298, loss_ic:0.14839000, loss_bd:0.01517564\n",
      "epoch 9 loss_pde:0.26281554, loss_ic:0.14757179, loss_bd:0.01453242\n",
      "epoch 9: loss 2.060830\n",
      "epoch 10 loss_pde:0.26816756, loss_ic:0.14711131, loss_bd:0.01373857\n",
      "epoch 10 loss_pde:0.27666786, loss_ic:0.14660557, loss_bd:0.01263396\n",
      "epoch 10 loss_pde:0.28019863, loss_ic:0.14623025, loss_bd:0.01210685\n",
      "epoch 10 loss_pde:0.28350547, loss_ic:0.14610706, loss_bd:0.01136770\n",
      "epoch 10 loss_pde:0.28665361, loss_ic:0.14575399, loss_bd:0.01081196\n",
      "epoch 10 loss_pde:0.28687659, loss_ic:0.14572425, loss_bd:0.01009737\n",
      "epoch 10 loss_pde:0.28873160, loss_ic:0.14556728, loss_bd:0.00943017\n",
      "epoch 10 loss_pde:0.28831464, loss_ic:0.14546400, loss_bd:0.00896305\n",
      "epoch 10 loss_pde:0.28780231, loss_ic:0.14545135, loss_bd:0.00834547\n",
      "epoch 10 loss_pde:0.28554860, loss_ic:0.14535363, loss_bd:0.00791059\n",
      "epoch 10 loss_pde:0.28297797, loss_ic:0.14541358, loss_bd:0.00727520\n",
      "epoch 10 loss_pde:0.27869654, loss_ic:0.14534910, loss_bd:0.00686359\n",
      "epoch 10 loss_pde:0.27390429, loss_ic:0.14545906, loss_bd:0.00626366\n",
      "epoch 10 loss_pde:0.26752284, loss_ic:0.14544988, loss_bd:0.00587537\n",
      "epoch 10 loss_pde:0.26040924, loss_ic:0.14560392, loss_bd:0.00532440\n",
      "epoch 10 loss_pde:0.25173077, loss_ic:0.14564826, loss_bd:0.00496962\n",
      "epoch 10 loss_pde:0.24226275, loss_ic:0.14584012, loss_bd:0.00448646\n",
      "epoch 10 loss_pde:0.23145109, loss_ic:0.14593048, loss_bd:0.00418285\n",
      "epoch 10 loss_pde:0.22008955, loss_ic:0.14612569, loss_bd:0.00381858\n",
      "epoch 10 loss_pde:0.20807469, loss_ic:0.14624432, loss_bd:0.00357753\n",
      "epoch 10: loss 1.876666\n",
      "epoch 11 loss_pde:0.19624031, loss_ic:0.14639243, loss_bd:0.00335753\n",
      "epoch 11 loss_pde:0.18476743, loss_ic:0.14648898, loss_bd:0.00320251\n",
      "epoch 11 loss_pde:0.17429163, loss_ic:0.14654483, loss_bd:0.00310107\n",
      "epoch 11 loss_pde:0.16474277, loss_ic:0.14656471, loss_bd:0.00302610\n",
      "epoch 11 loss_pde:0.15655300, loss_ic:0.14649752, loss_bd:0.00301936\n",
      "epoch 11 loss_pde:0.14941372, loss_ic:0.14644089, loss_bd:0.00299286\n",
      "epoch 11 loss_pde:0.14368731, loss_ic:0.14627077, loss_bd:0.00305531\n",
      "epoch 11 loss_pde:0.13891041, loss_ic:0.14615972, loss_bd:0.00306709\n",
      "epoch 11 loss_pde:0.13530573, loss_ic:0.14593312, loss_bd:0.00316997\n",
      "epoch 11 loss_pde:0.13232282, loss_ic:0.14579673, loss_bd:0.00320342\n",
      "epoch 11 loss_pde:0.13017583, loss_ic:0.14555743, loss_bd:0.00332217\n",
      "epoch 11 loss_pde:0.12838118, loss_ic:0.14542150, loss_bd:0.00335941\n",
      "epoch 11 loss_pde:0.12711440, loss_ic:0.14519709, loss_bd:0.00347693\n",
      "epoch 11 loss_pde:0.12607932, loss_ic:0.14506654, loss_bd:0.00351638\n",
      "epoch 11 loss_pde:0.12533203, loss_ic:0.14487521, loss_bd:0.00361730\n",
      "epoch 11 loss_pde:0.12475312, loss_ic:0.14474815, loss_bd:0.00366227\n",
      "epoch 11 loss_pde:0.12428906, loss_ic:0.14459237, loss_bd:0.00374023\n",
      "epoch 11 loss_pde:0.12392306, loss_ic:0.14446756, loss_bd:0.00378885\n",
      "epoch 11 loss_pde:0.12356797, loss_ic:0.14433439, loss_bd:0.00385029\n",
      "epoch 11 loss_pde:0.12326194, loss_ic:0.14420700, loss_bd:0.00390224\n",
      "epoch 11: loss 1.693740\n",
      "epoch 12 loss_pde:0.12290942, loss_ic:0.14408365, loss_bd:0.00395440\n",
      "epoch 12 loss_pde:0.12259196, loss_ic:0.14394714, loss_bd:0.00401229\n",
      "epoch 12 loss_pde:0.12218177, loss_ic:0.14382900, loss_bd:0.00405758\n",
      "epoch 12 loss_pde:0.12182198, loss_ic:0.14368014, loss_bd:0.00412325\n",
      "epoch 12 loss_pde:0.12132885, loss_ic:0.14356425, loss_bd:0.00416481\n",
      "epoch 12 loss_pde:0.12088598, loss_ic:0.14341050, loss_bd:0.00423318\n",
      "epoch 12 loss_pde:0.12027315, loss_ic:0.14329746, loss_bd:0.00427090\n",
      "epoch 12 loss_pde:0.11967794, loss_ic:0.14314386, loss_bd:0.00433782\n",
      "epoch 12 loss_pde:0.11886269, loss_ic:0.14303829, loss_bd:0.00436820\n",
      "epoch 12 loss_pde:0.11802869, loss_ic:0.14288926, loss_bd:0.00443013\n",
      "epoch 12 loss_pde:0.11691102, loss_ic:0.14280117, loss_bd:0.00444616\n",
      "epoch 12 loss_pde:0.11574765, loss_ic:0.14266728, loss_bd:0.00449567\n",
      "epoch 12 loss_pde:0.11424012, loss_ic:0.14261235, loss_bd:0.00448721\n",
      "epoch 12 loss_pde:0.11268871, loss_ic:0.14250971, loss_bd:0.00451333\n",
      "epoch 12 loss_pde:0.11074119, loss_ic:0.14250740, loss_bd:0.00446878\n",
      "epoch 12 loss_pde:0.10883332, loss_ic:0.14245257, loss_bd:0.00445869\n",
      "epoch 12 loss_pde:0.10651629, loss_ic:0.14246954, loss_bd:0.00440860\n",
      "epoch 12 loss_pde:0.10432688, loss_ic:0.14244305, loss_bd:0.00437253\n",
      "epoch 12 loss_pde:0.10200490, loss_ic:0.14250547, loss_bd:0.00426817\n",
      "epoch 12 loss_pde:0.09976312, loss_ic:0.14251842, loss_bd:0.00420385\n",
      "epoch 12: loss 1.603290\n",
      "epoch 13 loss_pde:0.09765188, loss_ic:0.14257981, loss_bd:0.00410823\n",
      "epoch 13 loss_pde:0.09575271, loss_ic:0.14259344, loss_bd:0.00405619\n",
      "epoch 13 loss_pde:0.09404129, loss_ic:0.14267829, loss_bd:0.00394563\n",
      "epoch 13 loss_pde:0.09262270, loss_ic:0.14270011, loss_bd:0.00389301\n",
      "epoch 13 loss_pde:0.09142049, loss_ic:0.14277540, loss_bd:0.00379485\n",
      "epoch 13 loss_pde:0.09047652, loss_ic:0.14279501, loss_bd:0.00374683\n",
      "epoch 13 loss_pde:0.08969925, loss_ic:0.14285648, loss_bd:0.00366083\n",
      "epoch 13 loss_pde:0.08911771, loss_ic:0.14287193, loss_bd:0.00361593\n",
      "epoch 13 loss_pde:0.08865559, loss_ic:0.14291917, loss_bd:0.00354078\n",
      "epoch 13 loss_pde:0.08833127, loss_ic:0.14292961, loss_bd:0.00349839\n",
      "epoch 13 loss_pde:0.08809415, loss_ic:0.14296372, loss_bd:0.00343220\n",
      "epoch 13 loss_pde:0.08795041, loss_ic:0.14296922, loss_bd:0.00339140\n",
      "epoch 13 loss_pde:0.08787618, loss_ic:0.14299172, loss_bd:0.00333185\n",
      "epoch 13 loss_pde:0.08786250, loss_ic:0.14299260, loss_bd:0.00329155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 13 loss_pde:0.08791265, loss_ic:0.14300522, loss_bd:0.00323597\n",
      "epoch 13 loss_pde:0.08800649, loss_ic:0.14299986, loss_bd:0.00319573\n",
      "epoch 13 loss_pde:0.08816233, loss_ic:0.14300364, loss_bd:0.00314148\n",
      "epoch 13 loss_pde:0.08835335, loss_ic:0.14299101, loss_bd:0.00310051\n",
      "epoch 13 loss_pde:0.08861139, loss_ic:0.14298600, loss_bd:0.00304514\n",
      "epoch 13 loss_pde:0.08890355, loss_ic:0.14296424, loss_bd:0.00300237\n",
      "epoch 13: loss 1.564532\n",
      "epoch 14 loss_pde:0.08927123, loss_ic:0.14294951, loss_bd:0.00294385\n",
      "epoch 14 loss_pde:0.08967801, loss_ic:0.14291608, loss_bd:0.00289817\n",
      "epoch 14 loss_pde:0.09017382, loss_ic:0.14288965, loss_bd:0.00283427\n",
      "epoch 14 loss_pde:0.09071763, loss_ic:0.14284092, loss_bd:0.00278475\n",
      "epoch 14 loss_pde:0.09136186, loss_ic:0.14280096, loss_bd:0.00271293\n",
      "epoch 14 loss_pde:0.09207442, loss_ic:0.14273129, loss_bd:0.00265927\n",
      "epoch 14 loss_pde:0.09287975, loss_ic:0.14267999, loss_bd:0.00257320\n",
      "epoch 14 loss_pde:0.09377730, loss_ic:0.14258565, loss_bd:0.00251477\n",
      "epoch 14 loss_pde:0.09471876, loss_ic:0.14252381, loss_bd:0.00241390\n",
      "epoch 14 loss_pde:0.09577660, loss_ic:0.14240681, loss_bd:0.00234936\n",
      "epoch 14 loss_pde:0.09678227, loss_ic:0.14232564, loss_bd:0.00224871\n",
      "epoch 14 loss_pde:0.09795780, loss_ic:0.14218535, loss_bd:0.00218021\n",
      "epoch 14 loss_pde:0.09912406, loss_ic:0.14205518, loss_bd:0.00209478\n",
      "epoch 14 loss_pde:0.10046892, loss_ic:0.14188600, loss_bd:0.00201557\n",
      "epoch 14 loss_pde:0.10187276, loss_ic:0.14170587, loss_bd:0.00193066\n",
      "epoch 14 loss_pde:0.10341974, loss_ic:0.14149611, loss_bd:0.00184675\n",
      "epoch 14 loss_pde:0.10493009, loss_ic:0.14129530, loss_bd:0.00174654\n",
      "epoch 14 loss_pde:0.10664298, loss_ic:0.14101772, loss_bd:0.00168802\n",
      "epoch 14 loss_pde:0.10809673, loss_ic:0.14085101, loss_bd:0.00153215\n",
      "epoch 14 loss_pde:0.10963271, loss_ic:0.14057858, loss_bd:0.00147721\n",
      "epoch 14: loss 1.548205\n",
      "epoch 15 loss_pde:0.11080433, loss_ic:0.14036570, loss_bd:0.00140669\n",
      "epoch 15 loss_pde:0.11198532, loss_ic:0.14009777, loss_bd:0.00137240\n",
      "epoch 15 loss_pde:0.11286542, loss_ic:0.13987242, loss_bd:0.00131318\n",
      "epoch 15 loss_pde:0.11369472, loss_ic:0.13959880, loss_bd:0.00129550\n",
      "epoch 15 loss_pde:0.11421129, loss_ic:0.13936986, loss_bd:0.00125761\n",
      "epoch 15 loss_pde:0.11469293, loss_ic:0.13909332, loss_bd:0.00125689\n",
      "epoch 15 loss_pde:0.11482494, loss_ic:0.13886860, loss_bd:0.00123008\n",
      "epoch 15 loss_pde:0.11477995, loss_ic:0.13860571, loss_bd:0.00124508\n",
      "epoch 15 loss_pde:0.11446390, loss_ic:0.13837381, loss_bd:0.00124360\n",
      "epoch 15 loss_pde:0.11282316, loss_ic:0.13828656, loss_bd:0.00119979\n",
      "epoch 15 loss_pde:0.11192057, loss_ic:0.13807125, loss_bd:0.00123840\n",
      "epoch 15 loss_pde:0.11081826, loss_ic:0.13785894, loss_bd:0.00129568\n",
      "epoch 15 loss_pde:0.10935502, loss_ic:0.13769999, loss_bd:0.00132509\n",
      "epoch 15 loss_pde:0.10815725, loss_ic:0.13741575, loss_bd:0.00143011\n",
      "epoch 15 loss_pde:0.10635354, loss_ic:0.13733639, loss_bd:0.00140132\n",
      "epoch 15 loss_pde:0.10414924, loss_ic:0.13709740, loss_bd:0.00155677\n",
      "epoch 15 loss_pde:0.10212955, loss_ic:0.13707840, loss_bd:0.00152079\n",
      "epoch 15 loss_pde:0.09997649, loss_ic:0.13695496, loss_bd:0.00162584\n",
      "epoch 15 loss_pde:0.09812641, loss_ic:0.13693832, loss_bd:0.00163794\n",
      "epoch 15 loss_pde:0.09594353, loss_ic:0.13688000, loss_bd:0.00172724\n",
      "epoch 15: loss 1.528528\n",
      "epoch 16 loss_pde:0.09399246, loss_ic:0.13686593, loss_bd:0.00173573\n",
      "epoch 16 loss_pde:0.09215985, loss_ic:0.13686065, loss_bd:0.00178502\n",
      "epoch 16 loss_pde:0.09054496, loss_ic:0.13686274, loss_bd:0.00181854\n",
      "epoch 16 loss_pde:0.08883634, loss_ic:0.13687555, loss_bd:0.00185134\n",
      "epoch 16 loss_pde:0.08745069, loss_ic:0.13684940, loss_bd:0.00187782\n",
      "epoch 16 loss_pde:0.08558927, loss_ic:0.13689269, loss_bd:0.00187350\n",
      "epoch 16 loss_pde:0.08401233, loss_ic:0.13683031, loss_bd:0.00195405\n",
      "epoch 16 loss_pde:0.08225594, loss_ic:0.13693297, loss_bd:0.00189803\n",
      "epoch 16 loss_pde:0.08070648, loss_ic:0.13688330, loss_bd:0.00197931\n",
      "epoch 16 loss_pde:0.07936012, loss_ic:0.13694540, loss_bd:0.00195100\n",
      "epoch 16 loss_pde:0.07815083, loss_ic:0.13692927, loss_bd:0.00199045\n",
      "epoch 16 loss_pde:0.07710170, loss_ic:0.13696082, loss_bd:0.00197197\n",
      "epoch 16 loss_pde:0.07613196, loss_ic:0.13693690, loss_bd:0.00199737\n",
      "epoch 16 loss_pde:0.07530100, loss_ic:0.13695081, loss_bd:0.00196992\n",
      "epoch 16 loss_pde:0.07452901, loss_ic:0.13690449, loss_bd:0.00198904\n",
      "epoch 16 loss_pde:0.07389922, loss_ic:0.13689481, loss_bd:0.00195646\n",
      "epoch 16 loss_pde:0.07337200, loss_ic:0.13683905, loss_bd:0.00195451\n",
      "epoch 16 loss_pde:0.07309905, loss_ic:0.13676587, loss_bd:0.00195003\n",
      "epoch 16 loss_pde:0.07297578, loss_ic:0.13670842, loss_bd:0.00191372\n",
      "epoch 16 loss_pde:0.07313832, loss_ic:0.13655770, loss_bd:0.00193565\n",
      "epoch 16: loss 1.480009\n",
      "epoch 17 loss_pde:0.07342044, loss_ic:0.13649717, loss_bd:0.00185605\n",
      "epoch 17 loss_pde:0.07405693, loss_ic:0.13629125, loss_bd:0.00187997\n",
      "epoch 17 loss_pde:0.07477976, loss_ic:0.13618331, loss_bd:0.00180886\n",
      "epoch 17 loss_pde:0.07581075, loss_ic:0.13597450, loss_bd:0.00180326\n",
      "epoch 17 loss_pde:0.07693608, loss_ic:0.13582179, loss_bd:0.00173538\n",
      "epoch 17 loss_pde:0.07838782, loss_ic:0.13557990, loss_bd:0.00171625\n",
      "epoch 17 loss_pde:0.07992841, loss_ic:0.13538621, loss_bd:0.00164327\n",
      "epoch 17 loss_pde:0.08176250, loss_ic:0.13510975, loss_bd:0.00161841\n",
      "epoch 17 loss_pde:0.08359974, loss_ic:0.13488805, loss_bd:0.00154867\n",
      "epoch 17 loss_pde:0.08558667, loss_ic:0.13461117, loss_bd:0.00151980\n",
      "epoch 17 loss_pde:0.08751343, loss_ic:0.13438225, loss_bd:0.00145984\n",
      "epoch 17 loss_pde:0.08946411, loss_ic:0.13412201, loss_bd:0.00143132\n",
      "epoch 17 loss_pde:0.09137049, loss_ic:0.13389356, loss_bd:0.00138256\n",
      "epoch 17 loss_pde:0.09323022, loss_ic:0.13364872, loss_bd:0.00135580\n",
      "epoch 17 loss_pde:0.09510092, loss_ic:0.13341622, loss_bd:0.00131864\n",
      "epoch 17 loss_pde:0.09688319, loss_ic:0.13318163, loss_bd:0.00129021\n",
      "epoch 17 loss_pde:0.09875855, loss_ic:0.13292895, loss_bd:0.00126918\n",
      "epoch 17 loss_pde:0.10062034, loss_ic:0.13268980, loss_bd:0.00121853\n",
      "epoch 17 loss_pde:0.10256574, loss_ic:0.13238180, loss_bd:0.00122931\n",
      "epoch 17 loss_pde:0.10417448, loss_ic:0.13217168, loss_bd:0.00119017\n",
      "epoch 17: loss 1.456953\n",
      "epoch 18 loss_pde:0.10572278, loss_ic:0.13191803, loss_bd:0.00119636\n",
      "epoch 18 loss_pde:0.10714933, loss_ic:0.13171212, loss_bd:0.00116823\n",
      "epoch 18 loss_pde:0.10845102, loss_ic:0.13148369, loss_bd:0.00117250\n",
      "epoch 18 loss_pde:0.10961366, loss_ic:0.13128774, loss_bd:0.00115671\n",
      "epoch 18 loss_pde:0.11064267, loss_ic:0.13108236, loss_bd:0.00115911\n",
      "epoch 18 loss_pde:0.11155042, loss_ic:0.13088876, loss_bd:0.00115297\n",
      "epoch 18 loss_pde:0.11233664, loss_ic:0.13068566, loss_bd:0.00116210\n",
      "epoch 18 loss_pde:0.11288383, loss_ic:0.13050859, loss_bd:0.00116593\n",
      "epoch 18 loss_pde:0.11328510, loss_ic:0.13033250, loss_bd:0.00117993\n",
      "epoch 18 loss_pde:0.11345303, loss_ic:0.13017623, loss_bd:0.00118609\n",
      "epoch 18 loss_pde:0.11352292, loss_ic:0.13000786, loss_bd:0.00120707\n",
      "epoch 18 loss_pde:0.11327431, loss_ic:0.12986210, loss_bd:0.00123351\n",
      "epoch 18 loss_pde:0.11297224, loss_ic:0.12971769, loss_bd:0.00125616\n",
      "epoch 18 loss_pde:0.11232441, loss_ic:0.12957203, loss_bd:0.00131606\n",
      "epoch 18 loss_pde:0.11173533, loss_ic:0.12946416, loss_bd:0.00133227\n",
      "epoch 18 loss_pde:0.11077192, loss_ic:0.12933573, loss_bd:0.00140871\n",
      "epoch 18 loss_pde:0.10995215, loss_ic:0.12925400, loss_bd:0.00143257\n",
      "epoch 18 loss_pde:0.10884976, loss_ic:0.12915045, loss_bd:0.00150931\n",
      "epoch 18 loss_pde:0.10794913, loss_ic:0.12909071, loss_bd:0.00152840\n",
      "epoch 18 loss_pde:0.10669439, loss_ic:0.12899031, loss_bd:0.00162201\n",
      "epoch 18: loss 1.436867\n",
      "epoch 19 loss_pde:0.10579087, loss_ic:0.12894845, loss_bd:0.00163053\n",
      "epoch 19 loss_pde:0.10456020, loss_ic:0.12885392, loss_bd:0.00172461\n",
      "epoch 19 loss_pde:0.10368472, loss_ic:0.12880719, loss_bd:0.00175162\n",
      "epoch 19 loss_pde:0.10274224, loss_ic:0.12872787, loss_bd:0.00182104\n",
      "epoch 19 loss_pde:0.10199070, loss_ic:0.12866765, loss_bd:0.00185960\n",
      "epoch 19 loss_pde:0.10127674, loss_ic:0.12858243, loss_bd:0.00191763\n",
      "epoch 19 loss_pde:0.10061900, loss_ic:0.12850498, loss_bd:0.00196448\n",
      "epoch 19 loss_pde:0.10008182, loss_ic:0.12840880, loss_bd:0.00201684\n",
      "epoch 19 loss_pde:0.09955776, loss_ic:0.12831356, loss_bd:0.00206818\n",
      "epoch 19 loss_pde:0.09920345, loss_ic:0.12820284, loss_bd:0.00211673\n",
      "epoch 19 loss_pde:0.09888303, loss_ic:0.12808193, loss_bd:0.00216827\n",
      "epoch 19 loss_pde:0.09872623, loss_ic:0.12793966, loss_bd:0.00222039\n",
      "epoch 19 loss_pde:0.09868186, loss_ic:0.12778772, loss_bd:0.00226592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 19 loss_pde:0.09869688, loss_ic:0.12760192, loss_bd:0.00233203\n",
      "epoch 19 loss_pde:0.09897102, loss_ic:0.12742671, loss_bd:0.00236325\n",
      "epoch 19 loss_pde:0.09903523, loss_ic:0.12719800, loss_bd:0.00245985\n",
      "epoch 19 loss_pde:0.09975408, loss_ic:0.12700775, loss_bd:0.00245514\n",
      "epoch 19 loss_pde:0.09995993, loss_ic:0.12674795, loss_bd:0.00256795\n",
      "epoch 19 loss_pde:0.10075821, loss_ic:0.12656075, loss_bd:0.00256848\n",
      "epoch 19 loss_pde:0.10120617, loss_ic:0.12632805, loss_bd:0.00264676\n",
      "epoch 19: loss 1.411581\n",
      "epoch 20 loss_pde:0.10212821, loss_ic:0.12613396, loss_bd:0.00264139\n",
      "epoch 20 loss_pde:0.10274071, loss_ic:0.12589765, loss_bd:0.00270772\n",
      "epoch 20 loss_pde:0.10361264, loss_ic:0.12569775, loss_bd:0.00272040\n",
      "epoch 20 loss_pde:0.10436615, loss_ic:0.12550642, loss_bd:0.00274523\n",
      "epoch 20 loss_pde:0.10505487, loss_ic:0.12532648, loss_bd:0.00277239\n",
      "epoch 20 loss_pde:0.10586730, loss_ic:0.12517457, loss_bd:0.00276635\n",
      "epoch 20 loss_pde:0.10640231, loss_ic:0.12501484, loss_bd:0.00280095\n",
      "epoch 20 loss_pde:0.10718872, loss_ic:0.12489503, loss_bd:0.00277718\n",
      "epoch 20 loss_pde:0.10762392, loss_ic:0.12476496, loss_bd:0.00280392\n",
      "epoch 20 loss_pde:0.10831448, loss_ic:0.12467161, loss_bd:0.00277317\n",
      "epoch 20 loss_pde:0.10869663, loss_ic:0.12457214, loss_bd:0.00278358\n",
      "epoch 20 loss_pde:0.10926746, loss_ic:0.12450327, loss_bd:0.00274719\n",
      "epoch 20 loss_pde:0.10957492, loss_ic:0.12442846, loss_bd:0.00274632\n",
      "epoch 20 loss_pde:0.11001525, loss_ic:0.12437975, loss_bd:0.00270819\n",
      "epoch 20 loss_pde:0.11024109, loss_ic:0.12432475, loss_bd:0.00269970\n",
      "epoch 20 loss_pde:0.11057442, loss_ic:0.12429189, loss_bd:0.00266005\n",
      "epoch 20 loss_pde:0.11074019, loss_ic:0.12425556, loss_bd:0.00264218\n",
      "epoch 20 loss_pde:0.11097512, loss_ic:0.12422998, loss_bd:0.00260707\n",
      "epoch 20 loss_pde:0.11112313, loss_ic:0.12420860, loss_bd:0.00257600\n",
      "epoch 20 loss_pde:0.11128277, loss_ic:0.12418273, loss_bd:0.00254895\n",
      "epoch 20: loss 1.389882\n",
      "epoch 21 loss_pde:0.11143098, loss_ic:0.12417262, loss_bd:0.00250805\n",
      "epoch 21 loss_pde:0.11156387, loss_ic:0.12415580, loss_bd:0.00247643\n",
      "epoch 21 loss_pde:0.11159664, loss_ic:0.12414926, loss_bd:0.00244303\n",
      "epoch 21 loss_pde:0.11177051, loss_ic:0.12414548, loss_bd:0.00239065\n",
      "epoch 21 loss_pde:0.11169782, loss_ic:0.12414327, loss_bd:0.00236132\n",
      "epoch 21 loss_pde:0.11178739, loss_ic:0.12413891, loss_bd:0.00231236\n",
      "epoch 21 loss_pde:0.11173938, loss_ic:0.12414725, loss_bd:0.00226922\n",
      "epoch 21 loss_pde:0.11149465, loss_ic:0.12416065, loss_bd:0.00223436\n",
      "epoch 21 loss_pde:0.11132823, loss_ic:0.12419514, loss_bd:0.00216553\n",
      "epoch 21 loss_pde:0.11095708, loss_ic:0.12422844, loss_bd:0.00211433\n",
      "epoch 21 loss_pde:0.11045491, loss_ic:0.12428457, loss_bd:0.00204659\n",
      "epoch 21 loss_pde:0.10984991, loss_ic:0.12434679, loss_bd:0.00198323\n",
      "epoch 21 loss_pde:0.10922845, loss_ic:0.12441576, loss_bd:0.00191396\n",
      "epoch 21 loss_pde:0.10844301, loss_ic:0.12448698, loss_bd:0.00186116\n",
      "epoch 21 loss_pde:0.10771058, loss_ic:0.12455960, loss_bd:0.00180828\n",
      "epoch 21 loss_pde:0.10690206, loss_ic:0.12462625, loss_bd:0.00177440\n",
      "epoch 21 loss_pde:0.10618371, loss_ic:0.12468877, loss_bd:0.00174139\n",
      "epoch 21 loss_pde:0.10539194, loss_ic:0.12474164, loss_bd:0.00172922\n",
      "epoch 21 loss_pde:0.10474187, loss_ic:0.12479396, loss_bd:0.00170698\n",
      "epoch 21 loss_pde:0.10393891, loss_ic:0.12483288, loss_bd:0.00171524\n",
      "epoch 21: loss 1.378238\n",
      "epoch 22 loss_pde:0.10335746, loss_ic:0.12487906, loss_bd:0.00169755\n",
      "epoch 22 loss_pde:0.10262781, loss_ic:0.12490690, loss_bd:0.00171505\n",
      "epoch 22 loss_pde:0.10210662, loss_ic:0.12493952, loss_bd:0.00171063\n",
      "epoch 22 loss_pde:0.10157712, loss_ic:0.12494949, loss_bd:0.00172932\n",
      "epoch 22 loss_pde:0.10109030, loss_ic:0.12497038, loss_bd:0.00173368\n",
      "epoch 22 loss_pde:0.10068226, loss_ic:0.12497501, loss_bd:0.00173641\n",
      "epoch 22 loss_pde:0.10011501, loss_ic:0.12498676, loss_bd:0.00173664\n",
      "epoch 22 loss_pde:0.09957228, loss_ic:0.12498658, loss_bd:0.00173975\n",
      "epoch 22 loss_pde:0.09894242, loss_ic:0.12498906, loss_bd:0.00174110\n",
      "epoch 22 loss_pde:0.09839284, loss_ic:0.12497722, loss_bd:0.00173980\n",
      "epoch 22 loss_pde:0.09763844, loss_ic:0.12497544, loss_bd:0.00173930\n",
      "epoch 22 loss_pde:0.09710051, loss_ic:0.12494320, loss_bd:0.00173630\n",
      "epoch 22 loss_pde:0.09627768, loss_ic:0.12493113, loss_bd:0.00173431\n",
      "epoch 22 loss_pde:0.09569436, loss_ic:0.12488776, loss_bd:0.00173235\n",
      "epoch 22 loss_pde:0.09493858, loss_ic:0.12486819, loss_bd:0.00171962\n",
      "epoch 22 loss_pde:0.09425120, loss_ic:0.12480944, loss_bd:0.00173119\n",
      "epoch 22 loss_pde:0.09370431, loss_ic:0.12479221, loss_bd:0.00168634\n",
      "epoch 22 loss_pde:0.09264515, loss_ic:0.12469806, loss_bd:0.00175160\n",
      "epoch 22 loss_pde:0.09238433, loss_ic:0.12470389, loss_bd:0.00164295\n",
      "epoch 22 loss_pde:0.09163689, loss_ic:0.12464447, loss_bd:0.00165652\n",
      "epoch 22: loss 1.369124\n",
      "epoch 23 loss_pde:0.09122704, loss_ic:0.12461630, loss_bd:0.00162728\n",
      "epoch 23 loss_pde:0.08908591, loss_ic:0.12479442, loss_bd:0.00152184\n",
      "epoch 23 loss_pde:0.08800607, loss_ic:0.12478833, loss_bd:0.00151352\n",
      "epoch 23 loss_pde:0.08702121, loss_ic:0.12478131, loss_bd:0.00150382\n",
      "epoch 23 loss_pde:0.08596654, loss_ic:0.12476676, loss_bd:0.00148819\n",
      "epoch 23 loss_pde:0.08494782, loss_ic:0.12474298, loss_bd:0.00145264\n",
      "epoch 23 loss_pde:0.08364788, loss_ic:0.12469023, loss_bd:0.00144554\n",
      "epoch 23 loss_pde:0.08273559, loss_ic:0.12464770, loss_bd:0.00138921\n",
      "epoch 23 loss_pde:0.08163176, loss_ic:0.12456163, loss_bd:0.00138567\n",
      "epoch 23 loss_pde:0.08100422, loss_ic:0.12448756, loss_bd:0.00133454\n",
      "epoch 23 loss_pde:0.08022224, loss_ic:0.12436872, loss_bd:0.00133509\n",
      "epoch 23 loss_pde:0.07972836, loss_ic:0.12425589, loss_bd:0.00130730\n",
      "epoch 23 loss_pde:0.07921552, loss_ic:0.12410904, loss_bd:0.00130554\n",
      "epoch 23 loss_pde:0.07851633, loss_ic:0.12395381, loss_bd:0.00132267\n",
      "epoch 23 loss_pde:0.07825204, loss_ic:0.12379203, loss_bd:0.00129371\n",
      "epoch 23 loss_pde:0.07730396, loss_ic:0.12358081, loss_bd:0.00137548\n",
      "epoch 23 loss_pde:0.07755559, loss_ic:0.12344499, loss_bd:0.00124499\n",
      "epoch 23 loss_pde:0.07722594, loss_ic:0.12324847, loss_bd:0.00126987\n",
      "epoch 23 loss_pde:0.07710464, loss_ic:0.12308480, loss_bd:0.00126248\n",
      "epoch 23 loss_pde:0.07695342, loss_ic:0.12288465, loss_bd:0.00128654\n",
      "epoch 23: loss 1.353663\n",
      "epoch 24 loss_pde:0.07671874, loss_ic:0.12271044, loss_bd:0.00129982\n",
      "epoch 24 loss_pde:0.07668103, loss_ic:0.12252285, loss_bd:0.00130860\n",
      "epoch 24 loss_pde:0.07626759, loss_ic:0.12235254, loss_bd:0.00134227\n",
      "epoch 24 loss_pde:0.07641812, loss_ic:0.12219073, loss_bd:0.00132279\n",
      "epoch 24 loss_pde:0.07598100, loss_ic:0.12203360, loss_bd:0.00137156\n",
      "epoch 24 loss_pde:0.07609437, loss_ic:0.12191620, loss_bd:0.00134833\n",
      "epoch 24 loss_pde:0.07580116, loss_ic:0.12179028, loss_bd:0.00138695\n",
      "epoch 24 loss_pde:0.07584200, loss_ic:0.12170535, loss_bd:0.00136558\n",
      "epoch 24 loss_pde:0.07565662, loss_ic:0.12160557, loss_bd:0.00139096\n",
      "epoch 24 loss_pde:0.07563776, loss_ic:0.12154179, loss_bd:0.00137613\n",
      "epoch 24 loss_pde:0.07552506, loss_ic:0.12146792, loss_bd:0.00138747\n",
      "epoch 24 loss_pde:0.07545688, loss_ic:0.12141849, loss_bd:0.00137846\n",
      "epoch 24 loss_pde:0.07536437, loss_ic:0.12136475, loss_bd:0.00137926\n",
      "epoch 24 loss_pde:0.07525165, loss_ic:0.12132513, loss_bd:0.00137250\n",
      "epoch 24 loss_pde:0.07514060, loss_ic:0.12128478, loss_bd:0.00136615\n",
      "epoch 24 loss_pde:0.07497118, loss_ic:0.12125237, loss_bd:0.00135851\n",
      "epoch 24 loss_pde:0.07481898, loss_ic:0.12121994, loss_bd:0.00134674\n",
      "epoch 24 loss_pde:0.07455777, loss_ic:0.12119546, loss_bd:0.00133705\n",
      "epoch 24 loss_pde:0.07435727, loss_ic:0.12116803, loss_bd:0.00131886\n",
      "epoch 24 loss_pde:0.07396425, loss_ic:0.12115487, loss_bd:0.00130331\n",
      "epoch 24: loss 1.316821\n",
      "epoch 25 loss_pde:0.07365219, loss_ic:0.12113339, loss_bd:0.00127997\n",
      "epoch 25 loss_pde:0.07314777, loss_ic:0.12113628, loss_bd:0.00124862\n",
      "epoch 25 loss_pde:0.07260840, loss_ic:0.12112386, loss_bd:0.00122376\n",
      "epoch 25 loss_pde:0.07199222, loss_ic:0.12114771, loss_bd:0.00116693\n",
      "epoch 25 loss_pde:0.07116225, loss_ic:0.12115333, loss_bd:0.00113008\n",
      "epoch 25 loss_pde:0.07032553, loss_ic:0.12119875, loss_bd:0.00105481\n",
      "epoch 25 loss_pde:0.06927337, loss_ic:0.12123464, loss_bd:0.00098932\n",
      "epoch 25 loss_pde:0.06827646, loss_ic:0.12128779, loss_bd:0.00091667\n",
      "epoch 25 loss_pde:0.06717089, loss_ic:0.12133928, loss_bd:0.00085356\n",
      "epoch 25 loss_pde:0.06612562, loss_ic:0.12139755, loss_bd:0.00079815\n",
      "epoch 25 loss_pde:0.06511996, loss_ic:0.12143762, loss_bd:0.00076856\n",
      "epoch 25 loss_pde:0.06417113, loss_ic:0.12148902, loss_bd:0.00073780\n",
      "epoch 25 loss_pde:0.06326859, loss_ic:0.12151342, loss_bd:0.00073667\n",
      "epoch 25 loss_pde:0.06243405, loss_ic:0.12155361, loss_bd:0.00072401\n",
      "epoch 25 loss_pde:0.06165710, loss_ic:0.12157242, loss_bd:0.00073235\n",
      "epoch 25 loss_pde:0.06091010, loss_ic:0.12159991, loss_bd:0.00073472\n",
      "epoch 25 loss_pde:0.06016981, loss_ic:0.12167162, loss_bd:0.00068196\n",
      "epoch 25 loss_pde:0.05937934, loss_ic:0.12169738, loss_bd:0.00069163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 25 loss_pde:0.05875183, loss_ic:0.12171331, loss_bd:0.00069980\n",
      "epoch 25 loss_pde:0.05804868, loss_ic:0.12173036, loss_bd:0.00071263\n",
      "epoch 25: loss 1.297786\n",
      "epoch 26 loss_pde:0.05739296, loss_ic:0.12175252, loss_bd:0.00071630\n",
      "epoch 26 loss_pde:0.05672709, loss_ic:0.12176739, loss_bd:0.00072856\n",
      "epoch 26 loss_pde:0.05608952, loss_ic:0.12179463, loss_bd:0.00072716\n",
      "epoch 26 loss_pde:0.05549971, loss_ic:0.12180507, loss_bd:0.00073815\n",
      "epoch 26 loss_pde:0.05488153, loss_ic:0.12183690, loss_bd:0.00073232\n",
      "epoch 26 loss_pde:0.05437143, loss_ic:0.12184034, loss_bd:0.00074338\n",
      "epoch 26 loss_pde:0.05375747, loss_ic:0.12187228, loss_bd:0.00073751\n",
      "epoch 26 loss_pde:0.05332166, loss_ic:0.12187311, loss_bd:0.00074415\n",
      "epoch 26 loss_pde:0.05266096, loss_ic:0.12190144, loss_bd:0.00074533\n",
      "epoch 26 loss_pde:0.05219039, loss_ic:0.12192371, loss_bd:0.00073617\n",
      "epoch 26 loss_pde:0.05138868, loss_ic:0.12194823, loss_bd:0.00075375\n",
      "epoch 26 loss_pde:0.05089923, loss_ic:0.12199590, loss_bd:0.00071708\n",
      "epoch 26 loss_pde:0.05030754, loss_ic:0.12199764, loss_bd:0.00073413\n",
      "epoch 26 loss_pde:0.04985054, loss_ic:0.12203166, loss_bd:0.00071459\n",
      "epoch 26 loss_pde:0.04941466, loss_ic:0.12203795, loss_bd:0.00071651\n",
      "epoch 26 loss_pde:0.04888441, loss_ic:0.12206872, loss_bd:0.00070201\n",
      "epoch 26 loss_pde:0.04840708, loss_ic:0.12207186, loss_bd:0.00070342\n",
      "epoch 26 loss_pde:0.04758232, loss_ic:0.12214124, loss_bd:0.00065849\n",
      "epoch 26 loss_pde:0.04703809, loss_ic:0.12215372, loss_bd:0.00065352\n",
      "epoch 26 loss_pde:0.04652065, loss_ic:0.12216724, loss_bd:0.00064451\n",
      "epoch 26: loss 1.282081\n",
      "epoch 27 loss_pde:0.04594699, loss_ic:0.12218567, loss_bd:0.00063160\n",
      "epoch 27 loss_pde:0.04534338, loss_ic:0.12219276, loss_bd:0.00062514\n",
      "epoch 27 loss_pde:0.04478678, loss_ic:0.12221615, loss_bd:0.00060152\n",
      "epoch 27 loss_pde:0.04417440, loss_ic:0.12220904, loss_bd:0.00060756\n",
      "epoch 27 loss_pde:0.04367730, loss_ic:0.12224802, loss_bd:0.00056196\n",
      "epoch 27 loss_pde:0.04302804, loss_ic:0.12223315, loss_bd:0.00057341\n",
      "epoch 27 loss_pde:0.04259562, loss_ic:0.12225625, loss_bd:0.00054276\n",
      "epoch 27 loss_pde:0.04213257, loss_ic:0.12225086, loss_bd:0.00054186\n",
      "epoch 27 loss_pde:0.04173576, loss_ic:0.12225894, loss_bd:0.00052772\n",
      "epoch 27 loss_pde:0.04132506, loss_ic:0.12225741, loss_bd:0.00052391\n",
      "epoch 27 loss_pde:0.04093261, loss_ic:0.12226406, loss_bd:0.00051018\n",
      "epoch 27 loss_pde:0.04050089, loss_ic:0.12225953, loss_bd:0.00050986\n",
      "epoch 27 loss_pde:0.04009907, loss_ic:0.12227006, loss_bd:0.00049396\n",
      "epoch 27 loss_pde:0.03968361, loss_ic:0.12226253, loss_bd:0.00049773\n",
      "epoch 27 loss_pde:0.03927023, loss_ic:0.12227921, loss_bd:0.00048152\n",
      "epoch 27 loss_pde:0.03890519, loss_ic:0.12226587, loss_bd:0.00049049\n",
      "epoch 27 loss_pde:0.03847567, loss_ic:0.12229280, loss_bd:0.00046966\n",
      "epoch 27 loss_pde:0.03811208, loss_ic:0.12228853, loss_bd:0.00047512\n",
      "epoch 27 loss_pde:0.03779085, loss_ic:0.12229939, loss_bd:0.00046758\n",
      "epoch 27 loss_pde:0.03741144, loss_ic:0.12231334, loss_bd:0.00046314\n",
      "epoch 27: loss 1.274120\n",
      "epoch 28 loss_pde:0.03714429, loss_ic:0.12231649, loss_bd:0.00046455\n",
      "epoch 28 loss_pde:0.03687334, loss_ic:0.12232898, loss_bd:0.00045780\n",
      "epoch 28 loss_pde:0.03658527, loss_ic:0.12232950, loss_bd:0.00046445\n",
      "epoch 28 loss_pde:0.03637161, loss_ic:0.12234473, loss_bd:0.00044929\n",
      "epoch 28 loss_pde:0.03603958, loss_ic:0.12234970, loss_bd:0.00045653\n",
      "epoch 28 loss_pde:0.03578770, loss_ic:0.12236081, loss_bd:0.00045224\n",
      "epoch 28 loss_pde:0.03544975, loss_ic:0.12237138, loss_bd:0.00045697\n",
      "epoch 28 loss_pde:0.03520957, loss_ic:0.12237838, loss_bd:0.00045503\n",
      "epoch 28 loss_pde:0.03483102, loss_ic:0.12239724, loss_bd:0.00045533\n",
      "epoch 28 loss_pde:0.03454775, loss_ic:0.12240322, loss_bd:0.00046014\n",
      "epoch 28 loss_pde:0.03414464, loss_ic:0.12242433, loss_bd:0.00045965\n",
      "epoch 28 loss_pde:0.03381590, loss_ic:0.12243230, loss_bd:0.00046554\n",
      "epoch 28 loss_pde:0.03346723, loss_ic:0.12245549, loss_bd:0.00045618\n",
      "epoch 28 loss_pde:0.03306090, loss_ic:0.12245931, loss_bd:0.00046878\n",
      "epoch 28 loss_pde:0.03265717, loss_ic:0.12248047, loss_bd:0.00046542\n",
      "epoch 28 loss_pde:0.03224596, loss_ic:0.12248818, loss_bd:0.00047582\n",
      "epoch 28 loss_pde:0.03182897, loss_ic:0.12250835, loss_bd:0.00047428\n",
      "epoch 28 loss_pde:0.03137318, loss_ic:0.12251770, loss_bd:0.00048602\n",
      "epoch 28 loss_pde:0.03091560, loss_ic:0.12254020, loss_bd:0.00048317\n",
      "epoch 28 loss_pde:0.03047066, loss_ic:0.12255419, loss_bd:0.00049005\n",
      "epoch 28: loss 1.264955\n",
      "epoch 29 loss_pde:0.03002197, loss_ic:0.12256715, loss_bd:0.00049917\n",
      "epoch 29 loss_pde:0.02962640, loss_ic:0.12257964, loss_bd:0.00050442\n",
      "epoch 29 loss_pde:0.02922768, loss_ic:0.12258739, loss_bd:0.00051540\n",
      "epoch 29 loss_pde:0.02887863, loss_ic:0.12259659, loss_bd:0.00052144\n",
      "epoch 29 loss_pde:0.02853784, loss_ic:0.12260010, loss_bd:0.00053275\n",
      "epoch 29 loss_pde:0.02822831, loss_ic:0.12260600, loss_bd:0.00053914\n",
      "epoch 29 loss_pde:0.02791813, loss_ic:0.12260564, loss_bd:0.00055158\n",
      "epoch 29 loss_pde:0.02763919, loss_ic:0.12260831, loss_bd:0.00055776\n",
      "epoch 29 loss_pde:0.02735135, loss_ic:0.12260395, loss_bd:0.00057120\n",
      "epoch 29 loss_pde:0.02710422, loss_ic:0.12260126, loss_bd:0.00057829\n",
      "epoch 29 loss_pde:0.02684206, loss_ic:0.12259375, loss_bd:0.00059084\n",
      "epoch 29 loss_pde:0.02662724, loss_ic:0.12258297, loss_bd:0.00060084\n",
      "epoch 29 loss_pde:0.02639026, loss_ic:0.12257311, loss_bd:0.00061097\n",
      "epoch 29 loss_pde:0.02619459, loss_ic:0.12255247, loss_bd:0.00062474\n",
      "epoch 29 loss_pde:0.02597600, loss_ic:0.12253817, loss_bd:0.00063287\n",
      "epoch 29 loss_pde:0.02573357, loss_ic:0.12250916, loss_bd:0.00064944\n",
      "epoch 29 loss_pde:0.02552245, loss_ic:0.12248320, loss_bd:0.00065992\n",
      "epoch 29 loss_pde:0.02522851, loss_ic:0.12245484, loss_bd:0.00067267\n",
      "epoch 29 loss_pde:0.02500025, loss_ic:0.12240399, loss_bd:0.00069259\n",
      "epoch 29 loss_pde:0.02477693, loss_ic:0.12236769, loss_bd:0.00069058\n",
      "epoch 29: loss 1.260685\n",
      "epoch 30 loss_pde:0.02460522, loss_ic:0.12229056, loss_bd:0.00071194\n",
      "epoch 30 loss_pde:0.02449900, loss_ic:0.12224553, loss_bd:0.00069628\n",
      "epoch 30 loss_pde:0.02449132, loss_ic:0.12215529, loss_bd:0.00070633\n",
      "epoch 30 loss_pde:0.02450480, loss_ic:0.12210432, loss_bd:0.00068516\n",
      "epoch 30 loss_pde:0.02463759, loss_ic:0.12201545, loss_bd:0.00068393\n",
      "epoch 30 loss_pde:0.02475441, loss_ic:0.12195716, loss_bd:0.00066363\n",
      "epoch 30 loss_pde:0.02496022, loss_ic:0.12187235, loss_bd:0.00065542\n",
      "epoch 30 loss_pde:0.02514071, loss_ic:0.12180674, loss_bd:0.00063464\n",
      "epoch 30 loss_pde:0.02539791, loss_ic:0.12171879, loss_bd:0.00062341\n",
      "epoch 30 loss_pde:0.02560572, loss_ic:0.12164887, loss_bd:0.00060067\n",
      "epoch 30 loss_pde:0.02589555, loss_ic:0.12155356, loss_bd:0.00058937\n",
      "epoch 30 loss_pde:0.02611434, loss_ic:0.12148036, loss_bd:0.00056653\n",
      "epoch 30 loss_pde:0.02640049, loss_ic:0.12138199, loss_bd:0.00055698\n",
      "epoch 30 loss_pde:0.02661176, loss_ic:0.12130661, loss_bd:0.00053720\n",
      "epoch 30 loss_pde:0.02687515, loss_ic:0.12120954, loss_bd:0.00053134\n",
      "epoch 30 loss_pde:0.02707612, loss_ic:0.12113640, loss_bd:0.00051545\n",
      "epoch 30 loss_pde:0.02732461, loss_ic:0.12104553, loss_bd:0.00051339\n",
      "epoch 30 loss_pde:0.02753404, loss_ic:0.12098004, loss_bd:0.00049858\n",
      "epoch 30 loss_pde:0.02777874, loss_ic:0.12089708, loss_bd:0.00050000\n",
      "epoch 30 loss_pde:0.02799394, loss_ic:0.12084093, loss_bd:0.00048650\n",
      "epoch 30: loss 1.254630\n",
      "epoch 31 loss_pde:0.02823521, loss_ic:0.12076600, loss_bd:0.00049169\n",
      "epoch 31 loss_pde:0.02847605, loss_ic:0.12071918, loss_bd:0.00047625\n",
      "epoch 31 loss_pde:0.02870735, loss_ic:0.12065389, loss_bd:0.00048214\n",
      "epoch 31 loss_pde:0.02895321, loss_ic:0.12061270, loss_bd:0.00046868\n",
      "epoch 31 loss_pde:0.02917537, loss_ic:0.12055735, loss_bd:0.00047309\n",
      "epoch 31 loss_pde:0.02941675, loss_ic:0.12051936, loss_bd:0.00046222\n",
      "epoch 31 loss_pde:0.02964111, loss_ic:0.12046892, loss_bd:0.00046566\n",
      "epoch 31 loss_pde:0.02988272, loss_ic:0.12043107, loss_bd:0.00045613\n",
      "epoch 31 loss_pde:0.03009597, loss_ic:0.12038592, loss_bd:0.00045755\n",
      "epoch 31 loss_pde:0.03031387, loss_ic:0.12034680, loss_bd:0.00045381\n",
      "epoch 31 loss_pde:0.03050823, loss_ic:0.12030663, loss_bd:0.00045356\n",
      "epoch 31 loss_pde:0.03073373, loss_ic:0.12026341, loss_bd:0.00045297\n",
      "epoch 31 loss_pde:0.03092161, loss_ic:0.12022534, loss_bd:0.00045035\n",
      "epoch 31 loss_pde:0.03119921, loss_ic:0.12016972, loss_bd:0.00045443\n",
      "epoch 31 loss_pde:0.03142249, loss_ic:0.12013195, loss_bd:0.00044814\n",
      "epoch 31 loss_pde:0.03163216, loss_ic:0.12008177, loss_bd:0.00045561\n",
      "epoch 31 loss_pde:0.03187717, loss_ic:0.12004524, loss_bd:0.00044684\n",
      "epoch 31 loss_pde:0.03201696, loss_ic:0.11999259, loss_bd:0.00046207\n",
      "epoch 31 loss_pde:0.03229138, loss_ic:0.11995292, loss_bd:0.00045084\n",
      "epoch 31 loss_pde:0.03245471, loss_ic:0.11990333, loss_bd:0.00046063\n",
      "epoch 31: loss 1.240812\n",
      "epoch 32 loss_pde:0.03269514, loss_ic:0.11985856, loss_bd:0.00045869\n",
      "epoch 32 loss_pde:0.03278851, loss_ic:0.11981875, loss_bd:0.00046417\n",
      "epoch 32 loss_pde:0.03301570, loss_ic:0.11976302, loss_bd:0.00046981\n",
      "epoch 32 loss_pde:0.03320204, loss_ic:0.11972055, loss_bd:0.00046904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 32 loss_pde:0.03343113, loss_ic:0.11966378, loss_bd:0.00047612\n",
      "epoch 32 loss_pde:0.03363992, loss_ic:0.11962013, loss_bd:0.00047428\n",
      "epoch 32 loss_pde:0.03384852, loss_ic:0.11956657, loss_bd:0.00047989\n",
      "epoch 32 loss_pde:0.03410880, loss_ic:0.11950735, loss_bd:0.00048517\n",
      "epoch 32 loss_pde:0.03435395, loss_ic:0.11945811, loss_bd:0.00048264\n",
      "epoch 32 loss_pde:0.03457772, loss_ic:0.11939371, loss_bd:0.00049850\n",
      "epoch 32 loss_pde:0.03484372, loss_ic:0.11935297, loss_bd:0.00048809\n",
      "epoch 32 loss_pde:0.03506772, loss_ic:0.11928903, loss_bd:0.00050274\n",
      "epoch 32 loss_pde:0.03531113, loss_ic:0.11924925, loss_bd:0.00049580\n",
      "epoch 32 loss_pde:0.03554065, loss_ic:0.11919570, loss_bd:0.00050257\n",
      "epoch 32 loss_pde:0.03576732, loss_ic:0.11915394, loss_bd:0.00050005\n",
      "epoch 32 loss_pde:0.03600129, loss_ic:0.11910329, loss_bd:0.00050358\n",
      "epoch 32 loss_pde:0.03624133, loss_ic:0.11905751, loss_bd:0.00050142\n",
      "epoch 32 loss_pde:0.03649230, loss_ic:0.11900220, loss_bd:0.00050439\n",
      "epoch 32 loss_pde:0.03673388, loss_ic:0.11895555, loss_bd:0.00050053\n",
      "epoch 32 loss_pde:0.03701994, loss_ic:0.11889642, loss_bd:0.00050220\n",
      "epoch 32: loss 1.235868\n",
      "epoch 33 loss_pde:0.03721882, loss_ic:0.11885489, loss_bd:0.00049824\n",
      "epoch 33 loss_pde:0.03746865, loss_ic:0.11879911, loss_bd:0.00050073\n",
      "epoch 33 loss_pde:0.03767753, loss_ic:0.11875970, loss_bd:0.00049347\n",
      "epoch 33 loss_pde:0.03787822, loss_ic:0.11870441, loss_bd:0.00049944\n",
      "epoch 33 loss_pde:0.03810377, loss_ic:0.11866920, loss_bd:0.00048571\n",
      "epoch 33 loss_pde:0.03825672, loss_ic:0.11861635, loss_bd:0.00049297\n",
      "epoch 33 loss_pde:0.03845748, loss_ic:0.11858389, loss_bd:0.00047987\n",
      "epoch 33 loss_pde:0.03858032, loss_ic:0.11854349, loss_bd:0.00048012\n",
      "epoch 33 loss_pde:0.03874886, loss_ic:0.11850663, loss_bd:0.00047388\n",
      "epoch 33 loss_pde:0.03887984, loss_ic:0.11847476, loss_bd:0.00046186\n",
      "epoch 33 loss_pde:0.03895757, loss_ic:0.11843356, loss_bd:0.00046715\n",
      "epoch 33 loss_pde:0.03906338, loss_ic:0.11840674, loss_bd:0.00045519\n",
      "epoch 33 loss_pde:0.03904337, loss_ic:0.11838488, loss_bd:0.00045125\n",
      "epoch 33 loss_pde:0.03913309, loss_ic:0.11833616, loss_bd:0.00045361\n",
      "epoch 33 loss_pde:0.03910257, loss_ic:0.11832472, loss_bd:0.00044157\n",
      "epoch 33 loss_pde:0.03905970, loss_ic:0.11829658, loss_bd:0.00044500\n",
      "epoch 33 loss_pde:0.03903665, loss_ic:0.11828462, loss_bd:0.00043428\n",
      "epoch 33 loss_pde:0.03894080, loss_ic:0.11826412, loss_bd:0.00043624\n",
      "epoch 33 loss_pde:0.03888536, loss_ic:0.11825211, loss_bd:0.00042556\n",
      "epoch 33 loss_pde:0.03871342, loss_ic:0.11823613, loss_bd:0.00042589\n",
      "epoch 33: loss 1.230750\n",
      "epoch 34 loss_pde:0.03859033, loss_ic:0.11822207, loss_bd:0.00041990\n",
      "epoch 34 loss_pde:0.03840527, loss_ic:0.11821016, loss_bd:0.00041624\n",
      "epoch 34 loss_pde:0.03823040, loss_ic:0.11819992, loss_bd:0.00041455\n",
      "epoch 34 loss_pde:0.03798831, loss_ic:0.11819586, loss_bd:0.00041039\n",
      "epoch 34 loss_pde:0.03774513, loss_ic:0.11818612, loss_bd:0.00041303\n",
      "epoch 34 loss_pde:0.03745614, loss_ic:0.11818805, loss_bd:0.00040774\n",
      "epoch 34 loss_pde:0.03716055, loss_ic:0.11818214, loss_bd:0.00040991\n",
      "epoch 34 loss_pde:0.03661780, loss_ic:0.11821002, loss_bd:0.00039727\n",
      "epoch 34 loss_pde:0.03624696, loss_ic:0.11821247, loss_bd:0.00039743\n",
      "epoch 34 loss_pde:0.03587132, loss_ic:0.11822136, loss_bd:0.00039499\n",
      "epoch 34 loss_pde:0.03549231, loss_ic:0.11819703, loss_bd:0.00043682\n",
      "epoch 34 loss_pde:0.03514981, loss_ic:0.11820117, loss_bd:0.00043711\n",
      "epoch 34 loss_pde:0.03479346, loss_ic:0.11822385, loss_bd:0.00041787\n",
      "epoch 34 loss_pde:0.03427513, loss_ic:0.11821271, loss_bd:0.00044237\n",
      "epoch 34 loss_pde:0.03388397, loss_ic:0.11823966, loss_bd:0.00042542\n",
      "epoch 34 loss_pde:0.03352044, loss_ic:0.11824930, loss_bd:0.00042633\n",
      "epoch 34 loss_pde:0.03312582, loss_ic:0.11826834, loss_bd:0.00042106\n",
      "epoch 34 loss_pde:0.03270445, loss_ic:0.11829138, loss_bd:0.00040358\n",
      "epoch 34 loss_pde:0.03204437, loss_ic:0.11832365, loss_bd:0.00039656\n",
      "epoch 34 loss_pde:0.03133198, loss_ic:0.11835306, loss_bd:0.00039203\n",
      "epoch 34: loss 1.225010\n",
      "epoch 35 loss_pde:0.03044838, loss_ic:0.11840125, loss_bd:0.00038068\n",
      "epoch 35 loss_pde:0.02957733, loss_ic:0.11843234, loss_bd:0.00037848\n",
      "epoch 35 loss_pde:0.02861781, loss_ic:0.11849425, loss_bd:0.00035844\n",
      "epoch 35 loss_pde:0.02767602, loss_ic:0.11852849, loss_bd:0.00035876\n",
      "epoch 35 loss_pde:0.02681221, loss_ic:0.11858641, loss_bd:0.00033801\n",
      "epoch 35 loss_pde:0.02593775, loss_ic:0.11861791, loss_bd:0.00034094\n",
      "epoch 35 loss_pde:0.02515595, loss_ic:0.11866958, loss_bd:0.00032254\n",
      "epoch 35 loss_pde:0.02438619, loss_ic:0.11869679, loss_bd:0.00032656\n",
      "epoch 35 loss_pde:0.02369419, loss_ic:0.11873921, loss_bd:0.00031414\n",
      "epoch 35 loss_pde:0.02303584, loss_ic:0.11876199, loss_bd:0.00031854\n",
      "epoch 35 loss_pde:0.02241246, loss_ic:0.11879791, loss_bd:0.00031075\n",
      "epoch 35 loss_pde:0.02183685, loss_ic:0.11881539, loss_bd:0.00031672\n",
      "epoch 35 loss_pde:0.02121408, loss_ic:0.11885308, loss_bd:0.00031014\n",
      "epoch 35 loss_pde:0.02065421, loss_ic:0.11887218, loss_bd:0.00031440\n",
      "epoch 35 loss_pde:0.02013117, loss_ic:0.11889805, loss_bd:0.00031511\n",
      "epoch 35 loss_pde:0.01968426, loss_ic:0.11891897, loss_bd:0.00031572\n",
      "epoch 35 loss_pde:0.01927149, loss_ic:0.11893437, loss_bd:0.00032052\n",
      "epoch 35 loss_pde:0.01886083, loss_ic:0.11895797, loss_bd:0.00031774\n",
      "epoch 35 loss_pde:0.01847915, loss_ic:0.11896752, loss_bd:0.00032665\n",
      "epoch 35 loss_pde:0.01808818, loss_ic:0.11899390, loss_bd:0.00032101\n",
      "epoch 35: loss 1.218268\n",
      "epoch 36 loss_pde:0.01773767, loss_ic:0.11900146, loss_bd:0.00033078\n",
      "epoch 36 loss_pde:0.01738908, loss_ic:0.11902482, loss_bd:0.00032650\n",
      "epoch 36 loss_pde:0.01707497, loss_ic:0.11903423, loss_bd:0.00033299\n",
      "epoch 36 loss_pde:0.01676347, loss_ic:0.11905343, loss_bd:0.00033044\n",
      "epoch 36 loss_pde:0.01645983, loss_ic:0.11906389, loss_bd:0.00033474\n",
      "epoch 36 loss_pde:0.01614748, loss_ic:0.11908282, loss_bd:0.00033032\n",
      "epoch 36 loss_pde:0.01581306, loss_ic:0.11908920, loss_bd:0.00033758\n",
      "epoch 36 loss_pde:0.01548221, loss_ic:0.11910912, loss_bd:0.00033042\n",
      "epoch 36 loss_pde:0.01518025, loss_ic:0.11911422, loss_bd:0.00033483\n",
      "epoch 36 loss_pde:0.01488244, loss_ic:0.11912821, loss_bd:0.00033088\n",
      "epoch 36 loss_pde:0.01462269, loss_ic:0.11913136, loss_bd:0.00033335\n",
      "epoch 36 loss_pde:0.01435245, loss_ic:0.11914129, loss_bd:0.00032999\n",
      "epoch 36 loss_pde:0.01412732, loss_ic:0.11914128, loss_bd:0.00033149\n",
      "epoch 36 loss_pde:0.01390176, loss_ic:0.11914426, loss_bd:0.00033054\n",
      "epoch 36 loss_pde:0.01370488, loss_ic:0.11914689, loss_bd:0.00032799\n",
      "epoch 36 loss_pde:0.01354361, loss_ic:0.11914101, loss_bd:0.00033081\n",
      "epoch 36 loss_pde:0.01338281, loss_ic:0.11914534, loss_bd:0.00032437\n",
      "epoch 36 loss_pde:0.01328395, loss_ic:0.11913442, loss_bd:0.00032606\n",
      "epoch 36 loss_pde:0.01314752, loss_ic:0.11913101, loss_bd:0.00032371\n",
      "epoch 36 loss_pde:0.01306383, loss_ic:0.11912443, loss_bd:0.00031942\n",
      "epoch 36: loss 1.211060\n",
      "epoch 37 loss_pde:0.01293766, loss_ic:0.11909258, loss_bd:0.00034058\n",
      "epoch 37 loss_pde:0.01285746, loss_ic:0.11909219, loss_bd:0.00033011\n",
      "epoch 37 loss_pde:0.01282250, loss_ic:0.11907668, loss_bd:0.00033163\n",
      "epoch 37 loss_pde:0.01279239, loss_ic:0.11906939, loss_bd:0.00032421\n",
      "epoch 37 loss_pde:0.01275253, loss_ic:0.11905186, loss_bd:0.00032522\n",
      "epoch 37 loss_pde:0.01275605, loss_ic:0.11903778, loss_bd:0.00031350\n",
      "epoch 37 loss_pde:0.01274041, loss_ic:0.11901222, loss_bd:0.00031431\n",
      "epoch 37 loss_pde:0.01274431, loss_ic:0.11899380, loss_bd:0.00030780\n",
      "epoch 37 loss_pde:0.01275105, loss_ic:0.11896835, loss_bd:0.00030514\n",
      "epoch 37 loss_pde:0.01276934, loss_ic:0.11894116, loss_bd:0.00029946\n",
      "epoch 37 loss_pde:0.01276674, loss_ic:0.11890973, loss_bd:0.00029406\n",
      "epoch 37 loss_pde:0.01279410, loss_ic:0.11886662, loss_bd:0.00029054\n",
      "epoch 37 loss_pde:0.01277881, loss_ic:0.11882707, loss_bd:0.00028107\n",
      "epoch 37 loss_pde:0.01283294, loss_ic:0.11876536, loss_bd:0.00027929\n",
      "epoch 37 loss_pde:0.01282684, loss_ic:0.11871409, loss_bd:0.00026896\n",
      "epoch 37 loss_pde:0.01290559, loss_ic:0.11864130, loss_bd:0.00026306\n",
      "epoch 37 loss_pde:0.01292283, loss_ic:0.11857384, loss_bd:0.00025744\n",
      "epoch 37 loss_pde:0.01294719, loss_ic:0.11850355, loss_bd:0.00024888\n",
      "epoch 37 loss_pde:0.01295231, loss_ic:0.11841615, loss_bd:0.00025052\n",
      "epoch 37 loss_pde:0.01296096, loss_ic:0.11834560, loss_bd:0.00023029\n",
      "epoch 37: loss 1.207269\n",
      "epoch 38 loss_pde:0.01293743, loss_ic:0.11821868, loss_bd:0.00023993\n",
      "epoch 38 loss_pde:0.01292926, loss_ic:0.11814298, loss_bd:0.00022010\n",
      "epoch 38 loss_pde:0.01288446, loss_ic:0.11804900, loss_bd:0.00022116\n",
      "epoch 38 loss_pde:0.01282704, loss_ic:0.11796202, loss_bd:0.00022284\n",
      "epoch 38 loss_pde:0.01276830, loss_ic:0.11787976, loss_bd:0.00021161\n",
      "epoch 38 loss_pde:0.01266942, loss_ic:0.11777875, loss_bd:0.00022456\n",
      "epoch 38 loss_pde:0.01263706, loss_ic:0.11769677, loss_bd:0.00021788\n",
      "epoch 38 loss_pde:0.01255473, loss_ic:0.11759423, loss_bd:0.00023332\n",
      "epoch 38 loss_pde:0.01254097, loss_ic:0.11751608, loss_bd:0.00023206\n",
      "epoch 38 loss_pde:0.01251384, loss_ic:0.11742757, loss_bd:0.00024547\n",
      "epoch 38 loss_pde:0.01253136, loss_ic:0.11735607, loss_bd:0.00024859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 38 loss_pde:0.01256353, loss_ic:0.11727889, loss_bd:0.00025790\n",
      "epoch 38 loss_pde:0.01262232, loss_ic:0.11720756, loss_bd:0.00026221\n",
      "epoch 38 loss_pde:0.01266709, loss_ic:0.11713400, loss_bd:0.00027063\n",
      "epoch 38 loss_pde:0.01276497, loss_ic:0.11706453, loss_bd:0.00027435\n",
      "epoch 38 loss_pde:0.01282795, loss_ic:0.11699339, loss_bd:0.00028533\n",
      "epoch 38 loss_pde:0.01296308, loss_ic:0.11693614, loss_bd:0.00028166\n",
      "epoch 38 loss_pde:0.01305132, loss_ic:0.11687295, loss_bd:0.00029396\n",
      "epoch 38 loss_pde:0.01318020, loss_ic:0.11682513, loss_bd:0.00029249\n",
      "epoch 38 loss_pde:0.01327659, loss_ic:0.11677878, loss_bd:0.00029797\n",
      "epoch 38: loss 1.197523\n",
      "epoch 39 loss_pde:0.01339402, loss_ic:0.11673819, loss_bd:0.00029884\n",
      "epoch 39 loss_pde:0.01349262, loss_ic:0.11670204, loss_bd:0.00030056\n",
      "epoch 39 loss_pde:0.01360811, loss_ic:0.11666644, loss_bd:0.00030069\n",
      "epoch 39 loss_pde:0.01370554, loss_ic:0.11663078, loss_bd:0.00029951\n",
      "epoch 39 loss_pde:0.01385250, loss_ic:0.11659183, loss_bd:0.00029830\n",
      "epoch 39 loss_pde:0.01397360, loss_ic:0.11655660, loss_bd:0.00030017\n",
      "epoch 39 loss_pde:0.01409646, loss_ic:0.11652687, loss_bd:0.00029772\n",
      "epoch 39 loss_pde:0.01422419, loss_ic:0.11649333, loss_bd:0.00029901\n",
      "epoch 39 loss_pde:0.01435119, loss_ic:0.11646469, loss_bd:0.00029644\n",
      "epoch 39 loss_pde:0.01448427, loss_ic:0.11643292, loss_bd:0.00029666\n",
      "epoch 39 loss_pde:0.01461623, loss_ic:0.11640363, loss_bd:0.00029478\n",
      "epoch 39 loss_pde:0.01475961, loss_ic:0.11637170, loss_bd:0.00029421\n",
      "epoch 39 loss_pde:0.01489493, loss_ic:0.11633801, loss_bd:0.00029585\n",
      "epoch 39 loss_pde:0.01508748, loss_ic:0.11630423, loss_bd:0.00028959\n",
      "epoch 39 loss_pde:0.01531490, loss_ic:0.11622872, loss_bd:0.00031410\n",
      "epoch 39 loss_pde:0.01549844, loss_ic:0.11619796, loss_bd:0.00030859\n",
      "epoch 39 loss_pde:0.01566787, loss_ic:0.11616996, loss_bd:0.00030324\n",
      "epoch 39 loss_pde:0.01583666, loss_ic:0.11613889, loss_bd:0.00030083\n",
      "epoch 39 loss_pde:0.01602163, loss_ic:0.11610980, loss_bd:0.00029361\n",
      "epoch 39 loss_pde:0.01620520, loss_ic:0.11607425, loss_bd:0.00029331\n",
      "epoch 39: loss 1.183764\n",
      "epoch 40 loss_pde:0.01640542, loss_ic:0.11604454, loss_bd:0.00028473\n",
      "epoch 40 loss_pde:0.01659773, loss_ic:0.11600731, loss_bd:0.00028581\n",
      "epoch 40 loss_pde:0.01679487, loss_ic:0.11597797, loss_bd:0.00027858\n",
      "epoch 40 loss_pde:0.01698568, loss_ic:0.11594203, loss_bd:0.00027988\n",
      "epoch 40 loss_pde:0.01717241, loss_ic:0.11591388, loss_bd:0.00027459\n",
      "epoch 40 loss_pde:0.01735613, loss_ic:0.11588022, loss_bd:0.00027576\n",
      "epoch 40 loss_pde:0.01753119, loss_ic:0.11585355, loss_bd:0.00027212\n",
      "epoch 40 loss_pde:0.01770651, loss_ic:0.11582232, loss_bd:0.00027297\n",
      "epoch 40 loss_pde:0.01787446, loss_ic:0.11579663, loss_bd:0.00027069\n",
      "epoch 40 loss_pde:0.01803204, loss_ic:0.11577026, loss_bd:0.00027043\n",
      "epoch 40 loss_pde:0.01817898, loss_ic:0.11574630, loss_bd:0.00027047\n",
      "epoch 40 loss_pde:0.01831368, loss_ic:0.11572539, loss_bd:0.00026890\n",
      "epoch 40 loss_pde:0.01843998, loss_ic:0.11570238, loss_bd:0.00027105\n",
      "epoch 40 loss_pde:0.01856494, loss_ic:0.11568433, loss_bd:0.00026822\n",
      "epoch 40 loss_pde:0.01867826, loss_ic:0.11566054, loss_bd:0.00027140\n",
      "epoch 40 loss_pde:0.01887950, loss_ic:0.11562786, loss_bd:0.00027203\n",
      "epoch 40 loss_pde:0.01896836, loss_ic:0.11561435, loss_bd:0.00026882\n",
      "epoch 40 loss_pde:0.01905007, loss_ic:0.11559483, loss_bd:0.00027271\n",
      "epoch 40 loss_pde:0.01911922, loss_ic:0.11558331, loss_bd:0.00026970\n",
      "epoch 40 loss_pde:0.01918142, loss_ic:0.11556549, loss_bd:0.00027324\n",
      "epoch 40: loss 1.179698\n",
      "epoch 41 loss_pde:0.01923449, loss_ic:0.11555466, loss_bd:0.00027053\n",
      "epoch 41 loss_pde:0.01927329, loss_ic:0.11553881, loss_bd:0.00027332\n",
      "epoch 41 loss_pde:0.01930729, loss_ic:0.11552824, loss_bd:0.00027142\n",
      "epoch 41 loss_pde:0.01931800, loss_ic:0.11551529, loss_bd:0.00027304\n",
      "epoch 41 loss_pde:0.01932503, loss_ic:0.11550531, loss_bd:0.00027213\n",
      "epoch 41 loss_pde:0.01930469, loss_ic:0.11549490, loss_bd:0.00027303\n",
      "epoch 41 loss_pde:0.01927886, loss_ic:0.11548606, loss_bd:0.00027259\n",
      "epoch 41 loss_pde:0.01922387, loss_ic:0.11547729, loss_bd:0.00027348\n",
      "epoch 41 loss_pde:0.01916077, loss_ic:0.11547041, loss_bd:0.00027290\n",
      "epoch 41 loss_pde:0.01907349, loss_ic:0.11546180, loss_bd:0.00027477\n",
      "epoch 41 loss_pde:0.01897158, loss_ic:0.11546022, loss_bd:0.00027152\n",
      "epoch 41 loss_pde:0.01885797, loss_ic:0.11545047, loss_bd:0.00027634\n",
      "epoch 41 loss_pde:0.01873871, loss_ic:0.11545366, loss_bd:0.00027056\n",
      "epoch 41 loss_pde:0.01861364, loss_ic:0.11544620, loss_bd:0.00027526\n",
      "epoch 41 loss_pde:0.01849506, loss_ic:0.11544966, loss_bd:0.00027043\n",
      "epoch 41 loss_pde:0.01837923, loss_ic:0.11544468, loss_bd:0.00027375\n",
      "epoch 41 loss_pde:0.01827289, loss_ic:0.11544751, loss_bd:0.00026968\n",
      "epoch 41 loss_pde:0.01817320, loss_ic:0.11544377, loss_bd:0.00027150\n",
      "epoch 41 loss_pde:0.01809059, loss_ic:0.11544511, loss_bd:0.00026797\n",
      "epoch 41 loss_pde:0.01801175, loss_ic:0.11544143, loss_bd:0.00026933\n",
      "epoch 41: loss 1.177487\n",
      "epoch 42 loss_pde:0.01795924, loss_ic:0.11544112, loss_bd:0.00026571\n",
      "epoch 42 loss_pde:0.01790389, loss_ic:0.11543775, loss_bd:0.00026598\n",
      "epoch 42 loss_pde:0.01787917, loss_ic:0.11543486, loss_bd:0.00026352\n",
      "epoch 42 loss_pde:0.01784912, loss_ic:0.11543190, loss_bd:0.00026226\n",
      "epoch 42 loss_pde:0.01784480, loss_ic:0.11542731, loss_bd:0.00026117\n",
      "epoch 42 loss_pde:0.01784083, loss_ic:0.11542437, loss_bd:0.00025896\n",
      "epoch 42 loss_pde:0.01786482, loss_ic:0.11541685, loss_bd:0.00025901\n",
      "epoch 42 loss_pde:0.01787727, loss_ic:0.11541622, loss_bd:0.00025377\n",
      "epoch 42 loss_pde:0.01791497, loss_ic:0.11540557, loss_bd:0.00025637\n",
      "epoch 42 loss_pde:0.01794453, loss_ic:0.11540331, loss_bd:0.00025233\n",
      "epoch 42 loss_pde:0.01797677, loss_ic:0.11539751, loss_bd:0.00025176\n",
      "epoch 42 loss_pde:0.01801630, loss_ic:0.11539325, loss_bd:0.00024945\n",
      "epoch 42 loss_pde:0.01805734, loss_ic:0.11538947, loss_bd:0.00024643\n",
      "epoch 42 loss_pde:0.01811974, loss_ic:0.11538228, loss_bd:0.00024475\n",
      "epoch 42 loss_pde:0.01817840, loss_ic:0.11537000, loss_bd:0.00024861\n",
      "epoch 42 loss_pde:0.01826687, loss_ic:0.11536278, loss_bd:0.00024421\n",
      "epoch 42 loss_pde:0.01833569, loss_ic:0.11535536, loss_bd:0.00024280\n",
      "epoch 42 loss_pde:0.01840673, loss_ic:0.11534888, loss_bd:0.00024027\n",
      "epoch 42 loss_pde:0.01849387, loss_ic:0.11533969, loss_bd:0.00023856\n",
      "epoch 42 loss_pde:0.01857780, loss_ic:0.11533156, loss_bd:0.00023579\n",
      "epoch 42: loss 1.175028\n",
      "epoch 43 loss_pde:0.01868816, loss_ic:0.11531935, loss_bd:0.00023433\n",
      "epoch 43 loss_pde:0.01879658, loss_ic:0.11530868, loss_bd:0.00023127\n",
      "epoch 43 loss_pde:0.01891832, loss_ic:0.11529506, loss_bd:0.00022982\n",
      "epoch 43 loss_pde:0.01904119, loss_ic:0.11528249, loss_bd:0.00022690\n",
      "epoch 43 loss_pde:0.01918172, loss_ic:0.11526666, loss_bd:0.00022540\n",
      "epoch 43 loss_pde:0.01932070, loss_ic:0.11525165, loss_bd:0.00022282\n",
      "epoch 43 loss_pde:0.01948667, loss_ic:0.11523308, loss_bd:0.00022089\n",
      "epoch 43 loss_pde:0.01962273, loss_ic:0.11521126, loss_bd:0.00022416\n",
      "epoch 43 loss_pde:0.01980786, loss_ic:0.11519148, loss_bd:0.00022097\n",
      "epoch 43 loss_pde:0.02001452, loss_ic:0.11516877, loss_bd:0.00021766\n",
      "epoch 43 loss_pde:0.02029941, loss_ic:0.11513767, loss_bd:0.00021282\n",
      "epoch 43 loss_pde:0.02054388, loss_ic:0.11510957, loss_bd:0.00020947\n",
      "epoch 43 loss_pde:0.02080763, loss_ic:0.11507845, loss_bd:0.00020574\n",
      "epoch 43 loss_pde:0.02109824, loss_ic:0.11504324, loss_bd:0.00020138\n",
      "epoch 43 loss_pde:0.02145367, loss_ic:0.11499611, loss_bd:0.00019905\n",
      "epoch 43 loss_pde:0.02181662, loss_ic:0.11495323, loss_bd:0.00019017\n",
      "epoch 43 loss_pde:0.02222322, loss_ic:0.11489502, loss_bd:0.00018923\n",
      "epoch 43 loss_pde:0.02257981, loss_ic:0.11485057, loss_bd:0.00017902\n",
      "epoch 43 loss_pde:0.02296010, loss_ic:0.11478937, loss_bd:0.00018011\n",
      "epoch 43 loss_pde:0.02333927, loss_ic:0.11473684, loss_bd:0.00016756\n",
      "epoch 43: loss 1.174225\n",
      "epoch 44 loss_pde:0.02368712, loss_ic:0.11467225, loss_bd:0.00017003\n",
      "epoch 44 loss_pde:0.02401390, loss_ic:0.11461535, loss_bd:0.00016054\n",
      "epoch 44 loss_pde:0.02426205, loss_ic:0.11455905, loss_bd:0.00016047\n",
      "epoch 44 loss_pde:0.02448457, loss_ic:0.11450020, loss_bd:0.00015663\n",
      "epoch 44 loss_pde:0.02463401, loss_ic:0.11444993, loss_bd:0.00015303\n",
      "epoch 44 loss_pde:0.02474130, loss_ic:0.11438867, loss_bd:0.00015546\n",
      "epoch 44 loss_pde:0.02475790, loss_ic:0.11435071, loss_bd:0.00014884\n",
      "epoch 44 loss_pde:0.02472394, loss_ic:0.11430033, loss_bd:0.00015509\n",
      "epoch 44 loss_pde:0.02461231, loss_ic:0.11426889, loss_bd:0.00014933\n",
      "epoch 44 loss_pde:0.02447885, loss_ic:0.11422194, loss_bd:0.00016042\n",
      "epoch 44 loss_pde:0.02426353, loss_ic:0.11419752, loss_bd:0.00015537\n",
      "epoch 44 loss_pde:0.02407454, loss_ic:0.11415541, loss_bd:0.00016845\n",
      "epoch 44 loss_pde:0.02381975, loss_ic:0.11413118, loss_bd:0.00017162\n",
      "epoch 44 loss_pde:0.02361317, loss_ic:0.11410912, loss_bd:0.00017597\n",
      "epoch 44 loss_pde:0.02337907, loss_ic:0.11408888, loss_bd:0.00018429\n",
      "epoch 44 loss_pde:0.02316868, loss_ic:0.11407512, loss_bd:0.00018635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 44 loss_pde:0.02290891, loss_ic:0.11406182, loss_bd:0.00019434\n",
      "epoch 44 loss_pde:0.02269274, loss_ic:0.11404860, loss_bd:0.00019958\n",
      "epoch 44 loss_pde:0.02241594, loss_ic:0.11404524, loss_bd:0.00020322\n",
      "epoch 44 loss_pde:0.02220526, loss_ic:0.11403039, loss_bd:0.00021566\n",
      "epoch 44: loss 1.172110\n",
      "epoch 45 loss_pde:0.02192681, loss_ic:0.11403915, loss_bd:0.00021083\n",
      "epoch 45 loss_pde:0.02172708, loss_ic:0.11403243, loss_bd:0.00021962\n",
      "epoch 45 loss_pde:0.02154310, loss_ic:0.11403462, loss_bd:0.00022097\n",
      "epoch 45 loss_pde:0.02137351, loss_ic:0.11402977, loss_bd:0.00022914\n",
      "epoch 45 loss_pde:0.02120744, loss_ic:0.11403266, loss_bd:0.00023069\n",
      "epoch 45 loss_pde:0.02105548, loss_ic:0.11403000, loss_bd:0.00023768\n",
      "epoch 45 loss_pde:0.02092030, loss_ic:0.11403137, loss_bd:0.00024059\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [74]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m tic \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m----> 5\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m toc \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "Input \u001b[0;32mIn [71]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     32\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss\n\u001b[0;32m---> 35\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m loss_value \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(loss, \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m loss\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_value\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/optim/optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/grad_mode.py:28\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m():\n\u001b[0;32m---> 28\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/optim/lbfgs.py:437\u001b[0m, in \u001b[0;36mLBFGS.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_iter \u001b[38;5;241m!=\u001b[39m max_iter:\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;66;03m# re-evaluate function only if not in last iteration\u001b[39;00m\n\u001b[1;32m    434\u001b[0m     \u001b[38;5;66;03m# the reason we do this: in a stochastic setting,\u001b[39;00m\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;66;03m# no use to re-evaluate that function here\u001b[39;00m\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[0;32m--> 437\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    438\u001b[0m     flat_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gather_flat_grad()\n\u001b[1;32m    439\u001b[0m     opt_cond \u001b[38;5;241m=\u001b[39m flat_grad\u001b[38;5;241m.\u001b[39mabs()\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m tolerance_grad\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/grad_mode.py:28\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m():\n\u001b[0;32m---> 28\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [71]\u001b[0m, in \u001b[0;36mtrain.<locals>.closure\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m loss_ic \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.001\u001b[39m\u001b[38;5;241m*\u001b[39mmodel\u001b[38;5;241m.\u001b[39mloss_ic(x_ic_ignore,u_ic_ignore) \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m     10\u001b[0m            \u001b[38;5;241m0.1\u001b[39m\u001b[38;5;241m*\u001b[39mmodel\u001b[38;5;241m.\u001b[39mloss_ic(x_ic_covert,u_ic_covert) \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m     11\u001b[0m             \u001b[38;5;241m100\u001b[39m\u001b[38;5;241m*\u001b[39mmodel\u001b[38;5;241m.\u001b[39mloss_ic(x_ic_memory,u_ic_memory) \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m     12\u001b[0m                   model\u001b[38;5;241m.\u001b[39mloss_ic(x_ic_overt,u_ic_overt)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#loss_bd = model.loss_bd(x_bdL,u_bdL_in,mark_bd) #+model.loss_bd(x_bdR,u_bdR_in,mark_bd) \u001b[39;00m\n\u001b[1;32m     15\u001b[0m loss_bd \u001b[38;5;241m=\u001b[39m  \u001b[38;5;241m0.001\u001b[39m\u001b[38;5;241m*\u001b[39mmodel\u001b[38;5;241m.\u001b[39mloss_bd(x_bdL_ignore,u_bdL_ignore) \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m---> 16\u001b[0m             \u001b[38;5;241m0.1\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_bd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_bdL_covert\u001b[49m\u001b[43m,\u001b[49m\u001b[43mu_bdL_covert\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m     17\u001b[0m              \u001b[38;5;241m100\u001b[39m\u001b[38;5;241m*\u001b[39mmodel\u001b[38;5;241m.\u001b[39mloss_bd(x_bdL_memory,u_bdL_memory) \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m     18\u001b[0m                 model\u001b[38;5;241m.\u001b[39mloss_bd(x_bdL_overt,u_bdL_overt) \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m     19\u001b[0m            \u001b[38;5;241m0.001\u001b[39m\u001b[38;5;241m*\u001b[39mmodel\u001b[38;5;241m.\u001b[39mloss_bd(x_bdR_ignore,u_bdR_ignore) \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m     20\u001b[0m             \u001b[38;5;241m0.1\u001b[39m\u001b[38;5;241m*\u001b[39mmodel\u001b[38;5;241m.\u001b[39mloss_bd(x_bdR_covert,u_bdR_covert) \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m     21\u001b[0m              \u001b[38;5;241m100\u001b[39m\u001b[38;5;241m*\u001b[39mmodel\u001b[38;5;241m.\u001b[39mloss_bd(x_bdR_memory,u_bdR_memory) \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m     22\u001b[0m                  model\u001b[38;5;241m.\u001b[39mloss_bd(x_bdR_overt,u_bdR_overt)\n\u001b[1;32m     25\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_pde \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m10\u001b[39m\u001b[38;5;241m*\u001b[39mloss_ic\u001b[38;5;241m+\u001b[39m \u001b[38;5;241m10\u001b[39m\u001b[38;5;241m*\u001b[39mloss_bd                                       \n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m loss_pde:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_pde\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.8f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, loss_ic:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_ic\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.8f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, loss_bd:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_bd\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.8f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Input \u001b[0;32mIn [71]\u001b[0m, in \u001b[0;36mDNN.loss_bd\u001b[0;34m(self, x, u_in)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss_bd\u001b[39m(\u001b[38;5;28mself\u001b[39m, x,u_in):\n\u001b[1;32m    105\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet(x)                                                      \n\u001b[0;32m--> 106\u001b[0m     loss_bds \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mu_in\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mlen\u001b[39m(x) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m    108\u001b[0m         loss_bds \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.LBFGS(model.parameters(),lr=0.1,max_iter=20)\n",
    "epochs = 10000\n",
    "tic = time.time()\n",
    "for epoch in range(1, epochs+1):\n",
    "    train(epoch)\n",
    "    \n",
    "toc = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_ic_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_bdR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device1 = torch.device(\"cpu\")\n",
    "model = torch.save('1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '1.pth'\n",
    "model = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [111]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_int_overt\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx_int_overt\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/matplotlib/pyplot.py:2807\u001b[0m, in \u001b[0;36mscatter\u001b[0;34m(x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, edgecolors, plotnonfinite, data, **kwargs)\u001b[0m\n\u001b[1;32m   2802\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mscatter)\n\u001b[1;32m   2803\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscatter\u001b[39m(\n\u001b[1;32m   2804\u001b[0m         x, y, s\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, c\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, marker\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   2805\u001b[0m         vmin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, vmax\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, linewidths\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   2806\u001b[0m         edgecolors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, plotnonfinite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m-> 2807\u001b[0m     __ret \u001b[38;5;241m=\u001b[39m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscatter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2808\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmarker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmarker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcmap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2809\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvmin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvmax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvmax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlinewidths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlinewidths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2810\u001b[0m \u001b[43m        \u001b[49m\u001b[43medgecolors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medgecolors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplotnonfinite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mplotnonfinite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2811\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2812\u001b[0m     sci(__ret)\n\u001b[1;32m   2813\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m __ret\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/matplotlib/__init__.py:1412\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1409\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m   1410\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(ax, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1411\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1412\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1414\u001b[0m     bound \u001b[38;5;241m=\u001b[39m new_sig\u001b[38;5;241m.\u001b[39mbind(ax, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1415\u001b[0m     auto_label \u001b[38;5;241m=\u001b[39m (bound\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mget(label_namer)\n\u001b[1;32m   1416\u001b[0m                   \u001b[38;5;129;01mor\u001b[39;00m bound\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(label_namer))\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/matplotlib/axes/_axes.py:4366\u001b[0m, in \u001b[0;36mAxes.scatter\u001b[0;34m(self, x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, edgecolors, plotnonfinite, **kwargs)\u001b[0m\n\u001b[1;32m   4363\u001b[0m x, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_unit_info([(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m, x), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, y)], kwargs)\n\u001b[1;32m   4364\u001b[0m \u001b[38;5;66;03m# np.ma.ravel yields an ndarray, not a masked array,\u001b[39;00m\n\u001b[1;32m   4365\u001b[0m \u001b[38;5;66;03m# unless its argument is a masked array.\u001b[39;00m\n\u001b[0;32m-> 4366\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4367\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mma\u001b[38;5;241m.\u001b[39mravel(y)\n\u001b[1;32m   4368\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m!=\u001b[39m y\u001b[38;5;241m.\u001b[39msize:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/ma/core.py:6770\u001b[0m, in \u001b[0;36m_frommethod.__call__\u001b[0;34m(self, a, *args, **params)\u001b[0m\n\u001b[1;32m   6767\u001b[0m     args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(args)\n\u001b[1;32m   6768\u001b[0m     a, args[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m], a\n\u001b[0;32m-> 6770\u001b[0m marr \u001b[38;5;241m=\u001b[39m \u001b[43masanyarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6771\u001b[0m method_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m   6772\u001b[0m method \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mtype\u001b[39m(marr), method_name, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/ma/core.py:8002\u001b[0m, in \u001b[0;36masanyarray\u001b[0;34m(a, dtype)\u001b[0m\n\u001b[1;32m   8000\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(a, MaskedArray) \u001b[38;5;129;01mand\u001b[39;00m (dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m dtype \u001b[38;5;241m==\u001b[39m a\u001b[38;5;241m.\u001b[39mdtype):\n\u001b[1;32m   8001\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m a\n\u001b[0;32m-> 8002\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmasked_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/ma/core.py:2829\u001b[0m, in \u001b[0;36mMaskedArray.__new__\u001b[0;34m(cls, data, mask, dtype, copy, subok, ndmin, fill_value, keep_mask, hard_mask, shrink, order)\u001b[0m\n\u001b[1;32m   2820\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2821\u001b[0m \u001b[38;5;124;03mCreate a new masked array from scratch.\u001b[39;00m\n\u001b[1;32m   2822\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2826\u001b[0m \n\u001b[1;32m   2827\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2828\u001b[0m \u001b[38;5;66;03m# Process data.\u001b[39;00m\n\u001b[0;32m-> 2829\u001b[0m _data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2830\u001b[0m \u001b[43m                 \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mndmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mndmin\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2831\u001b[0m _baseclass \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(data, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_baseclass\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mtype\u001b[39m(_data))\n\u001b[1;32m   2832\u001b[0m \u001b[38;5;66;03m# Check that we're not erasing the mask.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_tensor.py:678\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m__array__, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    677\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 678\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "\n",
    "plt.figure()\n",
    "plt.scatter(x_int_overt[:,0],x_int_overt[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Double but found Float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [134]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m x_test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhstack((T, X))                                      \n\u001b[1;32m      8\u001b[0m x_test \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(x_test, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 9\u001b[0m u \u001b[38;5;241m=\u001b[39m to_numpy(\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#res = to_numpy(model.res_pde(x_test))\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#d   = to_numpy(model.lambda_pde(x_test))\u001b[39;00m\n\u001b[1;32m     12\u001b[0m ue \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((Nd,Nd))\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [94]\u001b[0m, in \u001b[0;36mDNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 141\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/functional.py:1848\u001b[0m, in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[1;32m   1847\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(linear, (\u001b[38;5;28minput\u001b[39m, weight, bias), \u001b[38;5;28minput\u001b[39m, weight, bias\u001b[38;5;241m=\u001b[39mbias)\n\u001b[0;32m-> 1848\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected scalar type Double but found Float"
     ]
    }
   ],
   "source": [
    "Nd = 200\n",
    "x = np.linspace(0, 2, 200)                                  \n",
    "t = np.linspace(0, 0.1, 200)                                        \n",
    "t_grid, x_grid = np.meshgrid(t, x)                              \n",
    "T = t_grid.flatten()[:, None]                                   \n",
    "X = x_grid.flatten()[:, None]                                   \n",
    "x_test = np.hstack((T, X))                                      \n",
    "x_test = torch.tensor(x_test, requires_grad=True, dtype=torch.float32).to(device)\n",
    "u = to_numpy(model(x_test))\n",
    "#res = to_numpy(model.res_pde(x_test))\n",
    "#d   = to_numpy(model.lambda_pde(x_test))\n",
    "ue = np.zeros((Nd,Nd))\n",
    "for j in range(0,Nd):\n",
    "    for i in range(0,Nd):\n",
    "        ue[i,j] = u[i*Nd+j,0]\n",
    "#loss = model.loss_pde(x_test)                                 \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.figure()\n",
    "plt.contourf(t_grid[:,:],x_grid[:,:],ue,60)\n",
    "#plt.plot(x[:],u_pred[:,0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Nd = 200\n",
    "x = np.linspace(-1, 1, 200)                                  \n",
    "t = np.linspace(0.0, 0.0, 1)                                        \n",
    "t_grid, x_grid = np.meshgrid(t, x)                              \n",
    "T = t_grid.flatten()[:, None]                                   \n",
    "X = x_grid.flatten()[:, None]                                   \n",
    "x_test = np.hstack((T, X))                                      \n",
    "x_test = torch.tensor(x_test, requires_grad=True, dtype=torch.float32).to(device)\n",
    "u = to_numpy(model(x_test))\n",
    "#res = to_numpy(model.res_pde(x_test))\n",
    "#d   = to_numpy(model.lambda_pde(x_test))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.plot(x[:],u[:,0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(x[:],1/d[:,0])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_array = np.array(u_pred[:,0])\n",
    "file = open(\"burgers_u_we_11500.dat\", \"w+\")\n",
    "for i in range(u_pred.shape[0]):\n",
    "    print(f'{x[i]}  {u_pred[i,0]} ',file=file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_array = np.array(res[:,0])\n",
    "file = open(\"burgers_res_w_5000.dat\", \"w+\")\n",
    "for i in range(u_pred.shape[0]):\n",
    "    print(f'{x[i]}  {res[i,0]} ',file=file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"burgers_d_w_14931.dat\", \"w+\")\n",
    "for i in range(d.shape[0]):\n",
    "    print(f'{x[i]}  {1/d[i,0]} ',file=file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
