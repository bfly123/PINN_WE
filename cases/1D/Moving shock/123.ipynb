{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving Sod Problem with PINN-WE\n",
    "\n",
    "\n",
    "* This code is modified from the code https://github.com/alexpapados/Physics-Informed-Deep-Learning-Solid-and-Fluid-Mechanics\n",
    "\n",
    "* The paper about this code can be reffed to https://www.researchgate.net/publication/359480166_Discontinuity_Computing_with_Physics-Informed_Neural_Network\n",
    "\n",
    "* For strong shock waves, PINN-WE can let you get a discontinuous result quickly and sharply. But it can not guarantee the result must be physical. You can try to change the random seed or learning rate to get a different result. In the paper, we show the best result of some tests to demonstrate that PINN-WE has the potential ability to handle a non-linear discontinuous problem.\n",
    "\n",
    "* So physics constraints are needed near shock waves. They are conservation laws, Rankine-Hugoniot relations and entropy condition. We develop a physics screen method here to determinate the problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    def closure():\n",
    "        optimizer.zero_grad()                                                     \n",
    "        loss_pde = model.loss_pde(x_int,x_screen2_L,x_screen2_R,0.01)                                    \n",
    "        loss_ic = model.loss_ic(x_ic, rho_ic,u_ic,p_ic)   \n",
    "        loss_bc = model.loss_ic(x_bc, rho_bc,u_bc,p_bc)   \n",
    "        \n",
    "        loss_rh1 = model.loss_rh(x_screen2,x_screen2_L,x_screen2_R) # RH relation\n",
    "        loss_s0 = model.loss_character(x_screen2,x_screen2_R) # Entropy condition\n",
    "       # loss_con3 = model.loss_con(x_screen3 ,x_ic,T3) #Conservation laws\n",
    "        loss_con1 = model.loss_con(x_screen1 ,x_ic,T1) #Conservation laws\n",
    "        loss = loss_pde + 10*(loss_ic+loss_bc) #+  10*(loss_rh1 +loss_con1  )   +10* loss_s0  \n",
    "       # print(f'epoch {epoch} loss_pde:{loss_pde:.8f},loss_rh1:{loss_rh1:.8f},loss_con:{loss_con1:.8f}, loss_s:{loss_s0:.8f}, loss_ic:{loss_ic:.8f}')\n",
    "        loss.backward()\n",
    "        return loss\n",
    "    loss = optimizer.step(closure)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Unit_var(rhoL,uL,pL,rhoR,uR,pR,t):\n",
    "  rhoref = max(rhoL,rhoR)\n",
    "  pmax = max(pL,pR)\n",
    "  umax = max(abs(uL),abs(uR))\n",
    "  uref = max(np.sqrt(pmax/rhoref),umax)\n",
    "  pref = uref**2*rhoref\n",
    "\n",
    "  \n",
    "  uLn = (uL)/uref\n",
    "  uRn = (uR)/uref\n",
    "  pLn = pL/pref\n",
    "  pRn = pR/pref\n",
    "  rhoLn = rhoL/rhoref\n",
    "  rhoRn = rhoR/rhoref\n",
    "  \n",
    "  tn = t*uref\n",
    "  \n",
    "  return rhoLn,uLn,pLn,rhoRn,uRn,pRn, tn,rhoref,uref,pref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import time\n",
    "import scipy.io\n",
    "dtype=torch.float32\n",
    "dtype=torch.float64\n",
    "# Seeds\n",
    "#crhoL = 1\n",
    "#cuL = -2\n",
    "#cpL = 0.4\n",
    "#\n",
    "#crhoR =1\n",
    "#cuR = 2\n",
    "#cpR = 0.4\n",
    "#Ts = 0\n",
    "#Te = 0.1\n",
    "#rhoref = 1\n",
    "#uref = 1\n",
    "#pref = 1\n",
    "crhoL = 27/7\n",
    "cuL = 2.629369\n",
    "cpL = 31/3\n",
    "\n",
    "crhoR = 1\n",
    "cuR = 0\n",
    "cpR = 1\n",
    "\n",
    "#crhoL = 0.89\n",
    "#cuL = 0.098923\n",
    "#cpL = 1\n",
    "#\n",
    "#crhoR = 1\n",
    "#cuR = 0\n",
    "#cpR = 0.16185\n",
    "#\n",
    "#Ts = 0\n",
    "#Te = 0.91728\n",
    "\n",
    "Ts = 0\n",
    "Te = 0.18\n",
    "Xs = 0\n",
    "Xe = 1\n",
    "\n",
    "crhoL,cuL,cpL,crhoR,cuR,cpR,Te,rhoref,uref,pref = Unit_var(crhoL,cuL,cpL,crhoR,cuR,cpR,Te)\n",
    "\n",
    "###Ts = 0, Xs =0, Xe = 1\n",
    "\n",
    "#crhoL = 0.89\n",
    "#cuL = 0.098923\n",
    "#cpL = 1\n",
    "#\n",
    "#crhoR = 1\n",
    "#cuR = 0\n",
    "#cpR = 0.16185\n",
    "#\n",
    "#Ts = 0\n",
    "#Te = 0.91728\n",
    "\n",
    "Xs = 0\n",
    "Xe = 1\n",
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "setup_seed(5)\n",
    "   \n",
    "def gradients(outputs, inputs):\n",
    "    return torch.autograd.grad(outputs, inputs,grad_outputs=torch.ones_like(outputs), create_graph=True)\n",
    "\n",
    "# Convert torch tensor into np.array\n",
    "def to_numpy(input):\n",
    "    if isinstance(input, torch.Tensor):\n",
    "        return input.detach().cpu().numpy()\n",
    "    elif isinstance(input, np.ndarray):\n",
    "        return input\n",
    "    else:\n",
    "        raise TypeError('Unknown type of input, expected torch.Tensor or ' \\\n",
    "                        'np.ndarray, but got {}'.format(type(input)))\n",
    "\n",
    "# Initial conditions\n",
    "def IC(x):\n",
    "    N = x.shape[0]\n",
    "    rho_init = np.zeros((x.shape[0]))                                              \n",
    "    u_init = np.zeros((x.shape[0]))                                                \n",
    "    p_init = np.zeros((x.shape[0]))                                                \n",
    "\n",
    "    # rho, p - initial condition\n",
    "    for i in range(N):\n",
    "        if (x[i,1] <= 0.2):\n",
    "            rho_init[i] = crhoL\n",
    "            u_init[i] = cuL\n",
    "            p_init[i] = cpL\n",
    "        else:\n",
    "            rho_init[i] = crhoR*(1+0.2*np.sin(50*x[i,1]))\n",
    "            u_init[i] = cuR\n",
    "            p_init[i] = cpR\n",
    "\n",
    "    return rho_init, u_init, p_init\n",
    "\n",
    "def BC(x):\n",
    "    N = x.shape[0]\n",
    "    rho_init = np.zeros((x.shape[0]))                                              \n",
    "    u_init = np.zeros((x.shape[0]))                                                \n",
    "    p_init = np.zeros((x.shape[0]))                                                \n",
    "\n",
    "    # rho, p - initial condition\n",
    "    for i in range(N):\n",
    "        if (x[i,1] <= 0.2):\n",
    "            rho_init[i] = crhoL\n",
    "            u_init[i] = cuL\n",
    "            p_init[i] = cpL\n",
    "        else:\n",
    "            rho_init[i] = crhoR#*(1+0.2*np.sin(50))\n",
    "            u_init[i] = cuR\n",
    "            p_init[i] = cpR\n",
    "\n",
    "    return rho_init, u_init, p_init\n",
    "\n",
    "\n",
    "# Generate Neural Network\n",
    "class DNN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DNN, self).__init__()\n",
    "        self.net = nn.Sequential()                                                  \n",
    "        self.net.add_module('Linear_layer_1', nn.Linear(2, 30))                     \n",
    "        self.net.add_module('Tanh_layer_1', nn.Tanh())                              \n",
    "\n",
    "        for num in range(2, 7):                                                     \n",
    "            self.net.add_module('Linear_layer_%d' % (num), nn.Linear(30, 30))       \n",
    "            self.net.add_module('Tanh_layer_%d' % (num), nn.Tanh())                 \n",
    "        self.net.add_module('Linear_layer_final', nn.Linear(30, 3))                 \n",
    "\n",
    "    # Forward Feed\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "    # Loss function for PDE\n",
    "    def loss_pde(self, x,xL,xR,Dx):\n",
    "        y = self.net(x)                                                \n",
    "        rho,p,u = y[:, 0:1], y[:, 1:2], y[:, 2:]\n",
    "        \n",
    "        yR = self.net(xR)                                                \n",
    "        rhoR,pR,uR = yR[:, 0:1], yR[:, 1:2], yR[:, 2:]\n",
    "        yL = self.net(xL)                                                \n",
    "        rhoL,pL,uL = yL[:, 0:1], yL[:, 1:2], yL[:, 2:]\n",
    "        \n",
    "        U2 = rho*u\n",
    "        U3 = 0.5*rho*u**2 + p/0.4\n",
    "        \n",
    "        #F1 = U2\n",
    "        F2 = rho*u**2+p\n",
    "        F3 = u*(U3 + p)\n",
    "        \n",
    "        gamma = 1.4                                                    \n",
    "\n",
    "        # Gradients and partial derivatives\n",
    "        drho_g = gradients(rho, x)[0]                                  \n",
    "        rho_t, rho_x = drho_g[:, :1], drho_g[:, 1:]             \n",
    "\n",
    "\n",
    "        du_g = gradients(u, x)[0]                                      \n",
    "        u_t, u_x = du_g[:, :1], du_g[:, 1:]                            \n",
    "        \n",
    "        dp_g = gradients(p, x)[0]                                      \n",
    "        p_t, p_x = dp_g[:, :1], dp_g[:, 1:]                            \n",
    "\n",
    "       # dp_g = gradients(p, x)[0]                                     \n",
    "       # p_t, p_x = dp_g[:, :1], dp_g[:, 1:]                           \n",
    "        \n",
    "        dU2_g = gradients(U2, x)[0]\n",
    "        U2_t,U2_x = dU2_g[:,:1], dU2_g[:,1:]\n",
    "        dU3_g = gradients(U3, x)[0]\n",
    "        U3_t,U3_x = dU3_g[:,:1], dU3_g[:,1:]\n",
    "        dF2_g = gradients(F2, x)[0]\n",
    "        F2_t,F2_x = dF2_g[:,:1], dF2_g[:,1:]\n",
    "        dF3_g = gradients(F3, x)[0]\n",
    "        F3_t,F3_x = dF3_g[:,:1], dF3_g[:,1:]\n",
    "\n",
    "        d1 = 0.4*(abs(u_x)-(u_x) )+1 #+ (torch.sign(0.01 -abs(u_x))+1)*abs(rho_x))+ 1\n",
    "        d2 = 0.4*(abs(u_x)-(u_x) )+1 #+ (torch.sign(0.01 -abs(u_x))+1)*abs(rho_x))+ 1\n",
    "        d3 = 0.4*(abs(u_x)-(u_x) )+1 #+ (torch.sign(0.01 -abs(u_x))+1)*abs(rho_x))+ 1\n",
    "        \n",
    "        #d = 0.1*(abs(uR-uL)-(uR-uL))/Dx + 1\n",
    "        #d = torch.exp(-10*u_x)+1\n",
    "        #d1 = torch.clamp(d/5,min=1)\n",
    "     \n",
    "        f = (((rho_t + U2_x)/d1)**2).mean() + \\\n",
    "            (((U2_t  + F2_x)/d2)**2).mean() + \\\n",
    "            (((U3_t  + F3_x)/d3)**2).mean() #+\\\n",
    "            #((rho_t).mean())**2 +\\\n",
    "            #((U3_t).mean())**2 \n",
    "    \n",
    "        return f\n",
    "\n",
    "    def loss_ic(self, x, rho, u, p):\n",
    "        y = self.net(x)                                                      \n",
    "        rho_nn, p_nn,u_nn = y[:, 0], y[:, 1], y[:, 2]            \n",
    "\n",
    "        loss_ics = ((u_nn - u) ** 2).mean() + \\\n",
    "               ((rho_nn- rho) ** 2).mean()  + \\\n",
    "               ((p_nn - p) ** 2).mean()\n",
    "\n",
    "        return loss_ics\n",
    "    \n",
    "    # Loss function for conservation\n",
    "    def loss_con(self, x_en,x_in,t):\n",
    "        y_en = self.net(x_en)                                       \n",
    "        y_in = self.net(x_in)                                       \n",
    "        rhoen, pen,uen = y_en[:, 0], y_en[:, 1], y_en[:, 2]         \n",
    "        rhoin, pin,uin = y_in[:, 0], y_in[:, 1], y_in[:, 2]         \n",
    "\n",
    "        U3en = 0.5*rhoen*uen**2 + pen/0.4\n",
    "        U3in = 0.5*rhoin*uin**2 + pin/0.4\n",
    "        gamma = 1.4\n",
    "        cU3L = 0.5*crhoL*cuL**2 + cpL/0.4 \n",
    "        cU3R = 0.5*crhoR*cuR**2 + cpR/0.4 \n",
    "        # Loss function for the initial condition\n",
    "        loss_en = ((rhoen - rhoin).mean() - t*(crhoL*cuL-crhoR*cuR))**2+ \\\n",
    "            ((-U3en+ U3in).mean() + t*(cU3L*cuL - cU3R*cuR) + (cpL*cuL - cpR*cuR)*t )**2 +\\\n",
    "            ((-rhoen*uen + rhoin*uin).mean()+(cpL-cpR)*t +(crhoL*cuL*cuL-crhoR*cuR*cuR)*t)**2\n",
    "        return loss_en\n",
    "    \n",
    "    def loss_rh(self, x,x_l,x_r):\n",
    "        y = self.net(x)                                    \n",
    "        y_r = self.net(x_r)                                    \n",
    "        y_l = self.net(x_l)                                    \n",
    "        rho, p,u = y[:, 0], y[:, 1], y[:, 2]          \n",
    "        rhol, pl,ul = y_l[:, 0], y_l[:, 1], y_l[:, 2]          \n",
    "        rhor, pr,ur = y_r[:, 0], y_r[:, 1], y_r[:, 2]          \n",
    "\n",
    "        du_g = gradients(u, x)[0]                                      \n",
    "        u_t, u_x = du_g[:, 0], du_g[:, 1]                            \n",
    "        d = 1/(0.1*(abs(u_x)-u_x)  + 1)\n",
    "        eta =  torch.clamp(d-0.1,max=0)*torch.clamp(abs(pr-pl)-0.1,min=0)*torch.clamp(abs(ur-ul)-0.1,min=0)\n",
    "       # eta =  torch.clamp(abs(pr-pl)-0.1,min=0)*torch.clamp(abs(ur-ul)-0.1,min=0)\n",
    "        #eta = 1\n",
    "        \n",
    "        #loss_rh =  (((rho/rhol - (6*p+pl)/(6*pl+p))*eta)**2).mean()+\\\n",
    "        loss_rh = (((rhor/rhol - (6*pr+pl)/(6*pl+pr))*(ur-ul)*eta)**2).mean()+\\\n",
    "                   ((((ur-ul)**2 -2/rhor*(pr-pl)**2/(0.4*pr+2.4*pl))*eta)**2).mean()\n",
    "           #        ((((ur-u)**2 -2/rho*(pr-p)**2/(0.4*pr+2.4*p))*eta)**2).max()\n",
    "            \n",
    "        #loss_rh =  (((pr/pl - (6*rhor-rhol)/(6*rhol-rhor))*(pr-pl)*eta)**2).max()+\\\n",
    "                   #((((u-ul)**2 -2/rho*(p-pl)**2/(0.4*p+2.4*pl))*eta)**2).max()+\\\n",
    "        return loss_rh\n",
    "    \n",
    "    def loss_character(self, x_l,x_r):\n",
    "        y_r = self.net(x_r)                                                      # Initial condition\n",
    "        y_l = self.net(x_l)                                                      # Initial condition\n",
    "        rhol, pl,ul = y_l[:, 0], y_l[:, 1], y_l[:, 2]            # rho, u, p - initial condition\n",
    "        rhor, pr,ur = y_r[:, 0], y_r[:, 1], y_r[:, 2]            # rho, u, p - initial condition\n",
    "\n",
    "        #du_g = gradients(ul, x_l)[0]                                      \n",
    "        #u_t, u_x = du_g[:, :1], du_g[:, 1:]                            \n",
    "        #d = 1/(0.1*(abs(u_x)-u_x)  + 1)\n",
    "        #eta =  torch.clamp(d-0.1,max=0)*torch.clamp(abs(pr-pl)-0.01,min=0)*torch.clamp(abs(ur-ul)-0.01,min=0)\n",
    "        eta =  torch.clamp(abs(pr-pl)-0.01,min=0)*torch.clamp(abs(ur-ul)-0.01,min=0)\n",
    "       # eta = 1\n",
    "        # Loss function for the initial condition\n",
    "        gamma = 1.4\n",
    "        ss = 1.0e-10\n",
    "        cL = torch.sqrt(gamma*abs(pl)/(abs(rhol)+ss))\n",
    "        cR = torch.sqrt(gamma*abs(pr)/(abs(rhor)+ss))\n",
    "        sR = torch.max(ul+cL,ur+cR)* (rhol-rhor)\n",
    "        sL = torch.min(ul-cL,ur-cR)*(rhol-rhor)\n",
    "        \n",
    "        s = rhol*ul - rhor*ur\n",
    "       # if (s.max() > 1000):\n",
    "       #     print(rhol-rhor)\n",
    "       #     print(s)\n",
    "        #print(torch.clamp(s-sR,min=0))\n",
    "       # print(eta)\n",
    "       # sm = exp(-100*(s-sR))\n",
    "        loss_s = (((s-sR)*(s-sL)*eta)**2).mean()  #torch.min((((,torch.tensor(1.0))  #+ ((torch.clamp(sL-s,min=0))**2).max()\n",
    "        return loss_s\n",
    "        \n",
    "    \n",
    "def X_entropy(x,T,dt,dx):\n",
    "    N=x.shape[0]\n",
    "    xs   = np.zeros((N,2)) \n",
    "    xsL  = np.zeros((N,2)) \n",
    "    xsR  = np.zeros((N,2)) \n",
    "    xsP  = np.zeros((N,2)) \n",
    "    xsPL = np.zeros((N,2)) \n",
    "    xsPR = np.zeros((N,2)) \n",
    "    \n",
    "    for i in range(N):\n",
    "        xs[i,1] = x[i,1]\n",
    "        xs[i,0] = x[i,0] + T\n",
    "        xsL[i,1] = xs[i,1] - dx\n",
    "        xsL[i,0] = xs[i,0]\n",
    "        xsR[i,1] = xs[i,1] + dx\n",
    "        xsR[i,0] = xs[i,0]\n",
    "        xsP[i,0] = xs[i,0] + dt\n",
    "        xsP[i,1] = xs[i,1]\n",
    "        xsPL[i,0] = xsP[i,0]\n",
    "        xsPL[i,1] = xsP[i,1]+ dx\n",
    "        xsPR[i,0] = xsP[i,0]\n",
    "        xsPR[i,1] = xsP[i,1]- dx\n",
    "        \n",
    "    return xs,xsL,xsR,xsP,xsPL,xsPR\n",
    "\n",
    "\n",
    "def X_right(x,dx):\n",
    "    N=x.shape[0]\n",
    "    xen =np.zeros((N,2)) \n",
    "    \n",
    "    for i in range(N):\n",
    "        xen[i,1] = x[i,1] + dx\n",
    "        xen[i,0] = x[i,0] \n",
    "    return xen\n",
    "def bc_data(N,Ts,Te,Xs,Xe):\n",
    "    x =np.zeros((2*N,2)) \n",
    "    \n",
    "    for i in range(N):\n",
    "        x[i,0] = (Te - Ts)*i/N\n",
    "        x[i,1] = Xs\n",
    "        \n",
    "    for i in range(N):\n",
    "        x[i+N,0] = (Te - Ts)*i/N\n",
    "        x[i+N,1] = Xe\n",
    "    return x\n",
    "def Mesh_Data(num_x,num_t,Tstart,Tend, Xstart,Xend):\n",
    "    x_ic = np.zeros((num_x,2))\n",
    "    x_int = np.zeros((num_x*(num_t-1),2))\n",
    "    \n",
    "    x_bc =np.zeros((2*(num_t-1),2)) \n",
    "    \n",
    "    dt = (Tend - Tstart)/num_t\n",
    "    x =   np.linspace(Xs, Xe, num_x) \n",
    "    x_ic[:,0] = 0\n",
    "    x_ic[:,1] = x\n",
    "    t = np.linspace(Tstart+dt, Tend, num_t-1)                                     \n",
    "    x_bc[:num_t-1,0] = t\n",
    "    x_bc[:num_t-1,1] = Xstart \n",
    "    x_bc[num_t-1:,0] = t\n",
    "    x_bc[num_t-1:,1] = Xend\n",
    "\n",
    "    \n",
    "    t_grid, x_grid = np.meshgrid(t, x)                                 \n",
    "    T = t_grid.flatten()[:, None]                                      \n",
    "    X = x_grid.flatten()[:, None]                                      \n",
    "    x_int = X[:, 0][:,None]                                        \n",
    "    t_int = T[:, 0][:,None]                                        \n",
    "\n",
    "    x_int = np.hstack((t_int, x_int))                            \n",
    "    \n",
    "    return x_ic,x_bc,x_int\n",
    "    \n",
    "    \n",
    "device = torch.device('cuda')         # change to cpu if you dont have a cuda device                              \n",
    "\n",
    "Nx = 100\n",
    "Nt = 100\n",
    "\n",
    "x_ic,x_bc,x_int =  Mesh_Data(Nx,Nt,Ts,Te,Xs,Xe)\n",
    "rho_ic, u_ic, p_ic= IC(x_ic)                    \n",
    "rho_bc, u_bc, p_bc= BC(x_bc)                    \n",
    "\n",
    "T1 = Te/2\n",
    "T3 = Te\n",
    "dx = 0.002\n",
    "dt = 0.002\n",
    "x_screen1,x_screen1_L,x_screen1_R,x_screen1_P,x_screen1_PL,x_screen1_PR = X_entropy(x_ic,T1,dt,dx)\n",
    "x_screen2,x_screen2_L,x_screen2_R,x_screen2_P,x_screen2_PL,x_screen2_PR = X_entropy(x_int,0.0,dt,dx)\n",
    "x_screen3,x_screen3_L,x_screen3_R,x_screen3_P,x_screen3_PL,x_screen3_PR = X_entropy(x_ic,T3,dt,dx)\n",
    "\n",
    "x_screen1     = torch.tensor(x_screen1, requires_grad=True, dtype=dtype).to(device)\n",
    "x_screen1_L   = torch.tensor(x_screen1_L, dtype=dtype).to(device) \n",
    "x_screen1_R   = torch.tensor(x_screen1_R, dtype=dtype).to(device)\n",
    "x_screen1_P   = torch.tensor(x_screen1_P, requires_grad=True, dtype=dtype).to(device)\n",
    "x_screen1_PL  = torch.tensor(x_screen1_PL, dtype=dtype).to(device)\n",
    "x_screen1_PR  = torch.tensor(x_screen1_PR, dtype=dtype).to(device)\n",
    "\n",
    "x_screen2     = torch.tensor(x_screen2, requires_grad=True, dtype=dtype).to(device)\n",
    "x_screen2_L   = torch.tensor(x_screen2_L, dtype=dtype).to(device)\n",
    "x_screen2_R   = torch.tensor(x_screen2_R, dtype=dtype).to(device)\n",
    "x_screen2_P   = torch.tensor(x_screen2_P, requires_grad=True, dtype=dtype).to(device)\n",
    "x_screen2_PL  = torch.tensor(x_screen2_PL, dtype=dtype).to(device)\n",
    "x_screen2_PR  = torch.tensor(x_screen2_PR, dtype=dtype).to(device)\n",
    "\n",
    "x_screen3     = torch.tensor(x_screen3, requires_grad=True, dtype=dtype).to(device)\n",
    "x_screen3_L   = torch.tensor(x_screen3_L, dtype=dtype).to(device) \n",
    "x_screen3_R   = torch.tensor(x_screen3_R, dtype=dtype).to(device)\n",
    "x_screen3_P   = torch.tensor(x_screen3_P, requires_grad=True, dtype=dtype).to(device)\n",
    "x_screen3_PL  = torch.tensor(x_screen3_PL, dtype=dtype).to(device)\n",
    "x_screen3_PR  = torch.tensor(x_screen3_PR, dtype=dtype).to(device)\n",
    "\n",
    "x_ic = torch.tensor(x_ic,requires_grad=True, dtype=dtype).to(device)\n",
    "x_bc = torch.tensor(x_bc,requires_grad=True, dtype=dtype).to(device)\n",
    "x_int = torch.tensor(x_int, requires_grad=True, dtype=dtype).to(device)\n",
    "\n",
    "rho_ic = torch.tensor(rho_ic, dtype=dtype).to(device)\n",
    "u_ic = torch.tensor(u_ic, dtype=dtype).to(device)\n",
    "p_ic = torch.tensor(p_ic, dtype=dtype).to(device)\n",
    "\n",
    "rho_bc = torch.tensor(rho_bc, dtype=dtype).to(device)\n",
    "u_bc = torch.tensor(u_bc, dtype=dtype).to(device)\n",
    "p_bc = torch.tensor(p_bc, dtype=dtype).to(device)\n",
    "\n",
    "\n",
    "model = DNN().to(device).double()\n",
    "\n",
    "print('Start training...')\n",
    "\n",
    "\n",
    "lr = 0.001                                                           # Learning rate\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4129318/978759339.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_ic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_ic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_bc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_bc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1414\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1415\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1416\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1418\u001b[0m         \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mscatter\u001b[0;34m(self, x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, edgecolors, plotnonfinite, **kwargs)\u001b[0m\n\u001b[1;32m   4364\u001b[0m         \u001b[0;31m# np.ma.ravel yields an ndarray, not a masked array,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4365\u001b[0m         \u001b[0;31m# unless its argument is a masked array.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4366\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4367\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4368\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/numpy/ma/core.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, a, *args, **params)\u001b[0m\n\u001b[1;32m   6768\u001b[0m             \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6769\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6770\u001b[0;31m         \u001b[0mmarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6771\u001b[0m         \u001b[0mmethod_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6772\u001b[0m         \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/numpy/ma/core.py\u001b[0m in \u001b[0;36masanyarray\u001b[0;34m(a, dtype)\u001b[0m\n\u001b[1;32m   8000\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8001\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 8002\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmasked_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   8003\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/numpy/ma/core.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, data, mask, dtype, copy, subok, ndmin, fill_value, keep_mask, hard_mask, shrink, order)\u001b[0m\n\u001b[1;32m   2827\u001b[0m         \"\"\"\n\u001b[1;32m   2828\u001b[0m         \u001b[0;31m# Process data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m         _data = np.array(data, dtype=dtype, copy=copy,\n\u001b[0m\u001b[1;32m   2830\u001b[0m                          order=order, subok=True, ndmin=ndmin)\n\u001b[1;32m   2831\u001b[0m         \u001b[0m_baseclass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_baseclass'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    968\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__array__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    969\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 970\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    971\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x_ic[:,0],x_ic[:,1])\n",
    "ax.scatter(x_int[:,0],x_int[:,1])\n",
    "ax.scatter(x_bc[:,0],x_bc[:,1])\n",
    "#ax.set_ylim(0.0, 1.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4129318/1894156391.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mrho\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0myR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mrhoR\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpR\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0muR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myR\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myR\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myR\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "y = self.net(x)                                                \n",
    "rho,p,u = y[:, 0:1], y[:, 1:2], y[:, 2:]\n",
    "\n",
    "yR = self.net(xR)                                                \n",
    "rhoR,pR,uR = yR[:, 0:1], yR[:, 1:2], yR[:, 2:]\n",
    "yL = self.net(xL)                                                \n",
    "rhoL,pL,uL = yL[:, 0:1], yL[:, 1:2], yL[:, 2:]\n",
    "\n",
    "\n",
    "    #((rho_t).mean())**2 +\\\n",
    "            #((U3_t).mean())**2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_tot:23.58134764\n",
      "loss_tot:23.01200243\n",
      "loss_tot:22.32074976\n",
      "loss_tot:21.80192066\n",
      "loss_tot:21.55181293\n",
      "loss_tot:21.30648337\n",
      "loss_tot:21.18238068\n",
      "loss_tot:21.01631860\n",
      "loss_tot:20.94342149\n",
      "loss_tot:20.82401975\n",
      "loss_tot:20.79072368\n",
      "loss_tot:20.73824392\n",
      "loss_tot:20.65542788\n",
      "loss_tot:20.63283434\n",
      "loss_tot:20.61937949\n",
      "loss_tot:20.62642900\n",
      "loss_tot:20.62746056\n",
      "loss_tot:20.63827351\n",
      "loss_tot:20.66220744\n",
      "loss_tot:20.66767510\n",
      "loss_tot:20.67535734\n",
      "loss_tot:20.67605952\n",
      "loss_tot:20.67677184\n",
      "loss_tot:20.67659004\n",
      "loss_tot:20.67667917\n",
      "loss_tot:20.65827598\n",
      "loss_tot:20.65213102\n",
      "loss_tot:20.64980787\n",
      "loss_tot:20.64896560\n",
      "loss_tot:20.64500885\n",
      "loss_tot:20.64305763\n",
      "loss_tot:20.64452935\n",
      "loss_tot:20.64361257\n",
      "loss_tot:20.63841269\n",
      "loss_tot:20.62650286\n",
      "loss_tot:20.62588770\n",
      "loss_tot:20.62456082\n",
      "loss_tot:20.62729706\n",
      "loss_tot:20.61711108\n",
      "loss_tot:20.61001560\n",
      "loss_tot:20.60929789\n",
      "loss_tot:20.60621791\n",
      "loss_tot:20.60595621\n",
      "loss_tot:20.60569570\n",
      "loss_tot:20.60536188\n",
      "loss_tot:20.60503818\n",
      "loss_tot:20.59832494\n",
      "loss_tot:20.59821225\n",
      "loss_tot:20.59813862\n",
      "loss_tot:20.59804142\n",
      "loss_tot:20.59795436\n",
      "loss_tot:20.59789444\n",
      "loss_tot:20.59782288\n",
      "loss_tot:20.59777130\n",
      "loss_tot:20.59773256\n",
      "loss_tot:20.59860000\n",
      "loss_tot:20.59870518\n",
      "loss_tot:20.59874452\n",
      "loss_tot:20.59878446\n",
      "loss_tot:20.59885328\n",
      "loss_tot:20.59885328\n",
      "loss_tot:20.59885328\n",
      "loss_tot:20.59885328\n",
      "loss_tot:20.59885328\n",
      "loss_tot:20.59885328\n",
      "loss_tot:20.59885328\n",
      "loss_tot:20.59881334\n",
      "loss_tot:20.59877400\n",
      "loss_tot:20.59866643\n",
      "loss_tot:20.59862829\n",
      "loss_tot:20.59859073\n",
      "loss_tot:20.59848734\n",
      "loss_tot:20.59871776\n",
      "loss_tot:20.59875829\n",
      "loss_tot:20.59879941\n",
      "loss_tot:20.59879941\n",
      "loss_tot:20.59873536\n",
      "loss_tot:20.59794446\n",
      "loss_tot:20.59794446\n",
      "loss_tot:20.59795909\n",
      "loss_tot:20.59795909\n",
      "loss_tot:20.59791796\n",
      "loss_tot:20.59787743\n",
      "loss_tot:20.59783750\n",
      "loss_tot:20.59779816\n",
      "loss_tot:20.59775942\n",
      "loss_tot:20.59775967\n",
      "loss_tot:20.59779841\n",
      "loss_tot:20.59783775\n",
      "loss_tot:20.59783775\n",
      "loss_tot:20.59783775\n",
      "loss_tot:20.59783775\n",
      "loss_tot:20.59783775\n",
      "loss_tot:20.59783775\n",
      "loss_tot:20.59779841\n",
      "loss_tot:20.59775967\n",
      "loss_tot:20.59772152\n",
      "loss_tot:20.59768397\n",
      "loss_tot:20.59764701\n",
      "loss_tot:20.59787743\n",
      "loss_tot:20.59791796\n",
      "loss_tot:20.59791796\n",
      "loss_tot:20.59791796\n",
      "loss_tot:20.59791796\n",
      "loss_tot:20.59791796\n",
      "loss_tot:20.59791796\n",
      "loss_tot:20.59791796\n",
      "loss_tot:20.59787743\n",
      "loss_tot:20.59783750\n",
      "loss_tot:20.59779816\n",
      "loss_tot:20.59775942\n",
      "loss_tot:20.59775967\n",
      "loss_tot:20.59779841\n",
      "loss_tot:20.59783775\n",
      "loss_tot:20.59783775\n",
      "loss_tot:20.59783775\n",
      "loss_tot:20.59783775\n",
      "loss_tot:20.59783775\n",
      "loss_tot:20.59783775\n",
      "loss_tot:20.59779841\n",
      "loss_tot:20.59775967\n",
      "loss_tot:20.59772152\n",
      "loss_tot:20.59768397\n",
      "loss_tot:20.59783750\n",
      "loss_tot:20.59787743\n",
      "loss_tot:20.59787743\n",
      "loss_tot:20.59787743\n",
      "loss_tot:20.59787743\n",
      "loss_tot:20.59787743\n",
      "loss_tot:20.59787743\n",
      "loss_tot:20.59783750\n",
      "loss_tot:20.59779816\n",
      "loss_tot:20.59775942\n",
      "loss_tot:20.59775967\n",
      "loss_tot:20.59779841\n",
      "loss_tot:20.59779841\n",
      "loss_tot:20.59779841\n",
      "loss_tot:20.59779841\n",
      "loss_tot:20.59779841\n",
      "loss_tot:20.59779841\n",
      "loss_tot:20.59775967\n",
      "loss_tot:20.59775942\n",
      "loss_tot:20.59775942\n",
      "loss_tot:20.59775942\n",
      "loss_tot:20.59775942\n",
      "loss_tot:20.59775942\n",
      "loss_tot:20.59775942\n",
      "loss_tot:20.59775967\n",
      "loss_tot:20.59775967\n",
      "loss_tot:20.59775967\n",
      "loss_tot:20.59775967\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4129318/568428753.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mepochi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mepochi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'loss_tot:{loss:.8f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mloss_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4129318/544096852.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    278\u001b[0m                                                f\"but got {result}.\")\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'differentiable'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mclosure\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4129318/544096852.py\u001b[0m in \u001b[0;36mclosure\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mloss_pde\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_pde\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_int\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_screen2_L\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_screen2_R\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mloss_ic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_ic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_ic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrho_ic\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mu_ic\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp_ic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mloss_bc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_ic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_bc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrho_bc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mu_bc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp_bc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4129318/2295598692.py\u001b[0m in \u001b[0;36mloss_pde\u001b[0;34m(self, x, xL, xR, Dx)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0mdu_g\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0mu_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdu_g\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdu_g\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4129318/2295598692.py\u001b[0m in \u001b[0;36mgradients\u001b[0;34m(outputs, inputs)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgrad_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;31m# Convert torch tensor into np.array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_vmap_internals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvjp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_none_pass_through\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_outputs_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    304\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m             allow_unused, accumulate_grad=False)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epoch = 0\n",
    "epochi = epoch\n",
    "lr = 0.001                                                           # Learning rate\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "epochs = 100000\n",
    "loss_history=[]\n",
    "tic = time.time()\n",
    "for epoch in range(1+epochi, epochs+epochi):\n",
    "    loss = train(epoch)\n",
    "    print(f'loss_tot:{loss:.8f}')\n",
    "    loss_history.append(to_numpy(loss))\n",
    "    if loss < 0.05:\n",
    "        break\n",
    "toc = time.time()\n",
    "print(f'Total training time: {toc - tic}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f43179d1e40>]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_tot:3.17274511\n",
      "loss_tot:3.13270742\n",
      "loss_tot:3.07688390\n",
      "loss_tot:3.02613973\n",
      "loss_tot:2.98181962\n",
      "loss_tot:2.94680204\n",
      "loss_tot:2.91414632\n",
      "loss_tot:3.06230407\n",
      "loss_tot:2.77548371\n",
      "loss_tot:2.74152829\n",
      "loss_tot:2.69082647\n",
      "loss_tot:2.57624072\n",
      "loss_tot:2.54143499\n",
      "loss_tot:2.48075613\n",
      "loss_tot:2.44522605\n",
      "loss_tot:2.34865966\n",
      "loss_tot:2.15176825\n",
      "loss_tot:1.84098094\n",
      "loss_tot:30.28656618\n",
      "loss_tot:13.77965908\n",
      "loss_tot:5.72573749\n",
      "loss_tot:7.11731168\n",
      "loss_tot:4.52256112\n",
      "loss_tot:3.20572297\n",
      "loss_tot:2.54340219\n",
      "loss_tot:1.91947034\n",
      "loss_tot:1.67265388\n",
      "loss_tot:1.55945336\n",
      "loss_tot:1.49290054\n",
      "loss_tot:1.45033271\n",
      "loss_tot:1.41372327\n",
      "loss_tot:1.36720066\n",
      "loss_tot:1.34065302\n",
      "loss_tot:1.31926230\n",
      "loss_tot:1.29496884\n",
      "loss_tot:1.25185854\n",
      "loss_tot:1.17444952\n",
      "loss_tot:1.12557543\n",
      "loss_tot:1.06398667\n",
      "loss_tot:1.04300919\n",
      "loss_tot:1.02103420\n",
      "loss_tot:0.99885922\n",
      "loss_tot:0.97861143\n",
      "loss_tot:0.96618422\n",
      "loss_tot:0.94675688\n",
      "loss_tot:0.92998104\n",
      "loss_tot:0.89621857\n",
      "loss_tot:0.87760618\n",
      "loss_tot:0.86938512\n",
      "loss_tot:0.86026804\n",
      "loss_tot:0.85554754\n",
      "loss_tot:0.85076848\n",
      "loss_tot:0.84547106\n",
      "loss_tot:0.83791523\n",
      "loss_tot:0.83291608\n",
      "loss_tot:0.82919016\n",
      "loss_tot:0.82532877\n",
      "loss_tot:0.82130721\n",
      "loss_tot:0.81164303\n",
      "loss_tot:0.79837060\n",
      "loss_tot:0.76599989\n",
      "loss_tot:0.75525316\n",
      "loss_tot:0.73804138\n",
      "loss_tot:0.71267584\n",
      "loss_tot:0.69164903\n",
      "loss_tot:0.66545495\n",
      "loss_tot:0.65624069\n",
      "loss_tot:0.64893770\n",
      "loss_tot:0.62882136\n",
      "loss_tot:0.63068229\n",
      "loss_tot:0.62532572\n",
      "loss_tot:0.62154351\n",
      "loss_tot:0.61888149\n",
      "loss_tot:0.61696051\n",
      "loss_tot:0.61550223\n",
      "loss_tot:0.61423831\n",
      "loss_tot:0.61279402\n",
      "loss_tot:0.61019152\n",
      "loss_tot:0.60402428\n",
      "loss_tot:0.59536201\n",
      "loss_tot:0.58562689\n",
      "loss_tot:0.57591541\n",
      "loss_tot:0.56160887\n",
      "loss_tot:0.53611837\n",
      "loss_tot:0.52167980\n",
      "loss_tot:0.51670229\n",
      "loss_tot:0.51440114\n",
      "loss_tot:0.51284860\n",
      "loss_tot:0.51184705\n",
      "loss_tot:0.51120827\n",
      "loss_tot:0.51026300\n",
      "loss_tot:0.50465534\n",
      "loss_tot:0.49647127\n",
      "loss_tot:0.49154053\n",
      "loss_tot:0.48754435\n",
      "loss_tot:0.48339520\n",
      "loss_tot:0.47875151\n",
      "loss_tot:0.47246182\n",
      "loss_tot:604.43781697\n",
      "loss_tot:271292953508798876977379409920.00000000\n",
      "loss_tot:32622732752431889823280660480.00000000\n",
      "loss_tot:35716512449440717486910603264.00000000\n",
      "loss_tot:6678339573721323705749995520.00000000\n",
      "loss_tot:31187079254250979727582953472.00000000\n",
      "loss_tot:2583271035002062503340408832.00000000\n",
      "loss_tot:1405974548094041418706714624.00000000\n",
      "loss_tot:862202890724433466432159744.00000000\n",
      "loss_tot:4862984806524153374353391616.00000000\n",
      "loss_tot:2661631647406157609698328576.00000000\n",
      "loss_tot:30071747971539788915529482240.00000000\n",
      "loss_tot:98972559228692805977573949440.00000000\n",
      "loss_tot:43425891143966463283664257024.00000000\n",
      "loss_tot:127503466476258696012398329856.00000000\n",
      "loss_tot:162140499860192893139679182848.00000000\n",
      "loss_tot:42443193583745611258738507776.00000000\n",
      "loss_tot:23223484965468750639146205184.00000000\n",
      "loss_tot:17665027209383221735043629056.00000000\n",
      "loss_tot:9844809838004774124904251392.00000000\n",
      "loss_tot:67665691114330893281033977856.00000000\n",
      "loss_tot:102330945734439152525969981440.00000000\n",
      "loss_tot:81778839488034965138050121728.00000000\n",
      "loss_tot:76890373996962925366069952512.00000000\n",
      "loss_tot:84540503780365279314137055232.00000000\n",
      "loss_tot:94803380628286675485083041792.00000000\n",
      "loss_tot:22573433769446095555525607424.00000000\n",
      "loss_tot:17408985222823643177921019904.00000000\n",
      "loss_tot:9523945788069031711741050880.00000000\n",
      "loss_tot:21204470313987544743581581312.00000000\n",
      "loss_tot:10168939539800018560017760256.00000000\n",
      "loss_tot:2286035553668908146493489152.00000000\n",
      "loss_tot:1549541300626350513426792448.00000000\n",
      "loss_tot:1643455816016013370962477056.00000000\n",
      "loss_tot:1421756115209839799963746304.00000000\n",
      "loss_tot:1042543784274927397153800192.00000000\n",
      "loss_tot:714783090323122966172270592.00000000\n",
      "loss_tot:4617831214573239010362130432.00000000\n",
      "loss_tot:2199062520169408860822437888.00000000\n",
      "loss_tot:176709348307041611658519642112.00000000\n",
      "loss_tot:105235715718693349136755851264.00000000\n",
      "loss_tot:33891193128386765405066100736.00000000\n",
      "loss_tot:12568043768387594020804624384.00000000\n",
      "loss_tot:6926470872439563640289361920.00000000\n",
      "loss_tot:4678012842900867848368816128.00000000\n",
      "loss_tot:2606301606284559751359496192.00000000\n",
      "loss_tot:1481847250157390599328628736.00000000\n",
      "loss_tot:908826595471679813549293568.00000000\n",
      "loss_tot:355744199856323035857420288.00000000\n",
      "loss_tot:194647802325220884805058560.00000000\n",
      "loss_tot:106502837969006789453152256.00000000\n",
      "loss_tot:262965709408178635551014912.00000000\n",
      "loss_tot:227714281887397179956396032.00000000\n",
      "loss_tot:200009982575346883854598144.00000000\n",
      "loss_tot:175228654340348838072549376.00000000\n",
      "loss_tot:137749173447293256335360.00000000\n",
      "loss_tot:75392389058341508218880.00000000\n",
      "loss_tot:41251529978780896460800.00000000\n",
      "loss_tot:22571048636813060079616.00000000\n",
      "loss_tot:12349899187341502709760.00000000\n",
      "loss_tot:6757329373216568770560.00000000\n",
      "loss_tot:3697317651375036825600.00000000\n",
      "loss_tot:2023011911979017699328.00000000\n",
      "loss_tot:1106904405275292729344.00000000\n",
      "loss_tot:605650097836572147712.00000000\n",
      "loss_tot:331385473997246758912.00000000\n",
      "loss_tot:181319763291086127104.00000000\n",
      "loss_tot:99210312881261576192.00000000\n",
      "loss_tot:54283581686811459584.00000000\n",
      "loss_tot:29701622291649662976.00000000\n",
      "loss_tot:16251439927218853888.00000000\n",
      "loss_tot:8892083304403092480.00000000\n",
      "loss_tot:4865362444555183104.00000000\n",
      "loss_tot:2662115379431385600.00000000\n",
      "loss_tot:1456594112745929728.00000000\n",
      "loss_tot:796985144106714880.00000000\n",
      "loss_tot:436075715477137344.00000000\n",
      "loss_tot:238601724313434624.00000000\n",
      "loss_tot:130552518328542384.00000000\n",
      "loss_tot:71432677603125880.00000000\n",
      "loss_tot:39084863992093264.00000000\n",
      "loss_tot:21385542968496464.00000000\n",
      "loss_tot:11701241900727750.00000000\n",
      "loss_tot:6402412223722288.00000000\n",
      "loss_tot:3503122378702691.00000000\n",
      "loss_tot:1916756672562839.75000000\n",
      "loss_tot:1048766149080477.00000000\n",
      "loss_tot:573839359340909.62500000\n",
      "loss_tot:313980022982766.93750000\n",
      "loss_tot:171796252934359.71875000\n",
      "loss_tot:93999460790779.43750000\n",
      "loss_tot:51432429357245.37500000\n",
      "loss_tot:28141594800364.70703125\n",
      "loss_tot:15397860540078.32421875\n",
      "loss_tot:8425041872600.08886719\n",
      "loss_tot:4609817797452.95117188\n",
      "loss_tot:2522292388206.37500000\n",
      "loss_tot:1380089111739.20117188\n",
      "loss_tot:755124899174.65942383\n",
      "loss_tot:413171544498.22070312\n",
      "loss_tot:226069550628.15292358\n",
      "loss_tot:123695408578.15168762\n",
      "loss_tot:67680780701.61728668\n",
      "loss_tot:37031981017.97420502\n",
      "loss_tot:20262286442.05788040\n",
      "loss_tot:11086665111.57633018\n",
      "loss_tot:6066124278.80673409\n",
      "loss_tot:3319131361.79927111\n",
      "loss_tot:1816076049.65407848\n",
      "loss_tot:993683868.39903343\n",
      "loss_tot:543699965.47936487\n",
      "loss_tot:297488994.62498695\n",
      "loss_tot:162773955.64139897\n",
      "loss_tot:89062933.81119740\n",
      "loss_tot:48730573.97321934\n",
      "loss_tot:26663027.93443005\n",
      "loss_tot:14588823.52836369\n",
      "loss_tot:7982788.07853452\n",
      "loss_tot:4367762.23830923\n",
      "loss_tot:2389836.46595355\n",
      "loss_tot:1307464.98600304\n",
      "loss_tot:715459.11046229\n",
      "loss_tot:391342.04178833\n",
      "loss_tot:214139.33929665\n",
      "loss_tot:117073.19325678\n",
      "loss_tot:64023.67053624\n",
      "loss_tot:35043.24021662\n",
      "loss_tot:19152.18553779\n",
      "loss_tot:10464.25757533\n",
      "loss_tot:5696.33104859\n",
      "loss_tot:3101.93344416\n",
      "loss_tot:1693.48883180\n",
      "loss_tot:920.23939292\n",
      "loss_tot:494.07500523\n",
      "loss_tot:260.51465407\n",
      "loss_tot:126.69488848\n",
      "loss_tot:65.83394933\n",
      "loss_tot:24.79258527\n",
      "loss_tot:24.79258527\n",
      "loss_tot:24.79258527\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.19587829\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.14173375\n",
      "loss_tot:24.01268279\n",
      "loss_tot:24.01268279\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.95914861\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.90622478\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.85391130\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.68292284\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.63183006\n",
      "loss_tot:23.58134764\n",
      "loss_tot:23.58134764\n",
      "loss_tot:23.58134764\n",
      "loss_tot:23.58134764\n",
      "loss_tot:23.58134764\n",
      "loss_tot:23.58134764\n",
      "loss_tot:23.58134764\n",
      "loss_tot:23.58134764\n",
      "loss_tot:23.58134764\n",
      "loss_tot:23.58134764\n",
      "loss_tot:23.58134764\n",
      "loss_tot:23.58134764\n",
      "loss_tot:23.58134764\n",
      "loss_tot:23.58134764\n",
      "loss_tot:23.58134764\n",
      "loss_tot:23.58134764\n",
      "loss_tot:23.58134764\n",
      "loss_tot:23.58134764\n",
      "loss_tot:23.58134764\n",
      "loss_tot:23.58134764\n",
      "loss_tot:23.58134764\n",
      "loss_tot:23.58134764\n",
      "loss_tot:23.58134764\n",
      "loss_tot:23.58134764\n",
      "loss_tot:23.58134764\n",
      "loss_tot:23.58134764\n",
      "loss_tot:23.58134764\n",
      "loss_tot:23.58134764\n",
      "loss_tot:23.58134764\n",
      "loss_tot:23.58134764\n",
      "Total training time: 559.4882609844208\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.LBFGS(model.parameters(),lr=0.01,max_iter=30)\n",
    "epochi = epoch\n",
    "\n",
    "epochs = 2000\n",
    "tic = time.time()\n",
    "for epoch in range(epochi, epochs+epochi):\n",
    "    loss = train(epoch)\n",
    "    print(f'loss_tot:{loss:.8f}')\n",
    "    loss_history.append(to_numpy(loss))\n",
    "    #if loss < 0.01:\n",
    "    #    break\n",
    "toc = time.time()\n",
    "\n",
    "print(f'Total training time: {toc - tic}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.linspace(0.0, 1.0, 100)                                   \n",
    "t = np.linspace(0,  0, 1)                                     \n",
    "t_grid, x_grid = np.meshgrid(t, x)                               \n",
    "T = t_grid.flatten()[:, None]                                    \n",
    "X = x_grid.flatten()[:, None]                                    \n",
    "x_test = np.hstack((T, X))                                       \n",
    "x_1,xL,xR,xP,xPL,xPR = X_entropy(x_test,0.00,0.01,0.01)\n",
    "x_test = torch.tensor(x_test, requires_grad=True, dtype=dtype).to(device)\n",
    "xR = torch.tensor(xR, dtype=dtype).to(device)\n",
    "xL = torch.tensor(xL, dtype=dtype).to(device)\n",
    "u_pred = model(x_test)\n",
    "uL_pred = model(xL)\n",
    "uR_pred = model(xR)\n",
    "rho, p,u = u_pred[:, 0], u_pred[:, 1], u_pred[:, 2]          \n",
    "rhoL, pL,uL = uL_pred[:, 0], uL_pred[:, 1], uL_pred[:, 2]          \n",
    "rhoR, pR,uR = uR_pred[:, 0], uR_pred[:, 1], uR_pred[:, 2]          \n",
    "U2 = rho*u\n",
    "U3 = 0.5*rho*u**2 + p/0.4\n",
    "\n",
    "#F1 = U2\n",
    "F2 = rho*u**2+p\n",
    "F3 = u*(U3 + p)\n",
    "\n",
    "gamma = 1.4                                                    \n",
    "\n",
    "# Gradients and partial derivatives\n",
    "drho_g = gradients(rho, x_test)[0]                                  \n",
    "rho_t, rho_x = drho_g[:, :1], drho_g[:, 1:]             \n",
    "\n",
    "\n",
    "du_g = gradients(u, x_test)[0]                                      \n",
    "u_t, u_x = du_g[:, :1], du_g[:, 1:]                            \n",
    "\n",
    "dp_g = gradients(p, x_test)[0]                                      \n",
    "p_t, p_x = dp_g[:, :1], dp_g[:, 1:]                            \n",
    "\n",
    "dp_g = gradients(p, x_test)[0]                                     \n",
    "p_t, p_x = dp_g[:, :1], dp_g[:, 1:]                           \n",
    "\n",
    "dU2_g = gradients(U2, x_test)[0]\n",
    "U2_t,U2_x = dU2_g[:,:1], dU2_g[:,1:]\n",
    "dU3_g = gradients(U3, x_test)[0]\n",
    "U3_t,U3_x = dU3_g[:,:1], dU3_g[:,1:]\n",
    "dF2_g = gradients(F2, x_test)[0]\n",
    "F2_t,F2_x = dF2_g[:,:1], dF2_g[:,1:]\n",
    "dF3_g = gradients(F3, x_test)[0]\n",
    "F3_t,F3_x = dF3_g[:,:1], dF3_g[:,1:]\n",
    "\n",
    "d = 0.1*(abs(u_x)-(u_x))  + 1\n",
    "\n",
    "#d = 0.1*(abs(uR-uL)-(uR-uL))/Dx + 1\n",
    "#d = torch.exp(-10*u_x)+1\n",
    "#d1 = torch.clamp(d/5,min=1)\n",
    "\n",
    "f = ((rho_t + U2_x)/d)**2 + \\\n",
    "    ((U2_t  + F2_x)/d)**2 + \\\n",
    "    ((U3_t  + F3_x)/d)**2 #+\\\n",
    "#d = 1/(0.1*(torch.clamp((abs(u_x)-u_x))\n",
    "d = 1/(0.1*(abs(u_x)-u_x)+1)\n",
    "d2 =1/(0.1*(abs(uR-uL)-(uR-uL))/0.02  + 1)\n",
    "d = to_numpy(d)\n",
    "d2 = to_numpy(d2)\n",
    "f = to_numpy(f)\n",
    "#d3 = to_numpy(d3)\n",
    "u_pred = to_numpy(u_pred)\n",
    "np.size(u_pred[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x[:],u_pred[:,0]*rhoref)\n",
    "#ax.scatter(x[:],u_pred[:,1]*pref)\n",
    "#ax.scatter(x[:],u_pred[:,2]*uref)\n",
    "#ax.scatter(x[:],u_pred[:,1]*pref/u_pred[:,0]/rhoref/0.4)\n",
    "#ax.scatter(x[:],d)\n",
    "#ax.scatter(x[:],f)\n",
    "#ax.plot(Exact[:,0],Exact[:,1],color='black')\n",
    "#ax.plot(Exact[:,0],Exact[:,2],color='black')\n",
    "#ax.plot(Exact[:,0],Exact[:,3],color='black')\n",
    "#ax.plot(Exact[:,0],Exact[:,4]/0.4,color='black')\n",
    "ax.set_xlim(0.0, 1.0)\n",
    "plt.savefig('1.eps', format='eps')\n",
    "#ax.set_ylim(0.0, 1.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_pred_i[:,:,k] = u_pred\n",
    "k = k+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "u_pred_i = np.zeros((100,3,10))\n",
    "k = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x[:],u_pred[:,0])\n",
    "ax.scatter(x[:],u_pred[:,1])\n",
    "ax.scatter(x[:],u_pred[:,2])\n",
    "ax.plot(Exact[:,0],Exact[:,1])\n",
    "ax.plot(Exact[:,0],Exact[:,2])\n",
    "ax.plot(Exact[:,0],Exact[:,3])\n",
    "ax.set_xlim(0.0, 1.0)\n",
    "#ax.set_ylim(0.2, 1.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Exact = np.loadtxt('123e.dat')\n",
    "Exact[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "x = np.linspace(0,1,100)\n",
    "#ax.plot(x,np.max([0.1-x,np.zeros_like(x)],axis=0))\n",
    "#ax.plot(x,np.exp(100*(0.1-x))/ np.exp(100*(0.1)))\n",
    "#ax.plot(x,np.exp(100*(-x)))\n",
    "ax.plot(x,np.exp(100*(x-0.1)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max([0.1-x,np.zeros_like(x)],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xp= x.flatten()[:,None]\n",
    "rhop= u_pred[:,0].flatten()[:,None]\n",
    "up= u_pred[:,1].flatten()[:,None]\n",
    "pp= u_pred[:,2].flatten()[:,None]\n",
    "uxy= np.hstack((xp,rhop,up,pp))    \n",
    "np.savetxt('shockCrho.dat', uxy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first 15000step conservation\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.scatter(x[:],u_pred[:,0])\n",
    "plt.scatter(x[:],u_pred[:,1])\n",
    "plt.scatter(x[:],u_pred[:,2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first 15000step non-conservation\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.scatter(x[:],u_pred[:,0])\n",
    "plt.scatter(x[:],u_pred[:,1])\n",
    "plt.scatter(x[:],u_pred[:,2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
