{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 应用PINN求解一维欧拉方程（Sod激波管问题）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorchtools import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping(object):\n",
    "    def __init__(self, mode='min', min_delta=0, patience=10, percentage=False):\n",
    "        self.mode = mode\n",
    "        self.min_delta = min_delta\n",
    "        self.patience = patience\n",
    "        self.best = None\n",
    "        self.num_bad_epochs = 0\n",
    "        self.is_better = None\n",
    "        self._init_is_better(mode, min_delta, percentage)\n",
    "\n",
    "        if patience == 0:\n",
    "            self.is_better = lambda a, b: True\n",
    "            self.step = lambda a: False\n",
    "\n",
    "    def step(self, metrics):\n",
    "        if self.best is None:\n",
    "            self.best = metrics\n",
    "            return False\n",
    "\n",
    "        if np.isnan(metrics):\n",
    "            return True\n",
    "\n",
    "        if self.is_better(metrics, self.best):\n",
    "            self.num_bad_epochs = 0\n",
    "            self.best = metrics\n",
    "        else:\n",
    "            self.num_bad_epochs += 1\n",
    "\n",
    "        if self.num_bad_epochs >= self.patience:\n",
    "            print('terminating because of early stopping!')\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def _init_is_better(self, mode, min_delta, percentage):\n",
    "        if mode not in {'min', 'max'}:\n",
    "            raise ValueError('mode ' + mode + ' is unknown!')\n",
    "        if not percentage:\n",
    "            if mode == 'min':\n",
    "                self.is_better = lambda a, best: a < best - min_delta\n",
    "            if mode == 'max':\n",
    "                self.is_better = lambda a, best: a > best + min_delta\n",
    "        else:\n",
    "            if mode == 'min':\n",
    "                self.is_better = lambda a, best: a < best - (\n",
    "                            best * min_delta / 100)\n",
    "            if mode == 'max':\n",
    "                self.is_better = lambda a, best: a > best + (\n",
    "                            best * min_delta / 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    model.it = epoch\n",
    "    tocs1 = time.time()\n",
    "    #print(f'Training time with Adam: {tocs1 - tics1}')\n",
    "    def closure():\n",
    "        optimizer.zero_grad()                                                     # Optimizer\n",
    "        loss_pde = model.loss_pde(x_int_train)                                    # Loss function of PDE\n",
    "        loss_ic = model.loss_ic(x_ic_train, rho_ic_train,u_ic_train,p_ic_train)   # Loss function of IC\n",
    "        loss = loss_pde + 10*loss_ic                                          # Total loss function G(theta)\n",
    "        \n",
    "        if (loss < 0.0001):\n",
    "            return\n",
    "        print(f'epoch {epoch} loss_pde:{loss_pde:.8f}, loss_ic:{loss_ic:.8f}')\n",
    "        model.it = model.it + 1\n",
    "        # Print total loss\n",
    "        outputfile = open('loss_history_sod.txt','a+')\n",
    "        #print(f'epoch {model.it} loss_pde:{loss_pde: .8f},loss_ic:{loss_ic: .8f}',file=outputfile)\n",
    "        #print(f'epoch {model.it}: loss {loss:.6f}',file=outputfile)\n",
    "        print(f'{model.it}  {loss_pde:.6f}  {loss_ic:.6f}  {loss:.6f}',file=outputfile)\n",
    "        outputfile.close()\n",
    "        #outputfile = open('loss_history_ringV1.txt','a+')\n",
    "        ## Print iteration, loss of PDE and ICs\n",
    "        #print(f'epoch {i} loss_pde:{loss_pde: .8f},loss_ic:{loss_ic: .8f}',file=outputfile)\n",
    "        #print(f'epoch {i}: loss {loss:.6f}',file=outputfile)\n",
    "        #outputfile.close()\n",
    "        loss.backward()\n",
    "        return loss\n",
    "\n",
    "    # Optimize loss function\n",
    "    loss = optimizer.step(closure)\n",
    "    loss_value = loss.item() if not isinstance(loss, float) else loss\n",
    "    # Print total loss\n",
    "    print(f'epoch {epoch}: loss {loss_value:.6f}')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import time\n",
    "import scipy.io\n",
    "# Seeds\n",
    "torch.manual_seed(123)\n",
    "np.random.seed(123)\n",
    "\n",
    "   \n",
    "# Calculate gradients using torch.autograd.grad\n",
    "def gradients(outputs, inputs):\n",
    "    return torch.autograd.grad(outputs, inputs,grad_outputs=torch.ones_like(outputs), create_graph=True)\n",
    "\n",
    "# Convert torch tensor into np.array\n",
    "def to_numpy(input):\n",
    "    if isinstance(input, torch.Tensor):\n",
    "        return input.detach().cpu().numpy()\n",
    "    elif isinstance(input, np.ndarray):\n",
    "        return input\n",
    "    else:\n",
    "        raise TypeError('Unknown type of input, expected torch.Tensor or ' \\\n",
    "                        'np.ndarray, but got {}'.format(type(input)))\n",
    "\n",
    "# Initial conditions\n",
    "def IC(x):\n",
    "    N = len(x)\n",
    "    rho_init = np.zeros((x.shape[0]))                                              # rho - initial condition\n",
    "    u_init = np.zeros((x.shape[0]))                                                # u - initial condition\n",
    "    p_init = np.zeros((x.shape[0]))                                                # p - initial condition\n",
    "\n",
    "    # rho, p - initial condition\n",
    "    for i in range(N):\n",
    "        if (x[i] <= 0.5):\n",
    "            rho_init[i] = 1.0\n",
    "            p_init[i] = 1.0\n",
    "        else:\n",
    "            rho_init[i] = 0.125\n",
    "            p_init[i] = 0.1\n",
    "\n",
    "    return rho_init, u_init, p_init\n",
    "\n",
    "# Generate Neural Network\n",
    "class DNN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DNN, self).__init__()\n",
    "        self.net = nn.Sequential()                                                  # Define neural network\n",
    "        self.net.add_module('Linear_layer_1', nn.Linear(2, 50))                     # First linear layer\n",
    "        self.net.add_module('Tanh_layer_1', nn.Tanh())                              # First activation Layer\n",
    "\n",
    "        for num in range(2, 6):                                                     # Number of layers (2 through 7)\n",
    "            self.net.add_module('Linear_layer_%d' % (num), nn.Linear(50, 50))       # Linear layer\n",
    "            self.net.add_module('Tanh_layer_%d' % (num), nn.Tanh())                 # Activation Layer\n",
    "        self.net.add_module('Linear_layer_final', nn.Linear(50, 3))                 # Output Layer\n",
    "\n",
    "    # Forward Feed\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "    # Loss function for PDE\n",
    "    def loss_pde(self, x):\n",
    "        y = self.net(x)                                                # Neural network\n",
    "        rho,p,u = y[:, 0:1], y[:, 1:2], y[:, 2:]\n",
    "        \n",
    "        U2 = rho*u\n",
    "        U3 = 0.5*rho*u**2 + p/0.4\n",
    "        \n",
    "        #F1 = U2\n",
    "        F2 = rho*u**2+p\n",
    "        F3 = u*(U3 + p)\n",
    "        \n",
    "        gamma = 1.4                                                    # Heat Capacity Ratio\n",
    "\n",
    "        # Gradients and partial derivatives\n",
    "        drho_g = gradients(rho, x)[0]                                  # Gradient [rho_t, rho_x]\n",
    "        rho_t, rho_x = drho_g[:, :1], drho_g[:, 1:]                    # Partial derivatives rho_t, rho_x\n",
    "\n",
    "\n",
    "        du_g = gradients(u, x)[0]                                      # Gradient [u_t, u_x]\n",
    "        u_t, u_x = du_g[:, :1], du_g[:, 1:]                            # Partial derivatives u_t, u_x\n",
    "\n",
    "        dp_g = gradients(p, x)[0]                                      # Gradient [p_t, p_x]\n",
    "        p_t, p_x = dp_g[:, :1], dp_g[:, 1:]                            # Partial derivatives p_t, p_x\n",
    "        \n",
    "        dU2_g = gradients(U2, x)[0]\n",
    "        U2_t,U2_x = dU2_g[:,:1], dU2_g[:,1:]\n",
    "        dU3_g = gradients(U3, x)[0]\n",
    "        U3_t,U3_x = dU3_g[:,:1], dU3_g[:,1:]\n",
    "        dF2_g = gradients(F2, x)[0]\n",
    "        F2_t,F2_x = dF2_g[:,:1], dF2_g[:,1:]\n",
    "        dF3_g = gradients(F3, x)[0]\n",
    "        F3_t,F3_x = dF3_g[:,:1], dF3_g[:,1:]\n",
    "\n",
    "        d = 0.1*(abs(u_x)-u_x)  + 1\n",
    "        d = 0.1*(abs(u_x)-u_x)  + 1\n",
    "        d = 1\n",
    "     \n",
    "        f = (((rho_t + U2_x)/d)**2).mean() + \\\n",
    "            (((U2_t  + F2_x)/d)**2).mean() + \\\n",
    "            (((U3_t  + F3_x)/d)**2).mean() +\\\n",
    "            ((rho_t).mean())**2  +\\\n",
    "            ((U3_t).mean())**2 \n",
    "    \n",
    "        return f\n",
    "\n",
    "    # Loss function for initial condition\n",
    "    def loss_ic(self, x_ic, rho_ic, u_ic, p_ic):\n",
    "        y_ic = self.net(x_ic)                                                      # Initial condition\n",
    "        rho_ic_nn, p_ic_nn,u_ic_nn = y_ic[:, 0], y_ic[:, 1], y_ic[:, 2]            # rho, u, p - initial condition\n",
    "\n",
    "        # Loss function for the initial condition\n",
    "        loss_ics = ((u_ic_nn - u_ic) ** 2).mean() + \\\n",
    "               ((rho_ic_nn- rho_ic) ** 2).mean()  + \\\n",
    "               ((p_ic_nn - p_ic) ** 2).mean()\n",
    "\n",
    "        return loss_ics\n",
    "    \n",
    "device = torch.device('cuda')                                          # Run on CPU\n",
    "num_x = 1000                                                        # Number of points in t\n",
    "num_t = 2000                                                        # Number of points in x\n",
    "num_i_train = 1000                                                 # Random sampled points from IC0\n",
    "num_f_train = 10000                                                 # Random sampled points in interior\n",
    "x = np.linspace(0.0, 1.0, num_x)                                   # Partitioned spatial axis\n",
    "t = np.linspace(0, 0.2, num_t)                                        # Partitioned time axis\n",
    "t_grid, x_grid = np.meshgrid(t, x)                                    # (t,x) in [0,0.2]x[a,b]\n",
    "T = t_grid.flatten()[:, None]                                         # Vectorized t_grid\n",
    "X = x_grid.flatten()[:, None]                                         # Vectorized x_grid\n",
    "id_f = np.random.choice(num_x*num_t, num_f_train, replace=False)      # Random sample numbering for interior\n",
    "\n",
    "x_int = X[:, 0][id_f, None]                                           # Random x - interior\n",
    "t_int = T[:, 0][id_f, None]                                           # Random t - interior\n",
    "x_int_train = np.hstack((t_int, x_int))                               # Random (x,t) - vectorized\n",
    "\n",
    "x = np.linspace(0.0, 1.0, 1000)                                   # Partitioned spatial axis\n",
    "t = np.linspace(0, 0.2, 200)                                        # Partitioned time axis\n",
    "t_grid, x_grid = np.meshgrid(t, x)                                    # (t,x) in [0,0.2]x[a,b]\n",
    "id_ic = np.random.choice(1000, 1000, replace=False)           # Random sample numbering for IC\n",
    "T = t_grid.flatten()[:, None]                                         # Vectorized t_grid\n",
    "X = x_grid.flatten()[:, None]                                         # Vectorized x_grid\n",
    "x_ic = x_grid[id_ic, 0][:, None]                                      # Random x - initial condition\n",
    "t_ic = t_grid[id_ic, 0][:, None]                                      # random t - initial condition\n",
    "x_ic_train = np.hstack((t_ic, x_ic))                                  # Random (x,t) - vectorized\n",
    "\n",
    "rho_ic_train, u_ic_train, p_ic_train = IC(x_ic)                       # Initial condition evaluated at random sample\n",
    "x_ic_train = torch.tensor(x_ic_train, dtype=torch.float32).to(device)\n",
    "x_int_train = torch.tensor(x_int_train, requires_grad=True, dtype=torch.float32).to(device)\n",
    "\n",
    "rho_ic_train = torch.tensor(rho_ic_train, dtype=torch.float32).to(device)\n",
    "u_ic_train = torch.tensor(u_ic_train, dtype=torch.float32).to(device)\n",
    "p_ic_train = torch.tensor(p_ic_train, dtype=torch.float32).to(device)\n",
    "\n",
    "model = DNN().to(device)\n",
    "\n",
    "print('Start training...')\n",
    "\n",
    "\n",
    "lr = 0.002                                                           # Learning rate\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "epochi = epoch\n",
    "\n",
    "epochs = 8000\n",
    "tic = time.time()\n",
    "for epoch in range(1+epochi, epochs+epochi):\n",
    "    train(epoch)\n",
    "toc = time.time()\n",
    "print(f'Total training time: {toc - tic}')\n",
    "\n",
    "# Evaluate on the whole computational domain\n",
    "u_pred = to_numpy(model(x_test))\n",
    "#scipy.io.matlab('Sod_Shock_Tube.mat', {'x': x, 't': t,'rho': u_pred[:,0],\n",
    "#                                                          'u': u_pred[:,2],\n",
    "#                                                           'p': u_pred[:,1]})\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0001                                                           # Learning rate\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "#optimizer = torch.optim.LBFGS(model.parameters(),lr=0.1,max_iter=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7999 loss_pde:0.00721696, loss_ic:0.00115811\n",
      "epoch 7999: loss 0.018798\n",
      "epoch 8000 loss_pde:0.02923422, loss_ic:0.00118005\n",
      "epoch 8000: loss 0.041035\n",
      "epoch 8001 loss_pde:0.00863831, loss_ic:0.00116231\n",
      "epoch 8001: loss 0.020261\n",
      "epoch 8002 loss_pde:0.01207767, loss_ic:0.00116594\n",
      "epoch 8002: loss 0.023737\n",
      "epoch 8003 loss_pde:0.01960136, loss_ic:0.00117766\n",
      "epoch 8003: loss 0.031378\n",
      "epoch 8004 loss_pde:0.01586332, loss_ic:0.00117788\n",
      "epoch 8004: loss 0.027642\n",
      "epoch 8005 loss_pde:0.00920139, loss_ic:0.00117326\n",
      "epoch 8005: loss 0.020934\n",
      "epoch 8006 loss_pde:0.00731124, loss_ic:0.00116691\n",
      "epoch 8006: loss 0.018980\n",
      "epoch 8007 loss_pde:0.01048284, loss_ic:0.00116018\n",
      "epoch 8007: loss 0.022085\n",
      "epoch 8008 loss_pde:0.01339076, loss_ic:0.00115319\n",
      "epoch 8008: loss 0.024923\n",
      "epoch 8009 loss_pde:0.01258799, loss_ic:0.00114732\n",
      "epoch 8009: loss 0.024061\n",
      "epoch 8010 loss_pde:0.00957461, loss_ic:0.00114547\n",
      "epoch 8010: loss 0.021029\n",
      "epoch 8011 loss_pde:0.00753041, loss_ic:0.00114887\n",
      "epoch 8011: loss 0.019019\n",
      "epoch 8012 loss_pde:0.00788893, loss_ic:0.00115581\n",
      "epoch 8012: loss 0.019447\n",
      "epoch 8013 loss_pde:0.00947357, loss_ic:0.00116535\n",
      "epoch 8013: loss 0.021127\n",
      "epoch 8014 loss_pde:0.01022772, loss_ic:0.00117674\n",
      "epoch 8014: loss 0.021995\n",
      "epoch 8015 loss_pde:0.00938243, loss_ic:0.00118441\n",
      "epoch 8015: loss 0.021227\n",
      "epoch 8016 loss_pde:0.00787350, loss_ic:0.00118270\n",
      "epoch 8016: loss 0.019700\n",
      "epoch 8017 loss_pde:0.00712884, loss_ic:0.00117326\n",
      "epoch 8017: loss 0.018861\n",
      "epoch 8018 loss_pde:0.00764071, loss_ic:0.00116131\n",
      "epoch 8018: loss 0.019254\n",
      "epoch 8019 loss_pde:0.00862758, loss_ic:0.00115147\n",
      "epoch 8019: loss 0.020142\n",
      "epoch 8020 loss_pde:0.00900088, loss_ic:0.00114610\n",
      "epoch 8020: loss 0.020462\n",
      "epoch 8021 loss_pde:0.00849449, loss_ic:0.00114508\n",
      "epoch 8021: loss 0.019945\n",
      "epoch 8022 loss_pde:0.00771877, loss_ic:0.00114646\n",
      "epoch 8022: loss 0.019183\n",
      "epoch 8023 loss_pde:0.00736633, loss_ic:0.00114898\n",
      "epoch 8023: loss 0.018856\n",
      "epoch 8024 loss_pde:0.00756560, loss_ic:0.00115386\n",
      "epoch 8024: loss 0.019104\n",
      "epoch 8025 loss_pde:0.00789095, loss_ic:0.00116242\n",
      "epoch 8025: loss 0.019515\n",
      "epoch 8026 loss_pde:0.00787858, loss_ic:0.00117235\n",
      "epoch 8026: loss 0.019602\n",
      "epoch 8027 loss_pde:0.00750173, loss_ic:0.00117803\n",
      "epoch 8027: loss 0.019282\n",
      "epoch 8028 loss_pde:0.00714551, loss_ic:0.00117597\n",
      "epoch 8028: loss 0.018905\n",
      "epoch 8029 loss_pde:0.00715167, loss_ic:0.00116767\n",
      "epoch 8029: loss 0.018828\n",
      "epoch 8030 loss_pde:0.00745055, loss_ic:0.00115753\n",
      "epoch 8030: loss 0.019026\n",
      "epoch 8031 loss_pde:0.00768842, loss_ic:0.00114967\n",
      "epoch 8031: loss 0.019185\n",
      "epoch 8032 loss_pde:0.00765232, loss_ic:0.00114602\n",
      "epoch 8032: loss 0.019113\n",
      "epoch 8033 loss_pde:0.00745454, loss_ic:0.00114582\n",
      "epoch 8033: loss 0.018913\n",
      "epoch 8034 loss_pde:0.00732012, loss_ic:0.00114712\n",
      "epoch 8034: loss 0.018791\n",
      "epoch 8035 loss_pde:0.00732708, loss_ic:0.00114952\n",
      "epoch 8035: loss 0.018822\n",
      "epoch 8036 loss_pde:0.00737326, loss_ic:0.00115406\n",
      "epoch 8036: loss 0.018914\n",
      "epoch 8037 loss_pde:0.00732950, loss_ic:0.00116044\n",
      "epoch 8037: loss 0.018934\n",
      "epoch 8038 loss_pde:0.00719524, loss_ic:0.00116578\n",
      "epoch 8038: loss 0.018853\n",
      "epoch 8039 loss_pde:0.00709406, loss_ic:0.00116689\n",
      "epoch 8039: loss 0.018763\n",
      "epoch 8040 loss_pde:0.00712078, loss_ic:0.00116320\n",
      "epoch 8040: loss 0.018753\n",
      "epoch 8041 loss_pde:0.00722902, loss_ic:0.00115691\n",
      "epoch 8041: loss 0.018798\n",
      "epoch 8042 loss_pde:0.00730068, loss_ic:0.00115105\n",
      "epoch 8042: loss 0.018811\n",
      "epoch 8043 loss_pde:0.00729247, loss_ic:0.00114759\n",
      "epoch 8043: loss 0.018768\n",
      "epoch 8044 loss_pde:0.00725872, loss_ic:0.00114656\n",
      "epoch 8044: loss 0.018724\n",
      "epoch 8045 loss_pde:0.00724906, loss_ic:0.00114698\n",
      "epoch 8045: loss 0.018719\n",
      "epoch 8046 loss_pde:0.00724792, loss_ic:0.00114852\n",
      "epoch 8046: loss 0.018733\n",
      "epoch 8047 loss_pde:0.00721767, loss_ic:0.00115149\n",
      "epoch 8047: loss 0.018733\n",
      "epoch 8048 loss_pde:0.00715697, loss_ic:0.00115537\n",
      "epoch 8048: loss 0.018711\n",
      "epoch 8049 loss_pde:0.00710683, loss_ic:0.00115829\n",
      "epoch 8049: loss 0.018690\n",
      "epoch 8050 loss_pde:0.00710414, loss_ic:0.00115852\n",
      "epoch 8050: loss 0.018689\n",
      "epoch 8051 loss_pde:0.00713864, loss_ic:0.00115599\n",
      "epoch 8051: loss 0.018699\n",
      "epoch 8052 loss_pde:0.00716968, loss_ic:0.00115218\n",
      "epoch 8052: loss 0.018692\n",
      "epoch 8053 loss_pde:0.00717958, loss_ic:0.00114891\n",
      "epoch 8053: loss 0.018669\n",
      "epoch 8054 loss_pde:0.00718538, loss_ic:0.00114712\n",
      "epoch 8054: loss 0.018657\n",
      "epoch 8055 loss_pde:0.00719736, loss_ic:0.00114666\n",
      "epoch 8055: loss 0.018664\n",
      "epoch 8056 loss_pde:0.00719820, loss_ic:0.00114715\n",
      "epoch 8056: loss 0.018670\n",
      "epoch 8057 loss_pde:0.00717310, loss_ic:0.00114854\n",
      "epoch 8057: loss 0.018659\n",
      "epoch 8058 loss_pde:0.00713516, loss_ic:0.00115063\n",
      "epoch 8058: loss 0.018641\n",
      "epoch 8059 loss_pde:0.00711092, loss_ic:0.00115251\n",
      "epoch 8059: loss 0.018636\n",
      "epoch 8060 loss_pde:0.00711154, loss_ic:0.00115307\n",
      "epoch 8060: loss 0.018642\n",
      "epoch 8061 loss_pde:0.00712503, loss_ic:0.00115196\n",
      "epoch 8061: loss 0.018645\n",
      "epoch 8062 loss_pde:0.00713570, loss_ic:0.00114984\n",
      "epoch 8062: loss 0.018634\n",
      "epoch 8063 loss_pde:0.00714310, loss_ic:0.00114778\n",
      "epoch 8063: loss 0.018621\n",
      "epoch 8064 loss_pde:0.00715410, loss_ic:0.00114646\n",
      "epoch 8064: loss 0.018619\n",
      "epoch 8065 loss_pde:0.00716384, loss_ic:0.00114599\n",
      "epoch 8065: loss 0.018624\n",
      "epoch 8066 loss_pde:0.00715957, loss_ic:0.00114624\n",
      "epoch 8066: loss 0.018622\n",
      "epoch 8067 loss_pde:0.00714072, loss_ic:0.00114711\n",
      "epoch 8067: loss 0.018612\n",
      "epoch 8068 loss_pde:0.00712119, loss_ic:0.00114831\n",
      "epoch 8068: loss 0.018604\n",
      "epoch 8069 loss_pde:0.00711242, loss_ic:0.00114923\n",
      "epoch 8069: loss 0.018605\n",
      "epoch 8070 loss_pde:0.00711342, loss_ic:0.00114929\n",
      "epoch 8070: loss 0.018606\n",
      "epoch 8071 loss_pde:0.00711774, loss_ic:0.00114839\n",
      "epoch 8071: loss 0.018602\n",
      "epoch 8072 loss_pde:0.00712357, loss_ic:0.00114700\n",
      "epoch 8072: loss 0.018594\n",
      "epoch 8073 loss_pde:0.00713244, loss_ic:0.00114572\n",
      "epoch 8073: loss 0.018590\n",
      "epoch 8074 loss_pde:0.00714115, loss_ic:0.00114491\n",
      "epoch 8074: loss 0.018590\n",
      "epoch 8075 loss_pde:0.00714232, loss_ic:0.00114466\n",
      "epoch 8075: loss 0.018589\n",
      "epoch 8076 loss_pde:0.00713388, loss_ic:0.00114493\n",
      "epoch 8076: loss 0.018583\n",
      "epoch 8077 loss_pde:0.00712206, loss_ic:0.00114556\n",
      "epoch 8077: loss 0.018578\n",
      "epoch 8078 loss_pde:0.00711361, loss_ic:0.00114622\n",
      "epoch 8078: loss 0.018576\n",
      "epoch 8079 loss_pde:0.00710995, loss_ic:0.00114651\n",
      "epoch 8079: loss 0.018575\n",
      "epoch 8080 loss_pde:0.00710972, loss_ic:0.00114619\n",
      "epoch 8080: loss 0.018572\n",
      "epoch 8081 loss_pde:0.00711292, loss_ic:0.00114536\n",
      "epoch 8081: loss 0.018567\n",
      "epoch 8082 loss_pde:0.00711935, loss_ic:0.00114438\n",
      "epoch 8082: loss 0.018563\n",
      "epoch 8083 loss_pde:0.00712575, loss_ic:0.00114358\n",
      "epoch 8083: loss 0.018562\n",
      "epoch 8084 loss_pde:0.00712766, loss_ic:0.00114314\n",
      "epoch 8084: loss 0.018559\n",
      "epoch 8085 loss_pde:0.00712402, loss_ic:0.00114309\n",
      "epoch 8085: loss 0.018555\n",
      "epoch 8086 loss_pde:0.00711770, loss_ic:0.00114335\n",
      "epoch 8086: loss 0.018551\n",
      "epoch 8087 loss_pde:0.00711170, loss_ic:0.00114371\n",
      "epoch 8087: loss 0.018549\n",
      "epoch 8088 loss_pde:0.00710730, loss_ic:0.00114390\n",
      "epoch 8088: loss 0.018546\n",
      "epoch 8089 loss_pde:0.00710541, loss_ic:0.00114373\n",
      "epoch 8089: loss 0.018543\n",
      "epoch 8090 loss_pde:0.00710697, loss_ic:0.00114322\n",
      "epoch 8090: loss 0.018539\n",
      "epoch 8091 loss_pde:0.00711107, loss_ic:0.00114252\n",
      "epoch 8091: loss 0.018536\n",
      "epoch 8092 loss_pde:0.00711494, loss_ic:0.00114187\n",
      "epoch 8092: loss 0.018534\n",
      "epoch 8093 loss_pde:0.00711612, loss_ic:0.00114143\n",
      "epoch 8093: loss 0.018530\n",
      "epoch 8094 loss_pde:0.00711444, loss_ic:0.00114124\n",
      "epoch 8094: loss 0.018527\n",
      "epoch 8095 loss_pde:0.00711107, loss_ic:0.00114128\n",
      "epoch 8095: loss 0.018524\n",
      "epoch 8096 loss_pde:0.00710708, loss_ic:0.00114140\n",
      "epoch 8096: loss 0.018521\n",
      "epoch 8097 loss_pde:0.00710352, loss_ic:0.00114143\n",
      "epoch 8097: loss 0.018518\n",
      "epoch 8098 loss_pde:0.00710173, loss_ic:0.00114127\n",
      "epoch 8098: loss 0.018514\n",
      "epoch 8099 loss_pde:0.00710231, loss_ic:0.00114090\n",
      "epoch 8099: loss 0.018511\n",
      "epoch 8100 loss_pde:0.00710428, loss_ic:0.00114041\n",
      "epoch 8100: loss 0.018508\n",
      "epoch 8101 loss_pde:0.00710595, loss_ic:0.00113992\n",
      "epoch 8101: loss 0.018505\n",
      "epoch 8102 loss_pde:0.00710635, loss_ic:0.00113954\n",
      "epoch 8102: loss 0.018502\n",
      "epoch 8103 loss_pde:0.00710541, loss_ic:0.00113932\n",
      "epoch 8103: loss 0.018499\n",
      "epoch 8104 loss_pde:0.00710339, loss_ic:0.00113921\n",
      "epoch 8104: loss 0.018496\n",
      "epoch 8105 loss_pde:0.00710075, loss_ic:0.00113916\n",
      "epoch 8105: loss 0.018492\n",
      "epoch 8106 loss_pde:0.00709843, loss_ic:0.00113905\n",
      "epoch 8106: loss 0.018489\n",
      "epoch 8107 loss_pde:0.00709726, loss_ic:0.00113884\n",
      "epoch 8107: loss 0.018486\n",
      "epoch 8108 loss_pde:0.00709726, loss_ic:0.00113853\n",
      "epoch 8108: loss 0.018483\n",
      "epoch 8109 loss_pde:0.00709772, loss_ic:0.00113816\n",
      "epoch 8109: loss 0.018479\n",
      "epoch 8110 loss_pde:0.00709806, loss_ic:0.00113779\n",
      "epoch 8110: loss 0.018476\n",
      "epoch 8111 loss_pde:0.00709794, loss_ic:0.00113747\n",
      "epoch 8111: loss 0.018473\n",
      "epoch 8112 loss_pde:0.00709716, loss_ic:0.00113723\n",
      "epoch 8112: loss 0.018469\n",
      "epoch 8113 loss_pde:0.00709572, loss_ic:0.00113704\n",
      "epoch 8113: loss 0.018466\n",
      "epoch 8114 loss_pde:0.00709400, loss_ic:0.00113688\n",
      "epoch 8114: loss 0.018463\n",
      "epoch 8115 loss_pde:0.00709257, loss_ic:0.00113669\n",
      "epoch 8115: loss 0.018459\n",
      "epoch 8116 loss_pde:0.00709167, loss_ic:0.00113645\n",
      "epoch 8116: loss 0.018456\n",
      "epoch 8117 loss_pde:0.00709119, loss_ic:0.00113617\n",
      "epoch 8117: loss 0.018453\n",
      "epoch 8118 loss_pde:0.00709090, loss_ic:0.00113586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8118: loss 0.018449\n",
      "epoch 8119 loss_pde:0.00709062, loss_ic:0.00113555\n",
      "epoch 8119: loss 0.018446\n",
      "epoch 8120 loss_pde:0.00709011, loss_ic:0.00113527\n",
      "epoch 8120: loss 0.018443\n",
      "epoch 8121 loss_pde:0.00708923, loss_ic:0.00113502\n",
      "epoch 8121: loss 0.018439\n",
      "epoch 8122 loss_pde:0.00708806, loss_ic:0.00113479\n",
      "epoch 8122: loss 0.018436\n",
      "epoch 8123 loss_pde:0.00708686, loss_ic:0.00113457\n",
      "epoch 8123: loss 0.018433\n",
      "epoch 8124 loss_pde:0.00708582, loss_ic:0.00113433\n",
      "epoch 8124: loss 0.018429\n",
      "epoch 8125 loss_pde:0.00708498, loss_ic:0.00113408\n",
      "epoch 8125: loss 0.018426\n",
      "epoch 8126 loss_pde:0.00708432, loss_ic:0.00113380\n",
      "epoch 8126: loss 0.018422\n",
      "epoch 8127 loss_pde:0.00708378, loss_ic:0.00113351\n",
      "epoch 8127: loss 0.018419\n",
      "epoch 8128 loss_pde:0.00708318, loss_ic:0.00113323\n",
      "epoch 8128: loss 0.018415\n",
      "epoch 8129 loss_pde:0.00708242, loss_ic:0.00113295\n",
      "epoch 8129: loss 0.018412\n",
      "epoch 8130 loss_pde:0.00708151, loss_ic:0.00113270\n",
      "epoch 8130: loss 0.018408\n",
      "epoch 8131 loss_pde:0.00708055, loss_ic:0.00113244\n",
      "epoch 8131: loss 0.018405\n",
      "epoch 8132 loss_pde:0.00707961, loss_ic:0.00113219\n",
      "epoch 8132: loss 0.018402\n",
      "epoch 8133 loss_pde:0.00707873, loss_ic:0.00113193\n",
      "epoch 8133: loss 0.018398\n",
      "epoch 8134 loss_pde:0.00707794, loss_ic:0.00113166\n",
      "epoch 8134: loss 0.018394\n",
      "epoch 8135 loss_pde:0.00707722, loss_ic:0.00113137\n",
      "epoch 8135: loss 0.018391\n",
      "epoch 8136 loss_pde:0.00707651, loss_ic:0.00113110\n",
      "epoch 8136: loss 0.018387\n",
      "epoch 8137 loss_pde:0.00707572, loss_ic:0.00113082\n",
      "epoch 8137: loss 0.018384\n",
      "epoch 8138 loss_pde:0.00707487, loss_ic:0.00113055\n",
      "epoch 8138: loss 0.018380\n",
      "epoch 8139 loss_pde:0.00707397, loss_ic:0.00113028\n",
      "epoch 8139: loss 0.018377\n",
      "epoch 8140 loss_pde:0.00707306, loss_ic:0.00113002\n",
      "epoch 8140: loss 0.018373\n",
      "epoch 8141 loss_pde:0.00707217, loss_ic:0.00112975\n",
      "epoch 8141: loss 0.018370\n",
      "epoch 8142 loss_pde:0.00707135, loss_ic:0.00112948\n",
      "epoch 8142: loss 0.018366\n",
      "epoch 8143 loss_pde:0.00707058, loss_ic:0.00112919\n",
      "epoch 8143: loss 0.018363\n",
      "epoch 8144 loss_pde:0.00706983, loss_ic:0.00112891\n",
      "epoch 8144: loss 0.018359\n",
      "epoch 8145 loss_pde:0.00706902, loss_ic:0.00112863\n",
      "epoch 8145: loss 0.018355\n",
      "epoch 8146 loss_pde:0.00706815, loss_ic:0.00112835\n",
      "epoch 8146: loss 0.018352\n",
      "epoch 8147 loss_pde:0.00706723, loss_ic:0.00112808\n",
      "epoch 8147: loss 0.018348\n",
      "epoch 8148 loss_pde:0.00706631, loss_ic:0.00112781\n",
      "epoch 8148: loss 0.018344\n",
      "epoch 8149 loss_pde:0.00706543, loss_ic:0.00112754\n",
      "epoch 8149: loss 0.018341\n",
      "epoch 8150 loss_pde:0.00706459, loss_ic:0.00112725\n",
      "epoch 8150: loss 0.018337\n",
      "epoch 8151 loss_pde:0.00706381, loss_ic:0.00112697\n",
      "epoch 8151: loss 0.018333\n",
      "epoch 8152 loss_pde:0.00706302, loss_ic:0.00112668\n",
      "epoch 8152: loss 0.018330\n",
      "epoch 8153 loss_pde:0.00706220, loss_ic:0.00112639\n",
      "epoch 8153: loss 0.018326\n",
      "epoch 8154 loss_pde:0.00706131, loss_ic:0.00112611\n",
      "epoch 8154: loss 0.018322\n",
      "epoch 8155 loss_pde:0.00706038, loss_ic:0.00112584\n",
      "epoch 8155: loss 0.018319\n",
      "epoch 8156 loss_pde:0.00705944, loss_ic:0.00112556\n",
      "epoch 8156: loss 0.018315\n",
      "epoch 8157 loss_pde:0.00705853, loss_ic:0.00112528\n",
      "epoch 8157: loss 0.018311\n",
      "epoch 8158 loss_pde:0.00705770, loss_ic:0.00112499\n",
      "epoch 8158: loss 0.018308\n",
      "epoch 8159 loss_pde:0.00705689, loss_ic:0.00112470\n",
      "epoch 8159: loss 0.018304\n",
      "epoch 8160 loss_pde:0.00705610, loss_ic:0.00112441\n",
      "epoch 8160: loss 0.018300\n",
      "epoch 8161 loss_pde:0.00705526, loss_ic:0.00112412\n",
      "epoch 8161: loss 0.018296\n",
      "epoch 8162 loss_pde:0.00705436, loss_ic:0.00112383\n",
      "epoch 8162: loss 0.018293\n",
      "epoch 8163 loss_pde:0.00705345, loss_ic:0.00112355\n",
      "epoch 8163: loss 0.018289\n",
      "epoch 8164 loss_pde:0.00705251, loss_ic:0.00112326\n",
      "epoch 8164: loss 0.018285\n",
      "epoch 8165 loss_pde:0.00705160, loss_ic:0.00112298\n",
      "epoch 8165: loss 0.018281\n",
      "epoch 8166 loss_pde:0.00705073, loss_ic:0.00112269\n",
      "epoch 8166: loss 0.018278\n",
      "epoch 8167 loss_pde:0.00704988, loss_ic:0.00112239\n",
      "epoch 8167: loss 0.018274\n",
      "epoch 8168 loss_pde:0.00704905, loss_ic:0.00112210\n",
      "epoch 8168: loss 0.018270\n",
      "epoch 8169 loss_pde:0.00704818, loss_ic:0.00112180\n",
      "epoch 8169: loss 0.018266\n",
      "epoch 8170 loss_pde:0.00704729, loss_ic:0.00112151\n",
      "epoch 8170: loss 0.018262\n",
      "epoch 8171 loss_pde:0.00704638, loss_ic:0.00112122\n",
      "epoch 8171: loss 0.018259\n",
      "epoch 8172 loss_pde:0.00704544, loss_ic:0.00112093\n",
      "epoch 8172: loss 0.018255\n",
      "epoch 8173 loss_pde:0.00704453, loss_ic:0.00112064\n",
      "epoch 8173: loss 0.018251\n",
      "epoch 8174 loss_pde:0.00704364, loss_ic:0.00112035\n",
      "epoch 8174: loss 0.018247\n",
      "epoch 8175 loss_pde:0.00704276, loss_ic:0.00112005\n",
      "epoch 8175: loss 0.018243\n",
      "epoch 8176 loss_pde:0.00704191, loss_ic:0.00111975\n",
      "epoch 8176: loss 0.018239\n",
      "epoch 8177 loss_pde:0.00704103, loss_ic:0.00111946\n",
      "epoch 8177: loss 0.018236\n",
      "epoch 8178 loss_pde:0.00704012, loss_ic:0.00111916\n",
      "epoch 8178: loss 0.018232\n",
      "epoch 8179 loss_pde:0.00703920, loss_ic:0.00111886\n",
      "epoch 8179: loss 0.018228\n",
      "epoch 8180 loss_pde:0.00703828, loss_ic:0.00111857\n",
      "epoch 8180: loss 0.018224\n",
      "epoch 8181 loss_pde:0.00703735, loss_ic:0.00111827\n",
      "epoch 8181: loss 0.018220\n",
      "epoch 8182 loss_pde:0.00703645, loss_ic:0.00111797\n",
      "epoch 8182: loss 0.018216\n",
      "epoch 8183 loss_pde:0.00703556, loss_ic:0.00111767\n",
      "epoch 8183: loss 0.018212\n",
      "epoch 8184 loss_pde:0.00703466, loss_ic:0.00111737\n",
      "epoch 8184: loss 0.018208\n",
      "epoch 8185 loss_pde:0.00703376, loss_ic:0.00111707\n",
      "epoch 8185: loss 0.018204\n",
      "epoch 8186 loss_pde:0.00703286, loss_ic:0.00111677\n",
      "epoch 8186: loss 0.018201\n",
      "epoch 8187 loss_pde:0.00703194, loss_ic:0.00111647\n",
      "epoch 8187: loss 0.018197\n",
      "epoch 8188 loss_pde:0.00703102, loss_ic:0.00111617\n",
      "epoch 8188: loss 0.018193\n",
      "epoch 8189 loss_pde:0.00703010, loss_ic:0.00111587\n",
      "epoch 8189: loss 0.018189\n",
      "epoch 8190 loss_pde:0.00702917, loss_ic:0.00111556\n",
      "epoch 8190: loss 0.018185\n",
      "epoch 8191 loss_pde:0.00702825, loss_ic:0.00111526\n",
      "epoch 8191: loss 0.018181\n",
      "epoch 8192 loss_pde:0.00702735, loss_ic:0.00111496\n",
      "epoch 8192: loss 0.018177\n",
      "epoch 8193 loss_pde:0.00702644, loss_ic:0.00111465\n",
      "epoch 8193: loss 0.018173\n",
      "epoch 8194 loss_pde:0.00702552, loss_ic:0.00111435\n",
      "epoch 8194: loss 0.018169\n",
      "epoch 8195 loss_pde:0.00702460, loss_ic:0.00111404\n",
      "epoch 8195: loss 0.018165\n",
      "epoch 8196 loss_pde:0.00702366, loss_ic:0.00111374\n",
      "epoch 8196: loss 0.018161\n",
      "epoch 8197 loss_pde:0.00702273, loss_ic:0.00111343\n",
      "epoch 8197: loss 0.018157\n",
      "epoch 8198 loss_pde:0.00702180, loss_ic:0.00111312\n",
      "epoch 8198: loss 0.018153\n",
      "epoch 8199 loss_pde:0.00702088, loss_ic:0.00111282\n",
      "epoch 8199: loss 0.018149\n",
      "epoch 8200 loss_pde:0.00701996, loss_ic:0.00111251\n",
      "epoch 8200: loss 0.018145\n",
      "epoch 8201 loss_pde:0.00701906, loss_ic:0.00111220\n",
      "epoch 8201: loss 0.018141\n",
      "epoch 8202 loss_pde:0.00701815, loss_ic:0.00111189\n",
      "epoch 8202: loss 0.018137\n",
      "epoch 8203 loss_pde:0.00701724, loss_ic:0.00111157\n",
      "epoch 8203: loss 0.018133\n",
      "epoch 8204 loss_pde:0.00701630, loss_ic:0.00111126\n",
      "epoch 8204: loss 0.018129\n",
      "epoch 8205 loss_pde:0.00701534, loss_ic:0.00111096\n",
      "epoch 8205: loss 0.018125\n",
      "epoch 8206 loss_pde:0.00701440, loss_ic:0.00111064\n",
      "epoch 8206: loss 0.018121\n",
      "epoch 8207 loss_pde:0.00701347, loss_ic:0.00111033\n",
      "epoch 8207: loss 0.018117\n",
      "epoch 8208 loss_pde:0.00701257, loss_ic:0.00111002\n",
      "epoch 8208: loss 0.018113\n",
      "epoch 8209 loss_pde:0.00701168, loss_ic:0.00110970\n",
      "epoch 8209: loss 0.018109\n",
      "epoch 8210 loss_pde:0.00701077, loss_ic:0.00110938\n",
      "epoch 8210: loss 0.018105\n",
      "epoch 8211 loss_pde:0.00700981, loss_ic:0.00110907\n",
      "epoch 8211: loss 0.018101\n",
      "epoch 8212 loss_pde:0.00700883, loss_ic:0.00110876\n",
      "epoch 8212: loss 0.018096\n",
      "epoch 8213 loss_pde:0.00700785, loss_ic:0.00110845\n",
      "epoch 8213: loss 0.018092\n",
      "epoch 8214 loss_pde:0.00700691, loss_ic:0.00110814\n",
      "epoch 8214: loss 0.018088\n",
      "epoch 8215 loss_pde:0.00700598, loss_ic:0.00110782\n",
      "epoch 8215: loss 0.018084\n",
      "epoch 8216 loss_pde:0.00700505, loss_ic:0.00110750\n",
      "epoch 8216: loss 0.018080\n",
      "epoch 8217 loss_pde:0.00700412, loss_ic:0.00110718\n",
      "epoch 8217: loss 0.018076\n",
      "epoch 8218 loss_pde:0.00700317, loss_ic:0.00110686\n",
      "epoch 8218: loss 0.018072\n",
      "epoch 8219 loss_pde:0.00700221, loss_ic:0.00110655\n",
      "epoch 8219: loss 0.018068\n",
      "epoch 8220 loss_pde:0.00700124, loss_ic:0.00110623\n",
      "epoch 8220: loss 0.018064\n",
      "epoch 8221 loss_pde:0.00700028, loss_ic:0.00110592\n",
      "epoch 8221: loss 0.018059\n",
      "epoch 8222 loss_pde:0.00699935, loss_ic:0.00110560\n",
      "epoch 8222: loss 0.018055\n",
      "epoch 8223 loss_pde:0.00699841, loss_ic:0.00110528\n",
      "epoch 8223: loss 0.018051\n",
      "epoch 8224 loss_pde:0.00699745, loss_ic:0.00110496\n",
      "epoch 8224: loss 0.018047\n",
      "epoch 8225 loss_pde:0.00699649, loss_ic:0.00110464\n",
      "epoch 8225: loss 0.018043\n",
      "epoch 8226 loss_pde:0.00699554, loss_ic:0.00110432\n",
      "epoch 8226: loss 0.018039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8227 loss_pde:0.00699458, loss_ic:0.00110400\n",
      "epoch 8227: loss 0.018035\n",
      "epoch 8228 loss_pde:0.00699362, loss_ic:0.00110368\n",
      "epoch 8228: loss 0.018030\n",
      "epoch 8229 loss_pde:0.00699267, loss_ic:0.00110336\n",
      "epoch 8229: loss 0.018026\n",
      "epoch 8230 loss_pde:0.00699173, loss_ic:0.00110303\n",
      "epoch 8230: loss 0.018022\n",
      "epoch 8231 loss_pde:0.00699076, loss_ic:0.00110271\n",
      "epoch 8231: loss 0.018018\n",
      "epoch 8232 loss_pde:0.00698980, loss_ic:0.00110239\n",
      "epoch 8232: loss 0.018014\n",
      "epoch 8233 loss_pde:0.00698882, loss_ic:0.00110207\n",
      "epoch 8233: loss 0.018010\n",
      "epoch 8234 loss_pde:0.00698786, loss_ic:0.00110175\n",
      "epoch 8234: loss 0.018005\n",
      "epoch 8235 loss_pde:0.00698689, loss_ic:0.00110142\n",
      "epoch 8235: loss 0.018001\n",
      "epoch 8236 loss_pde:0.00698593, loss_ic:0.00110110\n",
      "epoch 8236: loss 0.017997\n",
      "epoch 8237 loss_pde:0.00698495, loss_ic:0.00110078\n",
      "epoch 8237: loss 0.017993\n",
      "epoch 8238 loss_pde:0.00698398, loss_ic:0.00110045\n",
      "epoch 8238: loss 0.017989\n",
      "epoch 8239 loss_pde:0.00698301, loss_ic:0.00110013\n",
      "epoch 8239: loss 0.017984\n",
      "epoch 8240 loss_pde:0.00698203, loss_ic:0.00109981\n",
      "epoch 8240: loss 0.017980\n",
      "epoch 8241 loss_pde:0.00698107, loss_ic:0.00109948\n",
      "epoch 8241: loss 0.017976\n",
      "epoch 8242 loss_pde:0.00698010, loss_ic:0.00109915\n",
      "epoch 8242: loss 0.017972\n",
      "epoch 8243 loss_pde:0.00697913, loss_ic:0.00109883\n",
      "epoch 8243: loss 0.017967\n",
      "epoch 8244 loss_pde:0.00697815, loss_ic:0.00109850\n",
      "epoch 8244: loss 0.017963\n",
      "epoch 8245 loss_pde:0.00697718, loss_ic:0.00109818\n",
      "epoch 8245: loss 0.017959\n",
      "epoch 8246 loss_pde:0.00697620, loss_ic:0.00109785\n",
      "epoch 8246: loss 0.017955\n",
      "epoch 8247 loss_pde:0.00697522, loss_ic:0.00109752\n",
      "epoch 8247: loss 0.017950\n",
      "epoch 8248 loss_pde:0.00697423, loss_ic:0.00109719\n",
      "epoch 8248: loss 0.017946\n",
      "epoch 8249 loss_pde:0.00697325, loss_ic:0.00109687\n",
      "epoch 8249: loss 0.017942\n",
      "epoch 8250 loss_pde:0.00697228, loss_ic:0.00109654\n",
      "epoch 8250: loss 0.017938\n",
      "epoch 8251 loss_pde:0.00697131, loss_ic:0.00109621\n",
      "epoch 8251: loss 0.017933\n",
      "epoch 8252 loss_pde:0.00697033, loss_ic:0.00109588\n",
      "epoch 8252: loss 0.017929\n",
      "epoch 8253 loss_pde:0.00696934, loss_ic:0.00109555\n",
      "epoch 8253: loss 0.017925\n",
      "epoch 8254 loss_pde:0.00696835, loss_ic:0.00109523\n",
      "epoch 8254: loss 0.017921\n",
      "epoch 8255 loss_pde:0.00696736, loss_ic:0.00109490\n",
      "epoch 8255: loss 0.017916\n",
      "epoch 8256 loss_pde:0.00696638, loss_ic:0.00109457\n",
      "epoch 8256: loss 0.017912\n",
      "epoch 8257 loss_pde:0.00696539, loss_ic:0.00109424\n",
      "epoch 8257: loss 0.017908\n",
      "epoch 8258 loss_pde:0.00696441, loss_ic:0.00109391\n",
      "epoch 8258: loss 0.017903\n",
      "epoch 8259 loss_pde:0.00696342, loss_ic:0.00109358\n",
      "epoch 8259: loss 0.017899\n",
      "epoch 8260 loss_pde:0.00696244, loss_ic:0.00109324\n",
      "epoch 8260: loss 0.017895\n",
      "epoch 8261 loss_pde:0.00696144, loss_ic:0.00109291\n",
      "epoch 8261: loss 0.017891\n",
      "epoch 8262 loss_pde:0.00696045, loss_ic:0.00109258\n",
      "epoch 8262: loss 0.017886\n",
      "epoch 8263 loss_pde:0.00695946, loss_ic:0.00109225\n",
      "epoch 8263: loss 0.017882\n",
      "epoch 8264 loss_pde:0.00695845, loss_ic:0.00109192\n",
      "epoch 8264: loss 0.017878\n",
      "epoch 8265 loss_pde:0.00695747, loss_ic:0.00109159\n",
      "epoch 8265: loss 0.017873\n",
      "epoch 8266 loss_pde:0.00695647, loss_ic:0.00109125\n",
      "epoch 8266: loss 0.017869\n",
      "epoch 8267 loss_pde:0.00695549, loss_ic:0.00109092\n",
      "epoch 8267: loss 0.017865\n",
      "epoch 8268 loss_pde:0.00695449, loss_ic:0.00109059\n",
      "epoch 8268: loss 0.017860\n",
      "epoch 8269 loss_pde:0.00695349, loss_ic:0.00109025\n",
      "epoch 8269: loss 0.017856\n",
      "epoch 8270 loss_pde:0.00695250, loss_ic:0.00108992\n",
      "epoch 8270: loss 0.017852\n",
      "epoch 8271 loss_pde:0.00695150, loss_ic:0.00108959\n",
      "epoch 8271: loss 0.017847\n",
      "epoch 8272 loss_pde:0.00695050, loss_ic:0.00108925\n",
      "epoch 8272: loss 0.017843\n",
      "epoch 8273 loss_pde:0.00694951, loss_ic:0.00108892\n",
      "epoch 8273: loss 0.017839\n",
      "epoch 8274 loss_pde:0.00694851, loss_ic:0.00108858\n",
      "epoch 8274: loss 0.017834\n",
      "epoch 8275 loss_pde:0.00694750, loss_ic:0.00108825\n",
      "epoch 8275: loss 0.017830\n",
      "epoch 8276 loss_pde:0.00694649, loss_ic:0.00108791\n",
      "epoch 8276: loss 0.017826\n",
      "epoch 8277 loss_pde:0.00694550, loss_ic:0.00108757\n",
      "epoch 8277: loss 0.017821\n",
      "epoch 8278 loss_pde:0.00694449, loss_ic:0.00108724\n",
      "epoch 8278: loss 0.017817\n",
      "epoch 8279 loss_pde:0.00694349, loss_ic:0.00108690\n",
      "epoch 8279: loss 0.017812\n",
      "epoch 8280 loss_pde:0.00694248, loss_ic:0.00108656\n",
      "epoch 8280: loss 0.017808\n",
      "epoch 8281 loss_pde:0.00694148, loss_ic:0.00108623\n",
      "epoch 8281: loss 0.017804\n",
      "epoch 8282 loss_pde:0.00694048, loss_ic:0.00108589\n",
      "epoch 8282: loss 0.017799\n",
      "epoch 8283 loss_pde:0.00693948, loss_ic:0.00108555\n",
      "epoch 8283: loss 0.017795\n",
      "epoch 8284 loss_pde:0.00693848, loss_ic:0.00108522\n",
      "epoch 8284: loss 0.017791\n",
      "epoch 8285 loss_pde:0.00693747, loss_ic:0.00108488\n",
      "epoch 8285: loss 0.017786\n",
      "epoch 8286 loss_pde:0.00693646, loss_ic:0.00108454\n",
      "epoch 8286: loss 0.017782\n",
      "epoch 8287 loss_pde:0.00693543, loss_ic:0.00108420\n",
      "epoch 8287: loss 0.017777\n",
      "epoch 8288 loss_pde:0.00693444, loss_ic:0.00108386\n",
      "epoch 8288: loss 0.017773\n",
      "epoch 8289 loss_pde:0.00693344, loss_ic:0.00108352\n",
      "epoch 8289: loss 0.017769\n",
      "epoch 8290 loss_pde:0.00693242, loss_ic:0.00108319\n",
      "epoch 8290: loss 0.017764\n",
      "epoch 8291 loss_pde:0.00693140, loss_ic:0.00108285\n",
      "epoch 8291: loss 0.017760\n",
      "epoch 8292 loss_pde:0.00693038, loss_ic:0.00108251\n",
      "epoch 8292: loss 0.017755\n",
      "epoch 8293 loss_pde:0.00692936, loss_ic:0.00108217\n",
      "epoch 8293: loss 0.017751\n",
      "epoch 8294 loss_pde:0.00692837, loss_ic:0.00108183\n",
      "epoch 8294: loss 0.017747\n",
      "epoch 8295 loss_pde:0.00692735, loss_ic:0.00108149\n",
      "epoch 8295: loss 0.017742\n",
      "epoch 8296 loss_pde:0.00692635, loss_ic:0.00108114\n",
      "epoch 8296: loss 0.017738\n",
      "epoch 8297 loss_pde:0.00692531, loss_ic:0.00108080\n",
      "epoch 8297: loss 0.017733\n",
      "epoch 8298 loss_pde:0.00692431, loss_ic:0.00108046\n",
      "epoch 8298: loss 0.017729\n",
      "epoch 8299 loss_pde:0.00692330, loss_ic:0.00108012\n",
      "epoch 8299: loss 0.017725\n",
      "epoch 8300 loss_pde:0.00692229, loss_ic:0.00107978\n",
      "epoch 8300: loss 0.017720\n",
      "epoch 8301 loss_pde:0.00692126, loss_ic:0.00107944\n",
      "epoch 8301: loss 0.017716\n",
      "epoch 8302 loss_pde:0.00692024, loss_ic:0.00107910\n",
      "epoch 8302: loss 0.017711\n",
      "epoch 8303 loss_pde:0.00691921, loss_ic:0.00107875\n",
      "epoch 8303: loss 0.017707\n",
      "epoch 8304 loss_pde:0.00691819, loss_ic:0.00107841\n",
      "epoch 8304: loss 0.017702\n",
      "epoch 8305 loss_pde:0.00691719, loss_ic:0.00107807\n",
      "epoch 8305: loss 0.017698\n",
      "epoch 8306 loss_pde:0.00691641, loss_ic:0.00107770\n",
      "epoch 8306: loss 0.017693\n",
      "epoch 8307 loss_pde:0.00691564, loss_ic:0.00107733\n",
      "epoch 8307: loss 0.017689\n",
      "epoch 8308 loss_pde:0.00691463, loss_ic:0.00107699\n",
      "epoch 8308: loss 0.017685\n",
      "epoch 8309 loss_pde:0.00691340, loss_ic:0.00107666\n",
      "epoch 8309: loss 0.017680\n",
      "epoch 8310 loss_pde:0.00691214, loss_ic:0.00107634\n",
      "epoch 8310: loss 0.017676\n",
      "epoch 8311 loss_pde:0.00691105, loss_ic:0.00107600\n",
      "epoch 8311: loss 0.017671\n",
      "epoch 8312 loss_pde:0.00691015, loss_ic:0.00107565\n",
      "epoch 8312: loss 0.017667\n",
      "epoch 8313 loss_pde:0.00690925, loss_ic:0.00107529\n",
      "epoch 8313: loss 0.017662\n",
      "epoch 8314 loss_pde:0.00690821, loss_ic:0.00107495\n",
      "epoch 8314: loss 0.017658\n",
      "epoch 8315 loss_pde:0.00690704, loss_ic:0.00107461\n",
      "epoch 8315: loss 0.017653\n",
      "epoch 8316 loss_pde:0.00690585, loss_ic:0.00107428\n",
      "epoch 8316: loss 0.017649\n",
      "epoch 8317 loss_pde:0.00690479, loss_ic:0.00107394\n",
      "epoch 8317: loss 0.017644\n",
      "epoch 8318 loss_pde:0.00690383, loss_ic:0.00107359\n",
      "epoch 8318: loss 0.017640\n",
      "epoch 8319 loss_pde:0.00690290, loss_ic:0.00107323\n",
      "epoch 8319: loss 0.017635\n",
      "epoch 8320 loss_pde:0.00690191, loss_ic:0.00107288\n",
      "epoch 8320: loss 0.017631\n",
      "epoch 8321 loss_pde:0.00690079, loss_ic:0.00107254\n",
      "epoch 8321: loss 0.017626\n",
      "epoch 8322 loss_pde:0.00689964, loss_ic:0.00107221\n",
      "epoch 8322: loss 0.017622\n",
      "epoch 8323 loss_pde:0.00689854, loss_ic:0.00107187\n",
      "epoch 8323: loss 0.017617\n",
      "epoch 8324 loss_pde:0.00689754, loss_ic:0.00107152\n",
      "epoch 8324: loss 0.017613\n",
      "epoch 8325 loss_pde:0.00689657, loss_ic:0.00107116\n",
      "epoch 8325: loss 0.017608\n",
      "epoch 8326 loss_pde:0.00689556, loss_ic:0.00107081\n",
      "epoch 8326: loss 0.017604\n",
      "epoch 8327 loss_pde:0.00689445, loss_ic:0.00107047\n",
      "epoch 8327: loss 0.017599\n",
      "epoch 8328 loss_pde:0.00689330, loss_ic:0.00107014\n",
      "epoch 8328: loss 0.017595\n",
      "epoch 8329 loss_pde:0.00689221, loss_ic:0.00106979\n",
      "epoch 8329: loss 0.017590\n",
      "epoch 8330 loss_pde:0.00689120, loss_ic:0.00106944\n",
      "epoch 8330: loss 0.017586\n",
      "epoch 8331 loss_pde:0.00689021, loss_ic:0.00106909\n",
      "epoch 8331: loss 0.017581\n",
      "epoch 8332 loss_pde:0.00688918, loss_ic:0.00106874\n",
      "epoch 8332: loss 0.017577\n",
      "epoch 8333 loss_pde:0.00688809, loss_ic:0.00106840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8333: loss 0.017572\n",
      "epoch 8334 loss_pde:0.00688698, loss_ic:0.00106805\n",
      "epoch 8334: loss 0.017567\n",
      "epoch 8335 loss_pde:0.00688594, loss_ic:0.00106770\n",
      "epoch 8335: loss 0.017563\n",
      "epoch 8336 loss_pde:0.00688493, loss_ic:0.00106735\n",
      "epoch 8336: loss 0.017558\n",
      "epoch 8337 loss_pde:0.00688392, loss_ic:0.00106700\n",
      "epoch 8337: loss 0.017554\n",
      "epoch 8338 loss_pde:0.00688288, loss_ic:0.00106665\n",
      "epoch 8338: loss 0.017549\n",
      "epoch 8339 loss_pde:0.00688183, loss_ic:0.00106630\n",
      "epoch 8339: loss 0.017545\n",
      "epoch 8340 loss_pde:0.00688076, loss_ic:0.00106595\n",
      "epoch 8340: loss 0.017540\n",
      "epoch 8341 loss_pde:0.00687971, loss_ic:0.00106560\n",
      "epoch 8341: loss 0.017536\n",
      "epoch 8342 loss_pde:0.00687867, loss_ic:0.00106525\n",
      "epoch 8342: loss 0.017531\n",
      "epoch 8343 loss_pde:0.00687764, loss_ic:0.00106489\n",
      "epoch 8343: loss 0.017527\n",
      "epoch 8344 loss_pde:0.00687659, loss_ic:0.00106454\n",
      "epoch 8344: loss 0.017522\n",
      "epoch 8345 loss_pde:0.00687554, loss_ic:0.00106419\n",
      "epoch 8345: loss 0.017517\n",
      "epoch 8346 loss_pde:0.00687448, loss_ic:0.00106384\n",
      "epoch 8346: loss 0.017513\n",
      "epoch 8347 loss_pde:0.00687345, loss_ic:0.00106349\n",
      "epoch 8347: loss 0.017508\n",
      "epoch 8348 loss_pde:0.00687240, loss_ic:0.00106313\n",
      "epoch 8348: loss 0.017504\n",
      "epoch 8349 loss_pde:0.00687134, loss_ic:0.00106278\n",
      "epoch 8349: loss 0.017499\n",
      "epoch 8350 loss_pde:0.00687026, loss_ic:0.00106243\n",
      "epoch 8350: loss 0.017495\n",
      "epoch 8351 loss_pde:0.00686919, loss_ic:0.00106208\n",
      "epoch 8351: loss 0.017490\n",
      "epoch 8352 loss_pde:0.00686814, loss_ic:0.00106173\n",
      "epoch 8352: loss 0.017485\n",
      "epoch 8353 loss_pde:0.00686708, loss_ic:0.00106138\n",
      "epoch 8353: loss 0.017481\n",
      "epoch 8354 loss_pde:0.00686603, loss_ic:0.00106102\n",
      "epoch 8354: loss 0.017476\n",
      "epoch 8355 loss_pde:0.00686496, loss_ic:0.00106067\n",
      "epoch 8355: loss 0.017472\n",
      "epoch 8356 loss_pde:0.00686389, loss_ic:0.00106032\n",
      "epoch 8356: loss 0.017467\n",
      "epoch 8357 loss_pde:0.00686282, loss_ic:0.00105997\n",
      "epoch 8357: loss 0.017463\n",
      "epoch 8358 loss_pde:0.00686178, loss_ic:0.00105962\n",
      "epoch 8358: loss 0.017458\n",
      "epoch 8359 loss_pde:0.00686070, loss_ic:0.00105926\n",
      "epoch 8359: loss 0.017453\n",
      "epoch 8360 loss_pde:0.00685964, loss_ic:0.00105891\n",
      "epoch 8360: loss 0.017449\n",
      "epoch 8361 loss_pde:0.00685856, loss_ic:0.00105855\n"
     ]
    }
   ],
   "source": [
    "epochi = epoch\n",
    "epochs = 10000\n",
    "tic = time.time()\n",
    "for epoch in range(epochi, epochs+epochi):\n",
    "    train(epoch)\n",
    "toc = time.time()\n",
    "print(f'Total training time: {toc - tic}')\n",
    "\n",
    "# Evaluate on the whole computational domain\n",
    "u_pred = to_numpy(model(x_test))\n",
    "#scipy.io.matlab('Sod_Shock_Tube.mat', {'x': x, 't': t,'rho': u_pred[:,0],\n",
    "#                                                          'u': u_pred[:,2],\n",
    "#                                                           'p': u_pred[:,1]})\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(patience=5)\n",
    "\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    train_one_epoch(model, data_loader)  # train the model for one epoch.\n",
    "    metric = eval(model, data_loader_dev)  # evalution on dev set.\n",
    "    if es.step(metric):\n",
    "        break  # early stop criterion is met, we can stop now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model,'sod.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0001                                                           # Learning rate\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "optimizer = torch.optim.LBFGS(model.parameters(),lr=0.1,max_iter=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "epochi = epoch\n",
    "\n",
    "epochs = 2000\n",
    "tic = time.time()\n",
    "for epoch in range(epochi, epochs+epochi):\n",
    "    train(epoch)\n",
    "toc = time.time()\n",
    "print(f'Total training time: {toc - tic}')\n",
    "\n",
    "# Evaluate on the whole computational domain\n",
    "#scipy.io.matlab('Sod_Shock_Tube.mat', {'x': x, 't': t,'rho': u_pred[:,0],\n",
    "#                                                          'u': u_pred[:,2],\n",
    "#                                                           'p': u_pred[:,1]})\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('sod.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0.0, 1.0, 100)                                   # Partitioned spatial axis\n",
    "t = np.linspace(0.2, 0.2, 1)                                        # Partitioned time axis\n",
    "t_grid, x_grid = np.meshgrid(t, x)                                    # (t,x) in [0,0.2]x[a,b]\n",
    "T = t_grid.flatten()[:, None]                                        # Vectorized t_grid\n",
    "X = x_grid.flatten()[:, None]                                         # Vectorized x_grid\n",
    "x_test = np.hstack((T, X))                                            # Vectorized whole domain\n",
    "x_test = torch.tensor(x_test, dtype=torch.float32).to(device)\n",
    "u_pred = to_numpy(model(x_test))\n",
    "\n",
    "#loss = model.loss_pde(x_test)                                    # Loss function of PDE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.io.matlab('Sod_Shock_Tube.mat', {'x': x, 't': t,'rho': u_pred[:,0],\n",
    "                                                          'u': u_pred[:,2],\n",
    "                                                           'p': u_pred[:,1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 4184\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# first 15000step non-conservation\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.scatter(x[:],u_pred[:,0])\n",
    "plt.scatter(x[:],u_pred[:,1])\n",
    "plt.scatter(x[:],u_pred[:,2])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_array = np.array(u_pred[:,2])\n",
    "#new_array = np.array(x)\n",
    "file = open(\"p.txt\", \"w+\")\n",
    "content = str(new_array)\n",
    "file.write(content)\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "xp= x.flatten()[:,None]\n",
    "rhop= u_pred[:,0].flatten()[:,None]\n",
    "up= u_pred[:,1].flatten()[:,None]\n",
    "pp= u_pred[:,2].flatten()[:,None]\n",
    "uxy= np.hstack((xp,rhop,up,pp))    \n",
    "np.savetxt('shockCrho.dat', uxy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first 15000step conservation\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.scatter(x[:],u_pred[:,0])\n",
    "plt.scatter(x[:],u_pred[:,1])\n",
    "plt.scatter(x[:],u_pred[:,2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first 15000step non-conservation\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.scatter(x[:],u_pred[:,0])\n",
    "plt.scatter(x[:],u_pred[:,1])\n",
    "plt.scatter(x[:],u_pred[:,2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.size(id_f)\n",
    "x_int_train = np.hstack((t_int, x_int))    # Random (x,t) - vectorized\n",
    "#np.size(x_int_train)\n",
    "#np.size(x_int)\n",
    "#np.size(t_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NewPoint(x,t):\n",
    "    xl = -0.2\n",
    "    xr = 1.8\n",
    "    dgt = 0.0\n",
    "    dg1 = np.array([])\n",
    "    M = np.array([])\n",
    "    N = np.size(x)\n",
    "    L = np.array([])\n",
    "    y = np.copy(x)\n",
    "    for i in range(N):\n",
    "        if i == 0:\n",
    "            L = np.append(L,x[0]-xl)\n",
    "            dgl = dg(x[0],t)\n",
    "            dgr = dg(xl,t)\n",
    "            dga = 0.5*(np.abs(dgl)+np.abs(dgr))\n",
    "            dg1= np.append(dg1,dga)\n",
    "            dgt += dga*L[i]\n",
    "        elif i < N-1:\n",
    "            L = np.append(L,x[i+1]-x[i])\n",
    "            dgl = dg(x[i],t)\n",
    "            dgr = dg(x[i+1],t)\n",
    "            dga = 0.5*(np.abs(dgl)+np.abs(dgr))\n",
    "            dg1= np.append(dg1,dga)\n",
    "            dgt += dga*L[i]\n",
    "        else:\n",
    "            L = np.append(L,xr-x[N-1])\n",
    "            dgl = dg(x[N-1],t)\n",
    "            dgr = dg(xr,t)\n",
    "            dga = 0.5*(np.abs(dgl)+np.abs(dgr))\n",
    "            dg1= np.append(dg1,dga)\n",
    "            dgt += dga*L[N-1]\n",
    "    for i in range(N):\n",
    "        M = np.append(M,int((dg1[i]*L[i])/dgt*N+0.5))\n",
    "\n",
    "    print(\"M\")\n",
    "  #  print(M)\n",
    "  #  print(\"L\")\n",
    "  #  print(L)\n",
    "    print(\"dg1\")\n",
    "    print(dg1)\n",
    "    Mt = 0\n",
    "\n",
    "    for i in range(N):\n",
    "        if i ==0 and M[0] > 0:\n",
    "            for j in range(int(M[0])):\n",
    "                y[j] = (x[0] - xl)/M[0]*(j+1) + xl\n",
    "        elif i < N-1 and M[i]> 0 and Mt<N:\n",
    "            for j in range(int(M[i])):\n",
    "                y[j+Mt] = (x[i] - x[i-1])/M[i]*(j+1) + x[i-1]\n",
    "        elif i==N-1 and M[N-1] >0:\n",
    "            for j in range(int(M[N-1])):\n",
    "                y[j+Mt] = (xr - x[N-1])/M[N-1]*(j+1) + x[N-1]\n",
    "        \n",
    "        Mt = Mt + int(M[i])\n",
    "    return dg1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-0.2, 1.8, 100)                                   # Partitioned spatial axis\n",
    "t = np.linspace(0, 0.2, 100)                                   # Partitioned spatial axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_r = np.empty(10000, dtype=np.float32)\n",
    "t_r = np.empty(10000, dtype=np.float32)\n",
    "i=0\n",
    "for i in range(np.size(t)):\n",
    "    x_r[i*100:99+i*100] =NewPoint(x[0:99],t[99])\n",
    "    t_r[i*100:99+i*100] =t[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_r[600:699]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x[0:99],x_r[0:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_r = np.empty(10000, dtype=np.float32)\n",
    "t_r = np.empty(10000, dtype=np.float32)\n",
    "for i in range(np.size(t)):\n",
    "    x_r[i*100:99+i*100] =NewPoint(x_r[i*100:99+i*100],t[i])\n",
    "    #t_r[i*100:99+i*100] =t[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = t_r.flatten()[:, None]                                         # Vectorized t_grid\n",
    "X = x_r.flatten()[:, None]                                         # Vectorized x_grid\n",
    "x_test = np.hstack((T, X))                                            # Vectorized whole domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y =NewPoint(x,t[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dg(1.8,0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dg(x,t):\n",
    "    y_t=np.array([[x]])\n",
    "    t_t=np.array([[t]])\n",
    "    x_t = np.hstack((y_t,t_t))\n",
    "\n",
    "    x_t = torch.tensor(x_t, requires_grad=True, dtype=torch.float32).to(device)\n",
    "    loss = model.loss_pde(x_t)                                    # Loss function of PDE\n",
    "    y = model.net(x_t)                                                # Neural network\n",
    "    rho,p,u = y[:, 0:1], y[:, 1:2], y[:, 2:]                       # NN_{rho}, NN_{u}, NN_{p}\n",
    "    drho_g = gradients(rho, x_t)[0]\n",
    "    d = torch.tensor(drho_g, dtype=torch.float32).to(device1)\n",
    "    return d.numpy()[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-0.2, 1.8, 1000)                                   # Partitioned spatial axis\n",
    "t = np.linspace(0.2, 0.2, 1)                                        # Partitioned time axis\n",
    "t_grid, x_grid = np.meshgrid(t, x)                                    # (t,x) in [0,0.2]x[a,b]\n",
    "T = t_grid.flatten()[:, None]                                        # Vectorized t_grid\n",
    "X = x_grid.flatten()[:, None]                                         # Vectorized x_grid\n",
    "x_test = np.hstack((T, X))                                            # Vectorized whole domain\n",
    "x_test1 = torch.tensor(x_test, dtype=torch.float32).to(device)\n",
    "u_pred1 = to_numpy(model(x_test1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(x[:],u_pred[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(x[:],u_pred[:,0])\n",
    "plt.plot(x[:],u_pred[:,1])\n",
    "plt.plot(x[:],u_pred[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = model.net(x_int_train)                                                # Neural network\n",
    "rho,p,u = y[:, 0:1], y[:, 1:2], y[:, 2:]                       # NN_{rho}, NN_{u}, NN_{p}\n",
    "drho_g = gradients(rho, x_int_train)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(x,u_pred[:,0])\n",
    "plt.plot(x[:],u_pred[:,1])\n",
    "plt.plot(x[:],u_pred[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device1 = torch.device('cpu')                                          # Run on CPU\n",
    "torch.save(model,'1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load(torch.load(PATH))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文件读取模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "x = np.linspace(-8.0 / 28.0, 20.0 / 28.0, num_x)  # Partitioned spatial axis\n",
    "t = np.linspace(2.0, 2.0, 1)  # Partitioned time axis\n",
    "t_grid, x_grid = np.meshgrid(t, x)  # (t,x) in [0,0.2]x[a,b]\n",
    "T = t_grid.flatten()[:, None]  # Vectorized t_grid\n",
    "X = x_grid.flatten()[:, None]  # Vectorized x_grid\n",
    "x_test = np.hstack((T, X))                                            # Vectorized whole domain\n",
    "x_test = torch.tensor(x_test, dtype=torch.float32).to('cpu')\n",
    "u_pred = to_numpy(model(x_test))\n",
    "plt.plot(x,u_pred[:,0])\n",
    "plt.plot(x,u_pred[:,1])\n",
    "plt.plot(x,u_pred[:,2])\n",
    "plt.plot(x,u_pred[:,3])\n",
    "plt.plot(x,u_pred[:,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(x,u_pred[:,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#u_pred[:,0]\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(x[300:700],u_pred[300:700,0])\n",
    "plt.plot(x[300:700],u_pred[300:700,1])\n",
    "plt.plot(x[300:700],u_pred[300:700,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-1.5, 3.125, num_x)                                   # Partitioned spatial axis\n",
    "t = np.linspace(0.2, 0.2, 1)                                        # Partitioned time axis\n",
    "t_grid, x_grid = np.meshgrid(t, x)                                    # (t,x) in [0,0.2]x[a,b]\n",
    "T = t_grid.flatten()[:, None]                                         # Vectorized t_grid\n",
    "X = x_grid.flatten()[:, None]                                         # Vectorized x_grid\n",
    "x_test = np.hstack((T, X))                                            # Vectorized whole domain\n",
    "x_test = torch.tensor(x_test, dtype=torch.float32).to(device)\n",
    "u_pred = to_numpy(model(x_test))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(x[300:700],u_pred[300:700,0])\n",
    "plt.plot(x[300:700],u_pred[300:700,1])\n",
    "plt.plot(x[300:700],u_pred[300:700,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun(x):\n",
    "    return x.mean()\n",
    "\n",
    "fun([1,2,3,4;1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(1,2,3,4).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.zeros(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
